% Encoding: UTF-8

@Article{Son2016,
  author      = {Chang-Hwan Son and Xiao-Ping Zhang},
  date        = {2016-10-03},
  title       = {Rain Removal via Shrinkage-Based Sparse Coding and Learned Rain Dictionary},
  eprint      = {1610.00386},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {This paper introduces a new rain removal model based on the shrinkage of the sparse codes for a single image. Recently, dictionary learning and sparse coding have been widely used for image restoration problems. These methods can also be applied to the rain removal by learning two types of rain and non-rain dictionaries and forcing the sparse codes of the rain dictionary to be zero vectors. However, this approach can generate unwanted edge artifacts and detail loss in the non-rain regions. Based on this observation, a new approach for shrinking the sparse codes is presented in this paper. To effectively shrink the sparse codes in the rain and non-rain regions, an error map between the input rain image and the reconstructed rain image is generated by using the learned rain dictionary. Based on this error map, both the sparse codes of rain and non-rain dictionaries are used jointly to represent the image structures of objects and avoid the edge artifacts in the non-rain regions. In the rain regions, the correlation matrix between the rain and non-rain dictionaries is calculated. Then, the sparse codes corresponding to the highly correlated signal-atoms in the rain and non-rain dictionaries are shrunk jointly to improve the removal of the rain structures. The experimental results show that the proposed shrinkage-based sparse coding can preserve image structures and avoid the edge artifacts in the non-rain regions, and it can remove the rain structures in the rain regions. Also, visual quality evaluation confirms that the proposed method outperforms the conventional texture and rain removal methods.},
  file        = {:http\://arxiv.org/pdf/1610.00386v1:PDF},
  keywords    = {cs.CV},
}

@Article{Son2016a,
  author      = {Chang-Hwan Son and Xiao-Ping Zhang},
  date        = {2016-10-03},
  title       = {Rain structure transfer using an exemplar rain image for synthetic rain image generation},
  eprint      = {1610.00427},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {This letter proposes a simple method of transferring rain structures of a given exemplar rain image into a target image. Given the exemplar rain image and its corresponding masked rain image, rain patches including rain structures are extracted randomly, and then residual rain patches are obtained by subtracting those rain patches from their mean patches. Next, residual rain patches are selected randomly, and then added to the given target image along a raster scanning direction. To decrease boundary artifacts around the added patches on the target image, minimum error boundary cuts are found using dynamic programming, and then blending is conducted between overlapping patches. Our experiment shows that the proposed method can generate realistic rain images that have similar rain structures in the exemplar images. Moreover, it is expected that the proposed method can be used for rain removal. More specifically, natural images and synthetic rain images generated via the proposed method can be used to learn classifiers, for example, deep neural networks, in a supervised manner.},
  file        = {:http\://arxiv.org/pdf/1610.00427v1:PDF},
  keywords    = {cs.CV},
}

@Article{Huang2019,
  author      = {Zhe Huang and Weijiang Yu and Wayne Zhang and Litong Feng and Nong Xiao},
  date        = {2019-09-20},
  title       = {Gradual Network for Single Image De-raining},
  doi         = {10.1145/3343031.3350883},
  eprint      = {1909.09677},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Most advances in single image de-raining meet a key challenge, which is removing rain streaks with different scales and shapes while preserving image details. Existing single image de-raining approaches treat rain-streak removal as a process of pixel-wise regression directly. However, they are lacking in mining the balance between over-de-raining (e.g. removing texture details in rain-free regions) and under-de-raining (e.g. leaving rain streaks). In this paper, we firstly propose a coarse-to-fine network called Gradual Network (GraNet) consisting of coarse stage and fine stage for delving into single image de-raining with different granularities. Specifically, to reveal coarse-grained rain-streak characteristics (e.g. long and thick rain streaks/raindrops), we propose a coarse stage by utilizing local-global spatial dependencies via a local-global subnetwork composed of region-aware blocks. Taking the residual result (the coarse de-rained result) between the rainy image sample (i.e. the input data) and the output of coarse stage (i.e. the learnt rain mask) as input, the fine stage continues to de-rain by removing the fine-grained rain streaks (e.g. light rain streaks and water mist) to get a rain-free and well-reconstructed output image via a unified contextual merging sub-network with dense blocks and a merging block. Solid and comprehensive experiments on synthetic and real data demonstrate that our GraNet can significantly outperform the state-of-the-art methods by removing rain streaks with various densities, scales and shapes while keeping the image details of rain-free regions well-preserved.},
  file        = {:http\://arxiv.org/pdf/1909.09677v1:PDF},
  keywords    = {cs.CV, cs.AI, cs.MM},
}

@Article{Wang2018,
  author      = {Yinglong Wang and Shuaicheng Liu and Chen Chen and Dehua Xie and Bing Zeng},
  date        = {2018-12-20},
  title       = {Rain Removal By Image Quasi-Sparsity Priors},
  eprint      = {1812.08348},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain streaks will inevitably be captured by some outdoor vision systems, which lowers the image visual quality and also interferes various computer vision applications. We present a novel rain removal method in this paper, which consists of two steps, i.e., detection of rain streaks and reconstruction of the rain-removed image. An accurate detection of rain streaks determines the quality of the overall performance. To this end, we first detect rain streaks according to pixel intensities, motivated by the observation that rain streaks often possess higher intensities compared to other neighboring image structures. Some mis-detected locations are then refined through a morphological processing and the principal component analysis (PCA) such that only locations corresponding to real rain streaks are retained. In the second step, we separate image gradients into a background layer and a rain streak layer, thanks to the image quasi-sparsity prior, so that a rain image can be decomposed into a background layer and a rain layer. We validate the effectiveness of our method through quantitative and qualitative evaluations. We show that our method can remove rain (even for some relatively bright rain) from images robustly and outperforms some state-of-the-art rain removal algorithms.},
  file        = {:http\://arxiv.org/pdf/1812.08348v1:PDF},
  keywords    = {cs.CV},
}

@Article{Liu2019,
  author      = {Tie Liu and Mai Xu and Zulin Wang},
  date        = {2019-06-06},
  title       = {Removing Rain in Videos: A Large-scale Database and A Two-stream ConvLSTM Approach},
  eprint      = {1906.02526},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain removal has recently attracted increasing research attention, as it is able to enhance the visibility of rain videos. However, the existing learning based rain removal approaches for videos suffer from insufficient training data, especially when applying deep learning to remove rain. In this paper, we establish a large-scale video database for rain removal (LasVR), which consists of 316 rain videos. Then, we observe from our database that there exist the temporal correlation of clean content and similar patterns of rain across video frames. According to these two observations, we propose a two-stream convolutional long- and short- term memory (ConvLSTM) approach for rain removal in videos. The first stream is composed of the subnet for rain detection, while the second stream is the subnet of rain removal that leverages the features from the rain detection subnet. Finally, the experimental results on both synthetic and real rain videos show the proposed approach performs better than other state-of-the-art approaches.},
  file        = {:http\://arxiv.org/pdf/1906.02526v1:PDF},
  keywords    = {cs.CV, eess.IV},
}

@Article{Wang2018a,
  author      = {Yinglong Wang and Shuaicheng Liu and Bing Zeng},
  date        = {2018-12-19},
  title       = {Removing rain streaks by a linear model},
  eprint      = {1812.07870},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Removing rain streaks from a single image continues to draw attentions today in outdoor vision systems. In this paper, we present an efficient method to remove rain streaks. First, the location map of rain pixels needs to be known as precisely as possible, to which we implement a relatively accurate detection of rain streaks by utilizing two characteristics of rain streaks.The key component of our method is to represent the intensity of each detected rain pixel using a linear model: $p=\alpha s + \beta$, where $p$ is the observed intensity of a rain pixel and $s$ represents the intensity of the background (i.e., before rain-affected). To solve $\alpha$ and $\beta$ for each detected rain pixel, we concentrate on a window centered around it and form an $L_2$-norm cost function by considering all detected rain pixels within the window, where the corresponding rain-removed intensity of each detected rain pixel is estimated by some neighboring non-rain pixels. By minimizing this cost function, we determine $\alpha$ and $\beta$ so as to construct the final rain-removed pixel intensity. Compared with several state-of-the-art works, our proposed method can remove rain streaks from a single color image much more efficiently - it offers not only a better visual quality but also a speed-up of several times to one degree of magnitude.},
  file        = {:http\://arxiv.org/pdf/1812.07870v1:PDF},
  keywords    = {cs.CV},
}

@Article{Bahnsen2018,
  author      = {Chris H. Bahnsen and Thomas B. Moeslund},
  date        = {2018-10-30},
  title       = {Rain Removal in Traffic Surveillance: Does it Matter?},
  doi         = {10.1109/TITS.2018.2872502},
  eprint      = {1810.12574},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Varying weather conditions, including rainfall and snowfall, are generally regarded as a challenge for computer vision algorithms. One proposed solution to the challenges induced by rain and snowfall is to artificially remove the rain from images or video using rain removal algorithms. It is the promise of these algorithms that the rain-removed image frames will improve the performance of subsequent segmentation and tracking algorithms. However, rain removal algorithms are typically evaluated on their ability to remove synthetic rain on a small subset of images. Currently, their behavior is unknown on real-world videos when integrated with a typical computer vision pipeline. In this paper, we review the existing rain removal algorithms and propose a new dataset that consists of 22 traffic surveillance sequences under a broad variety of weather conditions that all include either rain or snowfall. We propose a new evaluation protocol that evaluates the rain removal algorithms on their ability to improve the performance of subsequent segmentation, instance segmentation, and feature tracking algorithms under rain and snow. If successful, the de-rained frames of a rain removal algorithm should improve segmentation performance and increase the number of accurately tracked features. The results show that a recent single-frame-based rain removal algorithm increases the segmentation performance by 19.7% on our proposed dataset, but it eventually decreases the feature tracking performance and showed mixed results with recent instance segmentation methods. However, the best video-based rain removal algorithm improves the feature tracking accuracy by 7.72%.},
  file        = {:http\://arxiv.org/pdf/1810.12574v1:PDF},
  keywords    = {cs.CV},
}

@Article{Wang2019,
  author      = {Tianyu Wang and Xin Yang and Ke Xu and Shaozhe Chen and Qiang Zhang and Rynson Lau},
  date        = {2019-04-02},
  title       = {Spatial Attentive Single-Image Deraining with a High Quality Real Rain Dataset},
  eprint      = {1904.01538},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Removing rain streaks from a single image has been drawing considerable attention as rain streaks can severely degrade the image quality and affect the performance of existing outdoor vision tasks. While recent CNN-based derainers have reported promising performances, deraining remains an open problem for two reasons. First, existing synthesized rain datasets have only limited realism, in terms of modeling real rain characteristics such as rain shape, direction and intensity. Second, there are no public benchmarks for quantitative comparisons on real rain images, which makes the current evaluation less objective. The core challenge is that real world rain/clean image pairs cannot be captured at the same time. In this paper, we address the single image rain removal problem in two ways. First, we propose a semi-automatic method that incorporates temporal priors and human supervision to generate a high-quality clean image from each input sequence of real rain images. Using this method, we construct a large-scale dataset of $\sim$$29.5K$ rain/rain-free image pairs that covers a wide range of natural rain scenes. Second, to better cover the stochastic distribution of real rain streaks, we propose a novel SPatial Attentive Network (SPANet) to remove rain streaks in a local-to-global manner. Extensive experiments demonstrate that our network performs favorably against the state-of-the-art deraining methods.},
  file        = {:http\://arxiv.org/pdf/1904.01538v2:PDF},
  keywords    = {cs.CV},
}

@Article{Wang2020,
  author      = {Yinglong Wang and Yibing Song and Chao Ma and Bing Zeng},
  date        = {2020-08-03},
  title       = {Rethinking Image Deraining via Rain Streaks and Vapors},
  eprint      = {2008.00823},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Single image deraining regards an input image as a fusion of a background image, a transmission map, rain streaks, and atmosphere light. While advanced models are proposed for image restoration (i.e., background image generation), they regard rain streaks with the same properties as background rather than transmission medium. As vapors (i.e., rain streaks accumulation or fog-like rain) are conveyed in the transmission map to model the veiling effect, the fusion of rain streaks and vapors do not naturally reflect the rain image formation. In this work, we reformulate rain streaks as transmission medium together with vapors to model rain imaging. We propose an encoder-decoder CNN named as SNet to learn the transmission map of rain streaks. As rain streaks appear with various shapes and directions, we use ShuffleNet units within SNet to capture their anisotropic representations. As vapors are brought by rain streaks, we propose a VNet containing spatial pyramid pooling (SSP) to predict the transmission map of vapors in multi-scales based on that of rain streaks. Meanwhile, we use an encoder CNN named ANet to estimate atmosphere light. The SNet, VNet, and ANet are jointly trained to predict transmission maps and atmosphere light for rain image restoration. Extensive experiments on the benchmark datasets demonstrate the effectiveness of the proposed visual model to predict rain streaks and vapors. The proposed deraining method performs favorably against state-of-the-art deraining approaches.},
  file        = {:http\://arxiv.org/pdf/2008.00823v1:PDF},
  keywords    = {cs.CV, cs.GR, eess.IV},
}

@Article{Zhang2018,
  author      = {He Zhang and Vishal M. Patel},
  date        = {2018-02-21},
  title       = {Density-aware Single Image De-raining using a Multi-stream Dense Network},
  eprint      = {1802.07412},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Single image rain streak removal is an extremely challenging problem due to the presence of non-uniform rain densities in images. We present a novel density-aware multi-stream densely connected convolutional neural network-based algorithm, called DID-MDN, for joint rain density estimation and de-raining. The proposed method enables the network itself to automatically determine the rain-density information and then efficiently remove the corresponding rain-streaks guided by the estimated rain-density label. To better characterize rain-streaks with different scales and shapes, a multi-stream densely connected de-raining network is proposed which efficiently leverages features from different scales. Furthermore, a new dataset containing images with rain-density labels is created and used to train the proposed density-aware network. Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. In addition, an ablation study is performed to demonstrate the improvements obtained by different modules in the proposed method. Code can be found at: https://github.com/hezhangsprinter},
  file        = {:http\://arxiv.org/pdf/1802.07412v1:PDF},
  keywords    = {cs.CV},
}

@Article{Ignaccolo2009,
  author      = {Massimiliano Ignaccolo and Carlo De Michele},
  date        = {2009-11-20},
  title       = {A non arbitrary definition of rain event: the case of stratiform rain},
  eprint      = {0911.3941},
  eprintclass = {physics.ao-ph},
  eprinttype  = {arXiv},
  abstract    = {A long standing issue in Hydrology is the arbitrariness of the rain "event" definition. In this manuscript, we show that 1) the event definition resting on the occurrence of a minimum rainless period and the one resting a sequence of consecutive wet intervals are statistically equivalent. 2) In the case of stratiform rain, a non arbitrary definition of rain event is possible. The dynamical properties of stratiform rain indicate the range [1.5,4] h as the proper one for the choice of a minimum rainless period for Chilbolton, UK. 3) The intra event dynamical variability is "described" by an alternate sequence of quiescent and active phases.},
  file        = {:http\://arxiv.org/pdf/0911.3941v1:PDF},
  keywords    = {physics.ao-ph},
}

@Article{Yang2016,
  author      = {Wenhan Yang and Robby T. Tan and Jiashi Feng and Jiaying Liu and Zongming Guo and Shuicheng Yan},
  date        = {2016-09-25},
  title       = {Deep Joint Rain Detection and Removal from a Single Image},
  eprint      = {1609.07769},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we address a rain removal problem from a single image, even in the presence of heavy rain and rain streak accumulation. Our core ideas lie in the new rain image models and a novel deep learning architecture. We first modify an existing model comprising a rain streak layer and a background layer, by adding a binary map that locates rain streak regions. Second, we create a new model consisting of a component representing rain streak accumulation (where individual streaks cannot be seen, and thus visually similar to mist or fog), and another component representing various shapes and directions of overlapping rain streaks, which usually happen in heavy rain. Based on the first model, we develop a multi-task deep learning architecture that learns the binary rain streak map, the appearance of rain streaks, and the clean background, which is our ultimate output. The additional binary map is critically beneficial, since its loss function can provide additional strong information to the network. To handle rain streak accumulation (again, a phenomenon visually similar to mist or fog) and various shapes and directions of overlapping rain streaks, we propose a recurrent rain detection and removal network that removes rain streaks and clears up the rain accumulation iteratively and progressively. In each recurrence of our method, a new contextualized dilated network is developed to exploit regional contextual information and outputs better representation for rain detection. The evaluation on real images, particularly on heavy rain, shows the effectiveness of our novel models and architecture, outperforming the state-of-the-art methods significantly. Our codes and data sets will be publicly available.},
  file        = {:http\://arxiv.org/pdf/1609.07769v3:PDF},
  keywords    = {cs.CV},
}

@Article{Wang2020a,
  author      = {Hong Wang and Yichen Wu and Qi Xie and Qian Zhao and Yong Liang and Deyu Meng},
  date        = {2020-05-19},
  title       = {Structural Residual Learning for Single Image Rain Removal},
  eprint      = {2005.09228},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {To alleviate the adverse effect of rain streaks in image processing tasks, CNN-based single image rain removal methods have been recently proposed. However, the performance of these deep learning methods largely relies on the covering range of rain shapes contained in the pre-collected training rainy-clean image pairs. This makes them easily trapped into the overfitting-to-the-training-samples issue and cannot finely generalize to practical rainy images with complex and diverse rain streaks. Against this generalization issue, this study proposes a new network architecture by enforcing the output residual of the network possess intrinsic rain structures. Such a structural residual setting guarantees the rain layer extracted by the network finely comply with the prior knowledge of general rain streaks, and thus regulates sound rain shapes capable of being well extracted from rainy images in both training and predicting stages. Such a general regularization function naturally leads to both its better training accuracy and testing generalization capability even for those non-seen rain configurations. Such superiority is comprehensively substantiated by experiments implemented on synthetic and real datasets both visually and quantitatively as compared with current state-of-the-art methods.},
  file        = {:http\://arxiv.org/pdf/2005.09228v1:PDF},
  keywords    = {eess.IV, cs.CV},
}

@Article{Wu2019,
  author      = {Qingbo Wu and Lei Wang and King N. Ngan and Hongliang Li and Fanman Meng and Linfeng Xu},
  date        = {2019-09-26},
  title       = {Subjective and Objective De-raining Quality Assessment Towards Authentic Rain Image},
  eprint      = {1909.11983},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Images acquired by outdoor vision systems easily suffer poor visibility and annoying interference due to the rainy weather, which brings great challenge for accurately understanding and describing the visual contents. Recent researches have devoted great efforts on the task of rain removal for improving the image visibility. However, there is very few exploration about the quality assessment of de-rained image, even it is crucial for accurately measuring the performance of various de-raining algorithms. In this paper, we first create a de-raining quality assessment (DQA) database that collects 206 authentic rain images and their de-rained versions produced by 6 representative single image rain removal algorithms. Then, a subjective study is conducted on our DQA database, which collects the subject-rated scores of all de-rained images. To quantitatively measure the quality of de-rained image with non-uniform artifacts, we propose a bi-directional feature embedding network (B-FEN) which integrates the features of global perception and local difference together. Experiments confirm that the proposed method significantly outperforms many existing universal blind image quality assessment models. To help the research towards perceptually preferred de-raining algorithm, we will publicly release our DQA database and B-FEN source code on https://github.com/wqb-uestc.},
  file        = {:http\://arxiv.org/pdf/1909.11983v3:PDF},
  keywords    = {eess.IV, cs.CV},
}

@Article{Li2018,
  author      = {Xia Li and Jianlong Wu and Zhouchen Lin and Hong Liu and Hongbin Zha},
  date        = {2018-07-16},
  title       = {Recurrent Squeeze-and-Excitation Context Aggregation Net for Single Image Deraining},
  eprint      = {1807.05698},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain streaks can severely degrade the visibility, which causes many current computer vision algorithms fail to work. So it is necessary to remove the rain from images. We propose a novel deep network architecture based on deep convolutional and recurrent neural networks for single image deraining. As contextual information is very important for rain removal, we first adopt the dilated convolutional neural network to acquire large receptive field. To better fit the rain removal task, we also modify the network. In heavy rain, rain streaks have various directions and shapes, which can be regarded as the accumulation of multiple rain streak layers. We assign different alpha-values to various rain streak layers according to the intensity and transparency by incorporating the squeeze-and-excitation block. Since rain streak layers overlap with each other, it is not easy to remove the rain in one stage. So we further decompose the rain removal into multiple stages. Recurrent neural network is incorporated to preserve the useful information in previous stages and benefit the rain removal in later stages. We conduct extensive experiments on both synthetic and real-world datasets. Our proposed method outperforms the state-of-the-art approaches under all evaluation metrics. Codes and supplementary material are available at our project webpage: https://xialipku.github.io/RESCAN .},
  file        = {:http\://arxiv.org/pdf/1807.05698v2:PDF},
  keywords    = {cs.CV},
}

@Article{Shen2018,
  author      = {Liang Shen and Zihan Yue and Quan Chen and Fan Feng and Jie Ma},
  date        = {2018-01-21},
  title       = {Deep joint rain and haze removal from single images},
  eprint      = {1801.06769},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain removal from a single image is a challenge which has been studied for a long time. In this paper, a novel convolutional neural network based on wavelet and dark channel is proposed. On one hand, we think that rain streaks correspond to high frequency component of the image. Therefore, haar wavelet transform is a good choice to separate the rain streaks and background to some extent. More specifically, the LL subband of a rain image is more inclined to express the background information, while LH, HL, HH subband tend to represent the rain streaks and the edges. On the other hand, the accumulation of rain streaks from long distance makes the rain image look like haze veil. We extract dark channel of rain image as a feature map in network. By increasing this mapping between the dark channel of input and output images, we achieve haze removal in an indirect way. All of the parameters are optimized by back-propagation. Experiments on both synthetic and real- world datasets reveal that our method outperforms other state-of- the-art methods from a qualitative and quantitative perspective.},
  file        = {:http\://arxiv.org/pdf/1801.06769v1:PDF},
  keywords    = {cs.CV},
}

@Article{Zhai2020,
  author      = {Liming Zhai and Felix Juefei-Xu and Qing Guo and Xiaofei Xie and Lei Ma and Wei Feng and Shengchao Qin and Yang Liu},
  date        = {2020-09-19},
  title       = {It's Raining Cats or Dogs? Adversarial Rain Attack on DNN Perception},
  eprint      = {2009.09205},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain is a common phenomenon in nature and an essential factor for many deep neural network (DNN) based perception systems. Rain can often post inevitable threats that must be carefully addressed especially in the context of safety and security-sensitive scenarios (e.g., autonomous driving). Therefore, a comprehensive investigation of the potential risks of the rain to a DNN is of great importance. Unfortunately, in practice, it is often rather difficult to collect or synthesize rainy images that can represent all raining situations that possibly occur in the real world. To this end, in this paper, we start from a new perspective and propose to combine two totally different studies, i.e., rainy image synthesis and adversarial attack. We present an adversarial rain attack, with which we could simulate various rainy situations with the guidance of deployed DNNs and reveal the potential threat factors that can be brought by rain, helping to develop more rain-robust DNNs. In particular, we propose a factor-aware rain generation that simulates rain steaks according to the camera exposure process and models the learnable rain factors for adversarial attack. With this generator, we further propose the adversarial rain attack against the image classification and object detection, where the rain factors are guided by the various DNNs. As a result, it enables to comprehensively study the impacts of the rain factors to DNNs. Our largescale evaluation on three datasets, i.e., NeurIPS'17 DEV, MS COCO and KITTI, demonstrates that our synthesized rainy images can not only present visually realistic appearances, but also exhibit strong adversarial capability, which builds the foundation for further rain-robust perception studies.},
  file        = {:http\://arxiv.org/pdf/2009.09205v1:PDF},
  keywords    = {cs.CV, cs.CR},
}

@Article{Bianchi2018,
  author      = {Blandine Bianchi and Peter Jan van Leeuwen and Robin J. Hogan and Alexis Berne},
  date        = {2018-10-28},
  title       = {Rainfall nowcasting by combining radars, microwave links and rain gauges},
  eprint      = {1810.11811},
  eprintclass = {physics.ao-ph},
  eprinttype  = {arXiv},
  abstract    = {The objective of this work is to provide high-resolution rain rate maps at short lead-time forecasts (nowcasts) necessary to anticipate flooding and properly manage sewage systems in urban areas by combining radars, rain gauges, and operational microwave links, and taking into account their respective uncertainties. A variational approach (3D-Var) is used to find the best estimate for the rain rate, and its error covariance, from the different rain sensors. Short-term rain rate forecasts are then produced by assuming Lagrangian persistence. A velocity field is obtained from the operational radar-derived rain fields, and the rain rate field is advected using the Total Variance Diminishing (TVD) scheme. The error covariance associated to the estimated rain rate is also propagated, and we use these two in the 3D-Var at the next observation time step. This approach can be seen as a Variational Kalman Filter (VKF), in which the covariance of the prior is not constant but dependent on time. The proposed approach has been tested using data from 14 rain gauges, 14 microwave links and the operational radar rain product from MeteoSwiss in the area of Zurich (Switzerland). During the applications the assumption of the Lagrangian persistence appears to be valid up to 20 min (a bit longer for stratiform events). During convective events, the algorithm is less powerful and shorter lead times should be considered (i.e., 15 min). Although such lead times are short, they are still useful to various hydrological and outdoor applications.},
  file        = {:http\://arxiv.org/pdf/1810.11811v1:PDF},
  keywords    = {physics.ao-ph},
}

@Article{Li2017,
  author      = {Ruoteng Li and Robby T. Tan and Loong-Fah Cheong},
  date        = {2017-04-18},
  title       = {Robust Optical Flow Estimation in Rainy Scenes},
  eprint      = {1704.05239},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Optical flow estimation in the rainy scenes is challenging due to background degradation introduced by rain streaks and rain accumulation effects in the scene. Rain accumulation effect refers to poor visibility of remote objects due to the intense rainfall. Most existing optical flow methods are erroneous when applied to rain sequences because the conventional brightness constancy constraint (BCC) and gradient constancy constraint (GCC) generally break down in this situation. Based on the observation that the RGB color channels receive raindrop radiance equally, we introduce a residue channel as a new data constraint to reduce the effect of rain streaks. To handle rain accumulation, our method decomposes the image into a piecewise-smooth background layer and a high-frequency detail layer. It also enforces the BCC on the background layer only. Results on both synthetic dataset and real images show that our algorithm outperforms existing methods on different types of rain sequences. To our knowledge, this is the first optical flow method specifically dealing with rain.},
  file        = {:http\://arxiv.org/pdf/1704.05239v2:PDF},
  keywords    = {cs.CV},
}

@Article{Liu2018,
  author      = {Hong Liu and Hanrong Ye and Xia Li and Wei Shi and Mengyuan Liu and Qianru Sun},
  date        = {2018-11-06},
  title       = {Self-Refining Deep Symmetry Enhanced Network for Rain Removal},
  eprint      = {1811.04761},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Rain removal aims to remove the rain streaks on rain images. The state-of-the-art methods are mostly based on Convolutional Neural Network~(CNN). However, as CNN is not equivariant to object rotation, these methods are unsuitable for dealing with the tilted rain streaks. To tackle this problem, we propose Deep Symmetry Enhanced Network~(DSEN) that is able to explicitly extract the rotation equivariant features from rain images. In addition, we design a self-refining mechanism to remove the accumulated rain streaks in a coarse-to-fine manner. This mechanism reuses DSEN with a novel information link which passes the gradient flow to the higher stages. Extensive experiments on both synthetic and real-world rain images show that our self-refining DSEN yields the top performance.},
  file        = {:http\://arxiv.org/pdf/1811.04761v3:PDF},
  keywords    = {eess.IV},
}

@Article{Lin2019,
  author      = {Huangxing Lin and Yanlong Li and Xinghao Ding and Weihong Zeng and Yue Huang and John Paisley},
  date        = {2019-04-09},
  title       = {Rain O'er Me: Synthesizing real rain to derain with data distillation},
  doi         = {10.1109/TIP.2020.3005517},
  eprint      = {1904.04605},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {We present a supervised technique for learning to remove rain from images without using synthetic rain software. The method is based on a two-stage data distillation approach: 1) A rainy image is first paired with a coarsely derained version using on a simple filtering technique ("rain-to-clean"). 2) Then a clean image is randomly matched with the rainy soft-labeled pair. Through a shared deep neural network, the rain that is removed from the first image is then added to the clean image to generate a second pair ("clean-to-rain"). The neural network simultaneously learns to map both images such that high resolution structure in the clean images can inform the deraining of the rainy images. Demonstrations show that this approach can address those visual characteristics of rain not easily synthesized by software in the usual way.},
  file        = {:http\://arxiv.org/pdf/1904.04605v2:PDF},
  keywords    = {cs.CV},
}

@Article{Yasarla2019,
  author      = {Rajeev Yasarla and Vishal M. Patel},
  date        = {2019-09-10},
  title       = {Confidence Measure Guided Single Image De-raining},
  doi         = {10.1109/TIP.2020.2973802},
  eprint      = {1909.04207},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Single image de-raining is an extremely challenging problem since the rainy images contain rain streaks which often vary in size, direction and density. This varying characteristic of rain streaks affect different parts of the image differently. Previous approaches have attempted to address this problem by leveraging some prior information to remove rain streaks from a single image. One of the major limitations of these approaches is that they do not consider the location information of rain drops in the image. The proposed Image Quality-based single image Deraining using Confidence measure (QuDeC), network addresses this issue by learning the quality or distortion level of each patch in the rainy image, and further processes this information to learn the rain content at different scales. In addition, we introduce a technique which guides the network to learn the network weights based on the confidence measure about the estimate of both quality at each location and residual rain streak information (residual map). Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods.},
  file        = {:http\://arxiv.org/pdf/1909.04207v1:PDF},
  keywords    = {eess.IV, cs.CV},
}

@Article{He2020,
  author      = {Jingwei He and Lei Yu and Gui-Song Xia and Wen Yang},
  date        = {2020-06-05},
  title       = {Single Image Deraining with Continuous Rain Density Estimation},
  eprint      = {2006.03190},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Single image deraining (SIDR) often suffers from over/under deraining due to the nonuniformity of rain densities and the variety of raindrop scales. In this paper, we propose a \textbf{\it co}ntinuous \textbf{\it de}nsity guided network (CODE-Net) for SIDR. Particularly, it is composed of { a rain {\color{black}streak} extractor and a denoiser}, where the convolutional sparse coding (CSC) is exploited to filter out noises from the extracted rain streaks. Inspired by the reweighted iterative soft-threshold for CSC, we address the problem of continuous rain density estimation by learning the weights with channel attention blocks from sparse codes. We further {\color{black}develop} a multiscale strategy to depict rain streaks appearing at different scales. Experiments on synthetic and real-world data demonstrate the superiority of our methods over recent {\color{black}state of the arts}, in terms of both quantitative and qualitative results. Additionally, instead of quantizing rain density with several levels, our CODE-Net can provide continuous-valued estimations of rain densities, which is more desirable in real applications.},
  file        = {:http\://arxiv.org/pdf/2006.03190v1:PDF},
  keywords    = {eess.IV},
}

@Article{Islam2020,
  author      = {Muhammad Rafiqul Islam and Manoranjan Paul},
  date        = {2020-07-10},
  title       = {Rain Streak Removal in a Video to Improve Visibility by TAWL Algorithm},
  eprint      = {2007.05167},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {In computer vision applications, the visibility of the video content is crucial to perform analysis for better accuracy. The visibility can be affected by several atmospheric interferences in challenging weather-one of them is the appearance of rain streak. In recent time, rain streak removal achieves lots of interest to the researchers as it has some exciting applications such as autonomous car, intelligent traffic monitoring system, multimedia, etc. In this paper, we propose a novel and simple method by combining three novel extracted features focusing on temporal appearance, wide shape and relative location of the rain streak and we called it TAWL (Temporal Appearance, Width, and Location) method. The proposed TAWL method adaptively uses features from different resolutions and frame rates. Moreover, it progressively processes features from the up-coming frames so that it can remove rain in the real-time. The experiments have been conducted using video sequences with both real rains and synthetic rains to compare the performance of the proposed method against the relevant state-of-the-art methods. The experimental results demonstrate that the proposed method outperforms the state-of-the-art methods by removing more rain streaks while keeping other moving regions.},
  file        = {:http\://arxiv.org/pdf/2007.05167v1:PDF},
  keywords    = {eess.IV, cs.CV},
}

@Article{Wang2020b,
  author      = {Zheng Wang and Jianwu Li and Ge Song},
  date        = {2020-08-21},
  title       = {DTDN: Dual-task De-raining Network},
  doi         = {10.1145/3343031.3350945},
  eprint      = {2008.09326},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Removing rain streaks from rainy images is necessary for many tasks in computer vision, such as object detection and recognition. It needs to address two mutually exclusive objectives: removing rain streaks and reserving realistic details. Balancing them is critical for de-raining methods. We propose an end-to-end network, called dual-task de-raining network (DTDN), consisting of two sub-networks: generative adversarial network (GAN) and convolutional neural network (CNN), to remove rain streaks via coordinating the two mutually exclusive objectives self-adaptively. DTDN-GAN is mainly used to remove structural rain streaks, and DTDN-CNN is designed to recover details in original images. We also design a training algorithm to train these two sub-networks of DTDN alternatively, which share same weights but use different training sets. We further enrich two existing datasets to approximate the distribution of real rain streaks. Experimental results show that our method outperforms several recent state-of-the-art methods, based on both benchmark testing datasets and real rainy images.},
  file        = {:http\://arxiv.org/pdf/2008.09326v1:PDF},
  keywords    = {eess.IV, cs.CV, cs.MM, I.4.3},
}

@Article{Zhang2021,
  author      = {Kaihao Zhang and Dongxu Li and Wenhan Luo and Wenqi Ren and Lin Ma and Hongdong Li},
  date        = {2021-03-12},
  title       = {Dual Attention-in-Attention Model for Joint Rain Streak and Raindrop Removal},
  eprint      = {2103.07051},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain streaks and rain drops are two natural phenomena, which degrade image capture in different ways. Currently, most existing deep deraining networks take them as two distinct problems and individually address one, and thus cannot deal adequately with both simultaneously. To address this, we propose a Dual Attention-in-Attention Model (DAiAM) which includes two DAMs for removing both rain streaks and raindrops. Inside the DAM, there are two attentive maps - each of which attends to the heavy and light rainy regions, respectively, to guide the deraining process differently for applicable regions. In addition, to further refine the result, a Differential-driven Dual Attention-in-Attention Model (D-DAiAM) is proposed with a "heavy-to-light" scheme to remove rain via addressing the unsatisfying deraining regions. Extensive experiments on one public raindrop dataset, one public rain streak and our synthesized joint rain streak and raindrop (JRSRD) dataset have demonstrated that the proposed method not only is capable of removing rain streaks and raindrops simultaneously, but also achieves the state-of-the-art performance on both tasks.},
  file        = {:http\://arxiv.org/pdf/2103.07051v1:PDF},
  keywords    = {cs.CV},
}

@Article{Hossain2017,
  author      = {S Md Sakir Hossain and Md Atiqul Islam},
  date        = {2017-10-26},
  title       = {Estimation of Rain Attenuation at EHF bands for Earth-to-Satellite Links in Bangladesh},
  doi         = {10.1109/ECACE.2017.7912973},
  eprint      = {1710.09836},
  eprintclass = {eess.SP},
  eprinttype  = {arXiv},
  abstract    = {Due to heavy congestion in lower frequency bands, engineers are looking for new frequency bands to support new services that require higher data rates, which in turn needs broader bandwidths. To meet this requirement, extremely high frequency (EHF), particularly Q (36 to 46 GHz) and V (46 to 56 GHz) bands, is the best viable solution because of its complete availability. The most serious challenge the EHF band poses is the attenuation caused by rain. This paper investigates the effect of the rain on Q and V bands' performances in Bangladeshi climatic conditions. The rain attenuations of the two bands are predicted for the four main regions of Bangladesh using ITU rain attenuation model. The measured rain statistics is used for this prediction. It is observed that the attenuation due to rain in the Q/V band reaches up to 150 dB which is much higher than that of the currently used Ka band. The variability of the rain attenuation is also investigated over different sessions of Bangladesh. The attenuation varies from 40 dB to 170 dB depending on the months. Finally, the amount of rain fade required to compensate the high rain attenuation is also predicted for different elevation angles.},
  file        = {:http\://arxiv.org/pdf/1710.09836v1:PDF},
  keywords    = {eess.SP},
}

@Article{Wei2018,
  author      = {Wei Wei and Deyu Meng and Qian Zhao and Zongben Xu and Ying Wu},
  date        = {2018-07-29},
  title       = {Semi-supervised Transfer Learning for Image Rain Removal},
  eprint      = {1807.11078},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Single image rain removal is a typical inverse problem in computer vision. The deep learning technique has been verified to be effective for this task and achieved state-of-the-art performance. However, previous deep learning methods need to pre-collect a large set of image pairs with/without synthesized rain for training, which tends to make the neural network be biased toward learning the specific patterns of the synthesized rain, while be less able to generalize to real test samples whose rain types differ from those in the training data. To this issue, this paper firstly proposes a semi-supervised learning paradigm toward this task. Different from traditional deep learning methods which only use supervised image pairs with/without synthesized rain, we further put real rainy images, without need of their clean ones, into the network training process. This is realized by elaborately formulating the residual between an input rainy image and its expected network output (clear image without rain) as a specific parametrized rain streaks distribution. The network is therefore trained to adapt real unsupervised diverse rain types through transferring from the supervised synthesized rain, and thus both the short-of-training-sample and bias-to-supervised-sample issues can be evidently alleviated. Experiments on synthetic and real data verify the superiority of our model compared to the state-of-the-arts.},
  file        = {:http\://arxiv.org/pdf/1807.11078v2:PDF},
  keywords    = {cs.CV},
}

@Article{Li2019,
  author      = {Ruotent Li and Loong Fah Cheong and Robby T. Tan},
  date        = {2019-04-10},
  title       = {Heavy Rain Image Restoration: Integrating Physics Model and Conditional Adversarial Learning},
  eprint      = {1904.05050},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Most deraining works focus on rain streaks removal but they cannot deal adequately with heavy rain images. In heavy rain, streaks are strongly visible, dense rain accumulation or rain veiling effect significantly washes out the image, further scenes are relatively more blurry, etc. In this paper, we propose a novel method to address these problems. We put forth a 2-stage network: a physics-based backbone followed by a depth-guided GAN refinement. The first stage estimates the rain streaks, the transmission, and the atmospheric light governed by the underlying physics. To tease out these components more reliably, a guided filtering framework is used to decompose the image into its low- and high-frequency components. This filtering is guided by a rain-free residue image --- its content is used to set the passbands for the two channels in a spatially-variant manner so that the background details do not get mixed up with the rain-streaks. For the second stage, the refinement stage, we put forth a depth-guided GAN to recover the background details failed to be retrieved by the first stage, as well as correcting artefacts introduced by that stage. We have evaluated our method against the state of the art methods. Extensive experiments show that our method outperforms them on real rain image data, recovering visually clean images with good details.},
  file        = {:http\://arxiv.org/pdf/1904.05050v1:PDF},
  keywords    = {cs.CV},
}

@Article{Fan2020,
  author      = {Yulong Fan and Rong Chen and Bo Li},
  date        = {2020-03-21},
  title       = {Multi-Task Learning Enhanced Single Image De-Raining},
  eprint      = {2003.09689},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain removal in images is an important task in computer vision filed and attracting attentions of more and more people. In this paper, we address a non-trivial issue of removing visual effect of rain streak from a single image. Differing from existing work, our method combines various semantic constraint task in a proposed multi-task regression model for rain removal. These tasks reinforce the model's capabilities from the content, edge-aware, and local texture similarity respectively. To further improve the performance of multi-task learning, we also present two simple but powerful dynamic weighting algorithms. The proposed multi-task enhanced network (MENET) is a powerful convolutional neural network based on U-Net for rain removal research, with a specific focus on utilize multiple tasks constraints and exploit the synergy among them to facilitate the model's rain removal capacity. It is noteworthy that the adaptive weighting scheme has further resulted in improved network capability. We conduct several experiments on synthetic and real rain images, and achieve superior rain removal performance over several selected state-of-the-art (SOTA) approaches. The overall effect of our method is impressive, even in the decomposition of heavy rain and rain streak accumulation.The source code and some results can be found at:https://github.com/SumiHui/MENET.},
  file        = {:http\://arxiv.org/pdf/2003.09689v2:PDF},
  keywords    = {cs.CV, eess.IV},
}

@Article{Tremblay2020,
  author      = {Maxime Tremblay and Shirsendu Sukanta Halder and Raoul de Charette and Jean-Franois Lalonde},
  date        = {2020-09-06},
  title       = {Rain rendering for evaluating and improving robustness to bad weather},
  doi         = {10.1007/s11263-020-01366-3},
  eprint      = {2009.03683},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain fills the atmosphere with water particles, which breaks the common assumption that light travels unaltered from the scene to the camera. While it is well-known that rain affects computer vision algorithms, quantifying its impact is difficult. In this context, we present a rain rendering pipeline that enables the systematic evaluation of common computer vision algorithms to controlled amounts of rain. We present three different ways to add synthetic rain to existing images datasets: completely physic-based; completely data-driven; and a combination of both. The physic-based rain augmentation combines a physical particle simulator and accurate rain photometric modeling. We validate our rendering methods with a user study, demonstrating our rain is judged as much as 73% more realistic than the state-of-theart. Using our generated rain-augmented KITTI, Cityscapes, and nuScenes datasets, we conduct a thorough evaluation of object detection, semantic segmentation, and depth estimation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection, 60% for semantic segmentation, and 6-fold increase in depth estimation error. Finetuning on our augmented synthetic data results in improvements of 21% on object detection, 37% on semantic segmentation, and 8% on depth estimation.},
  file        = {:http\://arxiv.org/pdf/2009.03683v1:PDF},
  keywords    = {cs.CV},
}

@Article{Hossain2014,
  author      = {Sakir Hossain},
  date        = {2014-06-19},
  title       = {Rain Attenuation Prediction for Terrestrial Microwave Link in Bangladesh},
  eprint      = {1406.5038},
  eprintclass = {cs.NI},
  eprinttype  = {arXiv},
  abstract    = {Rain attenuation is a major shortcoming of microwave transmission. As a subtropical country, Bangladesh is one of the highest rainy areas of the world. Thus, designing a terrestrial microwave link is a serious challenge to the engineers. In this paper, the annual rain rate and monthly variation of rate are predicted for different percentage of time of the year from the measured rainfall data. Using ITU rain model for terrestrial microwave communication, the rain attenuation is predicted for five major cities of Bangladesh, namely Dhaka, Chittagong, Rajshahi, Sylhet, and Khulna. It is found that rain attenuation is the most severe in Sylhet and least in Rajshahi. The attenuation is estimated for different frequency and polarization. A horizontally polarized signal encounters 15% more rain attenuation than that of vertically polarized signal. It is also found that attenuation in Rajshahi is about 20% lesser than that in Sylhet. Thus, the horizontally polarized transmission in Rajshahi experiences about 5% less attenuation than the vertically polarized transmission in Sylhet.},
  file        = {:http\://arxiv.org/pdf/1406.5038v1:PDF},
  keywords    = {cs.NI},
}

@Article{Weerasinghe2015,
  author       = {R. Weerasinghe and A. S. Pannila and M. K. Jayananda and D. U. J. Sonnadara},
  date         = {2015-08-08},
  journaltitle = {Proceedings of the Technical Sessions, Institute of Physics Sri Lanka, 31 (2015) 39-44},
  title        = {Automated Rain Sampler for Real time pH and Conductivity Measurements},
  eprint       = {1508.01860},
  eprintclass  = {physics.ins-det},
  eprinttype   = {arXiv},
  abstract     = {To monitor the acidity of rain water in real time, a rain water sampling system was developed. The rain sampler detects the initial rain after a dry spell and collects a water sample. Before performing the measurements, the pH probe is calibrated using a standard buffer solution whereas the conductivity probe is calibrated using deionized water. After calibrating the probes the pH and the conductivity of the collected rain water sample are measured using the pH and the conductivity probe. Weather parameters such as air temperature, humidity and pressure are also recorded simultaneously. The pH and conductivity measurement data including weather parameters are transmitted to central station using a GSM modem for further analysis. The collected rain water sample is preserved at the remote monitoring station for post chemical analysis. A programmable logic controller controls the entire process.},
  file         = {:http\://arxiv.org/pdf/1508.01860v1:PDF},
  keywords     = {physics.ins-det},
}

@Article{Chen2018,
  author      = {Jie Chen and Cheen-Hau Tan and Junhui Hou and Lap-Pui Chau and He Li},
  date        = {2018-03-28},
  title       = {Robust Video Content Alignment and Compensation for Rain Removal in a CNN Framework},
  eprint      = {1803.10433},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain removal is important for improving the robustness of outdoor vision based systems. Current rain removal methods show limitations either for complex dynamic scenes shot from fast moving cameras, or under torrential rain fall with opaque occlusions. We propose a novel derain algorithm, which applies superpixel (SP) segmentation to decompose the scene into depth consistent units. Alignment of scene contents are done at the SP level, which proves to be robust towards rain occlusion and fast camera motion. Two alignment output tensors, i.e., optimal temporal match tensor and sorted spatial-temporal match tensor, provide informative clues for rain streak location and occluded background contents to generate an intermediate derain output. These tensors will be subsequently prepared as input features for a convolutional neural network to restore high frequency details to the intermediate output for compensation of mis-alignment blur. Extensive evaluations show that up to 5 dB reconstruction PSNR advantage is achieved over state-of-the-art methods. Visual inspection shows that much cleaner rain removal is achieved especially for highly dynamic scenes with heavy and opaque rainfall from a fast moving camera.},
  file        = {:http\://arxiv.org/pdf/1803.10433v1:PDF},
  keywords    = {cs.CV},
}

@Article{Siyuan2018,
  author      = {Siyuan LI and Wenqi Ren and Jiawan Zhang and Jinke Yu and Xiaojie Guo},
  date        = {2018-04-08},
  title       = {Fast Single Image Rain Removal via a Deep Decomposition-Composition Network},
  eprint      = {1804.02688},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain effect in images typically is annoying for many multimedia and computer vision tasks. For removing rain effect from a single image, deep leaning techniques have been attracting considerable attentions. This paper designs a novel multi-task leaning architecture in an end-to-end manner to reduce the mapping range from input to output and boost the performance. Concretely, a decomposition net is built to split rain images into clean background and rain layers. Different from previous architectures, our model consists of, besides a component representing the desired clean image, an extra component for the rain layer. During the training phase, we further employ a composition structure to reproduce the input by the separated clean image and rain information for improving the quality of decomposition. Experimental results on both synthetic and real images are conducted to reveal the high-quality recovery by our design, and show its superiority over other state-of-the-art methods. Furthermore, our design is also applicable to other layer decomposition tasks like dust removal. More importantly, our method only requires about 50ms, significantly faster than the competitors, to process a testing image in VGA resolution on a GTX 1080 GPU, making it attractive for practical use.},
  file        = {:http\://arxiv.org/pdf/1804.02688v1:PDF},
  keywords    = {cs.CV},
}

@Article{Halder2019,
  author      = {Shirsendu Sukanta Halder and Jean-Franois Lalonde and Raoul de Charette},
  date        = {2019-08-27},
  title       = {Physics-Based Rendering for Improving Robustness to Rain},
  eprint      = {1908.10335},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {To improve the robustness to rain, we present a physically-based rain rendering pipeline for realistically inserting rain into clear weather images. Our rendering relies on a physical particle simulator, an estimation of the scene lighting and an accurate rain photometric modeling to augment images with arbitrary amount of realistic rain or fog. We validate our rendering with a user study, proving our rain is judged 40% more realistic that state-of-the-art. Using our generated weather augmented Kitti and Cityscapes dataset, we conduct a thorough evaluation of deep object detection and semantic segmentation algorithms and show that their performance decreases in degraded weather, on the order of 15% for object detection and 60% for semantic segmentation. Furthermore, we show refining existing networks with our augmented images improves the robustness of both object detection and semantic segmentation algorithms. We experiment on nuScenes and measure an improvement of 15% for object detection and 35% for semantic segmentation compared to original rainy performance. Augmented databases and code are available on the project page.},
  file        = {:http\://arxiv.org/pdf/1908.10335v1:PDF},
  keywords    = {cs.CV, cs.GR, cs.LG, eess.IV},
}

@Article{Shen2020,
  author      = {Yiyang Shen and Yidan Feng and Sen Deng and Dong Liang and Jing Qin and Haoran Xie and Mingqiang Wei},
  date        = {2020-05-21},
  title       = {MBA-RainGAN: Multi-branch Attention Generative Adversarial Network for Mixture of Rain Removal from Single Images},
  eprint      = {2005.10582},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain severely hampers the visibility of scene objects when images are captured through glass in heavily rainy days. We observe three intriguing phenomenons that, 1) rain is a mixture of raindrops, rain streaks and rainy haze; 2) the depth from the camera determines the degrees of object visibility, where objects nearby and faraway are visually blocked by rain streaks and rainy haze, respectively; and 3) raindrops on the glass randomly affect the object visibility of the whole image space. We for the first time consider that, the overall visibility of objects is determined by the mixture of rain (MOR). However, existing solutions and established datasets lack full consideration of the MOR. In this work, we first formulate a new rain imaging model; by then, we enrich the popular RainCityscapes by considering raindrops, named RainCityscapes++. Furthermore, we propose a multi-branch attention generative adversarial network (termed an MBA-RainGAN) to fully remove the MOR. The experiment shows clear visual and numerical improvements of our approach over the state-of-the-arts on RainCityscapes++. The code and dataset will be available.},
  file        = {:http\://arxiv.org/pdf/2005.10582v2:PDF},
  keywords    = {cs.CV},
}

@Article{Wang2018b,
  author      = {Ye-Tao Wang and Xi-Le Zhao and Tai-Xiang Jiang and Liang-Jian Deng and Yi Chang and Ting-Zhu Huang},
  date        = {2018-08-26},
  title       = {Rain Streak Removal for Single Image via Kernel Guided CNN},
  doi         = {10.1109/TNNLS.2020.3015897},
  eprint      = {1808.08545},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain streak removal is an important issue and has recently been investigated extensively. Existing methods, especially the newly emerged deep learning methods, could remove the rain streaks well in many cases. However the essential factor in the generative procedure of the rain streaks, i.e., the motion blur, which leads to the line pattern appearances, were neglected by the deep learning rain streaks approaches and this resulted in over-derain or under-derain results. In this paper, we propose a novel rain streak removal approach using a kernel guided convolutional neural network (KGCNN), achieving the state-of-the-art performance with simple network architectures. We first model the rain streak interference with its motion blur mechanism. Then, our framework starts with learning the motion blur kernel, which is determined by two factors including angle and length, by a plain neural network, denoted as parameter net, from a patch of the texture component. Then, after a dimensionality stretching operation, the learned motion blur kernel is stretched into a degradation map with the same spatial size as the rainy patch. The stretched degradation map together with the texture patch is subsequently input into a derain convolutional network, which is a typical ResNet architecture and trained to output the rain streaks with the guidance of the learned motion blur kernel. Experiments conducted on extensive synthetic and real data demonstrate the effectiveness of the proposed method, which preserves the texture and the contrast while removing the rain streaks.},
  file        = {:http\://arxiv.org/pdf/1808.08545v2:PDF},
  keywords    = {cs.CV},
}

@Article{Dickman2002,
  author      = {Ronald Dickman},
  date        = {2002-10-15},
  title       = {Rain, power laws, and advection},
  doi         = {10.1103/PhysRevLett.90.108701},
  eprint      = {cond-mat/0210327},
  eprintclass = {cond-mat.stat-mech},
  eprinttype  = {arXiv},
  abstract    = {Localized rain events have been found to follow power-law size and duration distributions over several decades, suggesting parallels between precipitation and seismic activity [O. Peters et al., PRL 88, 018701 (2002)]. Similar power laws are generated by treating rain as a passive tracer undergoing advection in a velocity field generated by a two-dimensional system of point vortices.},
  file        = {:http\://arxiv.org/pdf/cond-mat/0210327v1:PDF},
  keywords    = {cond-mat.stat-mech, physics.ao-ph},
}

@Article{Assis2010,
  author      = {Armando V. D. B. Assis},
  date        = {2010-11-27},
  title       = {Relativistically speaking: Let's walk or run through the rain?},
  eprint      = {1011.5955},
  eprintclass = {physics.pop-ph},
  eprinttype  = {arXiv},
  abstract    = {We analyse under a simple approach the problem one must decide the best strategy to minimize the contact with rain when moving between two points through the rain. The available strategies: walk (low speed boost $<<$ $c$) or run (relativistic speed boost $\approx$ $c$).},
  file        = {:http\://arxiv.org/pdf/1011.5955v2:PDF},
  keywords    = {physics.pop-ph},
}

@Article{Li2019a,
  author      = {Minghan Li and Xiangyong Cao and Qian Zhao and Lei Zhang and Chenqiang Gao and Deyu Meng},
  date        = {2019-09-13},
  title       = {Video Rain/Snow Removal by Transformed Online Multiscale Convolutional Sparse Coding},
  eprint      = {1909.06148},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Video rain/snow removal from surveillance videos is an important task in the computer vision community since rain/snow existed in videos can severely degenerate the performance of many surveillance system. Various methods have been investigated extensively, but most only consider consistent rain/snow under stable background scenes. Rain/snow captured from practical surveillance camera, however, is always highly dynamic in time with the background scene transformed occasionally. To this issue, this paper proposes a novel rain/snow removal approach, which fully considers dynamic statistics of both rain/snow and background scenes taken from a video sequence. Specifically, the rain/snow is encoded as an online multi-scale convolutional sparse coding (OMS-CSC) model, which not only finely delivers the sparse scattering and multi-scale shapes of real rain/snow, but also well encodes their temporally dynamic configurations by real-time ameliorated parameters in the model. Furthermore, a transformation operator imposed on the background scenes is further embedded into the proposed model, which finely conveys the dynamic background transformations, such as rotations, scalings and distortions, inevitably existed in a real video sequence. The approach so constructed can naturally better adapt to the dynamic rain/snow as well as background changes, and also suitable to deal with the streaming video attributed its online learning mode. The proposed model is formulated in a concise maximum a posterior (MAP) framework and is readily solved by the ADMM algorithm. Compared with the state-of-the-art online and offline video rain/snow removal methods, the proposed method achieves better performance on synthetic and real videos datasets both visually and quantitatively. Specifically, our method can be implemented in relatively high efficiency, showing its potential to real-time video rain/snow removal.},
  file        = {:http\://arxiv.org/pdf/1909.06148v1:PDF},
  keywords    = {cs.CV, eess.IV},
}

@Article{Aoki2016,
  author      = {Paul M. Aoki},
  date        = {2016-09-01},
  title       = {New Rain Rate Statistics for Emerging Regions: Implications for Wireless Backhaul Planning},
  eprint      = {1609.00426},
  eprintclass = {cs.NI},
  eprinttype  = {arXiv},
  abstract    = {As demand for broadband service increases in emerging regions, high-capacity wireless links can accelerate and cost-reduce the deployment of new networks (both backhaul and customer site connection). Such links are increasingly common in developed countries, but their reliability in emerging regions is questioned where very heavy tropical rain is present. Here, we investigate the robustness of the standard (ITU-R P.837-6) method for estimating rain rates using an expanded test dataset. We illustrate how bias/variance issues cause problematic predictions at higher rain rates. We confirm (by construction) that an improved rainfall climatology can largely address these prediction issues without compromising standard ITU fit evaluation metrics.},
  file        = {:http\://arxiv.org/pdf/1609.00426v1:PDF},
  keywords    = {cs.NI, C.2.1; J.2},
}

@Article{Cassidy1996,
  author      = {I. Cassidy and D. J. Raine},
  date        = {1996-02-15},
  title       = {The Broad Line Regions of NGC 4151},
  eprint      = {astro-ph/9602074},
  eprintclass = {astro-ph},
  eprinttype  = {arXiv},
  abstract    = {We compute the line strengths and profiles from a model of the cloud dynamics of the broad and intermediate line regions and compare the results with the observations of the Seyfert galaxy NGC 4151 in its various luminosity states.},
  file        = {:http\://arxiv.org/pdf/astro-ph/9602074v1:PDF},
  keywords    = {astro-ph},
}

@Article{Rains2000,
  author      = {Eric M. Rains},
  date        = {2000-06-13},
  title       = {Correlation functions for symmetrized increasing subsequences},
  eprint      = {math/0006097},
  eprintclass = {math.CO},
  eprinttype  = {arXiv},
  abstract    = {We show that the correlation functions associated to symmetrized increasing subsequence problems can be expressed as pfaffians of certain antisymmetric matrix kernels, thus generalizing the result of math.RT/9907127 for the unsymmetrized case.},
  file        = {:http\://arxiv.org/pdf/math/0006097v1:PDF},
  keywords    = {math.CO, math.PR},
}

@Article{Xia2017,
  author       = {Chun Xia and Rony Keppens and Xia Fang},
  date         = {2017-06-06},
  journaltitle = {A&A 603, A42 (2017)},
  title        = {Coronal rain in magnetic bipolar weak fields},
  doi          = {10.1051/0004-6361/201730660},
  eprint       = {1706.01804},
  eprintclass  = {astro-ph.SR},
  eprinttype   = {arXiv},
  abstract     = {We intend to investigate the underlying physics for the coronal rain phenomenon in a representative bipolar magnetic field, including the formation and the dynamics of coronal rain blobs. With the MPI-AMRVAC code, we performed three dimensional radiative magnetohydrodynamic (MHD) simulation with strong heating localized on footpoints of magnetic loops after a relaxation to quiet solar atmosphere. Progressive cooling and in-situ condensation starts at the loop top due to radiative thermal instability. The first large-scale condensation on the loop top suffers Rayleigh-Taylor instability and becomes fragmented into smaller blobs. The blobs fall vertically dragging magnetic loops until they reach low beta regions and start to fall along the loops from loop top to loop footpoints. A statistic study of the coronal rain blobs finds that small blobs with masses of less than 10^10 g dominate the population. When blobs fall to lower regions along the magnetic loops, they are stretched and develop a non-uniform velocity pattern with an anti-parallel shearing pattern seen to develop along the central axis of the blobs. Synthetic images of simulated coronal rain with Solar Dynamics Observatory Atmospheric Imaging Assembly well resemble real observations presenting dark falling clumps in hot channels and bright rain blobs in a cool channel. We also find density inhomogeneities during a coronal rain "shower", which reflects the observed multi-stranded nature of coronal rain.},
  file         = {:http\://arxiv.org/pdf/1706.01804v1:PDF},
  keywords     = {astro-ph.SR},
}

@Article{Li2017a,
  author      = {Ruoteng Li and Loong-Fah Cheong and Robby T. Tan},
  date        = {2017-12-19},
  title       = {Single Image Deraining using Scale-Aware Multi-Stage Recurrent Network},
  eprint      = {1712.06830},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Given a single input rainy image, our goal is to visually remove rain streaks and the veiling effect caused by scattering and transmission of rain streaks and rain droplets. We are particularly concerned with heavy rain, where rain streaks of various sizes and directions can overlap each other and the veiling effect reduces contrast severely. To achieve our goal, we introduce a scale-aware multi-stage convolutional neural network. Our main idea here is that different sizes of rain-streaks visually degrade the scene in different ways. Large nearby streaks obstruct larger regions and are likely to reflect specular highlights more prominently than smaller distant streaks. These different effects of different streaks have their own characteristics in their image features, and thus need to be treated differently. To realize this, we create parallel sub-networks that are trained and made aware of these different scales of rain streaks. To our knowledge, this idea of parallel sub-networks that treats the same class of objects according to their unique sub-classes is novel, particularly in the context of rain removal. To verify our idea, we conducted experiments on both synthetic and real images, and found that our method is effective and outperforms the state-of-the-art methods.},
  file        = {:http\://arxiv.org/pdf/1712.06830v1:PDF},
  keywords    = {cs.CV},
}

@Article{Wang2019a,
  author      = {Yinglong Wang and Dong Gong and Jie Yang and Qinfeng Shi and Anton van den Hengel and Dehua Xie and Bing Zeng},
  date        = {2019-05-14},
  title       = {An Effective Two-Branch Model-Based Deep Network for Single Image Deraining},
  eprint      = {1905.05404},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Removing rain effects from an image is of importance for various applications such as autonomous driving, drone piloting, and photo editing. Conventional methods rely on some heuristics to handcraft various priors to remove or separate the rain effects from an image. Recent deep learning models are proposed to learn end-to-end methods to complete this task. However, they often fail to obtain satisfactory results in many realistic scenarios, especially when the observed images suffer from heavy rain. Heavy rain brings not only rain streaks but also haze-like effect caused by the accumulation of tiny raindrops. Different from the existing deep learning deraining methods that mainly focus on handling the rain streaks, we design a deep neural network by incorporating a physical raining image model. Specifically, in the proposed model, two branches are designed to handle both the rain streaks and haze-like effects. An additional submodule is jointly trained to finally refine the results, which give the model flexibility to control the strength of removing the mist. Extensive experiments on several datasets show that our method outperforms the state-of-the-art in both objective assessments and visual quality.},
  file        = {:http\://arxiv.org/pdf/1905.05404v2:PDF},
  keywords    = {cs.CV},
}

@Article{Wang2019b,
  author      = {Yinglong Wang and Qinfeng Shi and Ehsan Abbasnejad and Chao Ma and Xiaoping Ma and Bing Zeng},
  date        = {2019-06-22},
  title       = {Deep Single Image Deraining Via Estimating Transmission and Atmospheric Light in rainy Scenes},
  eprint      = {1906.09433},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain removal in images/videos is still an important task in computer vision field and attracting attentions of more and more people. Traditional methods always utilize some incomplete priors or filters (e.g. guided filter) to remove rain effect. Deep learning gives more probabilities to better solve this task. However, they remove rain either by evaluating background from rainy image directly or learning a rain residual first then subtracting the residual to obtain a clear background. No other models are used in deep learning based de-raining methods to remove rain and obtain other information about rainy scenes. In this paper, we utilize an extensively-used image degradation model which is derived from atmospheric scattering principles to model the formation of rainy images and try to learn the transmission, atmospheric light in rainy scenes and remove rain further. To reach this goal, we propose a robust evaluation method of global atmospheric light in a rainy scene. Instead of using the estimated atmospheric light directly to learn a network to calculate transmission, we utilize it as ground truth and design a simple but novel triangle-shaped network structure to learn atmospheric light for every rainy image, then fine-tune the network to obtain a better estimation of atmospheric light during the training of transmission network. Furthermore, more efficient ShuffleNet Units are utilized in transmission network to learn transmission map and the de-raining image is then obtained by the image degradation model. By subjective and objective comparisons, our method outperforms the selected state-of-the-art works.},
  file        = {:http\://arxiv.org/pdf/1906.09433v1:PDF},
  keywords    = {cs.CV},
}

@Article{Yasarla2019a,
  author      = {Rajeev Yasarla and Vishal M. Patel},
  date        = {2019-06-12},
  title       = {Uncertainty Guided Multi-Scale Residual Learning-using a Cycle Spinning CNN for Single Image De-Raining},
  eprint      = {1906.11129},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Single image de-raining is an extremely challenging problem since the rainy image may contain rain streaks which may vary in size, direction and density. Previous approaches have attempted to address this problem by leveraging some prior information to remove rain streaks from a single image. One of the major limitations of these approaches is that they do not consider the location information of rain drops in the image. The proposed Uncertainty guided Multi-scale Residual Learning (UMRL) network attempts to address this issue by learning the rain content at different scales and using them to estimate the final de-rained output. In addition, we introduce a technique which guides the network to learn the network weights based on the confidence measure about the estimate. Furthermore, we introduce a new training and testing procedure based on the notion of cycle spinning to improve the final de-raining performance. Extensive experiments on synthetic and real datasets to demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods. Code is available at: https://github.com/rajeevyasarla/UMRL--using-Cycle-Spinning},
  file        = {:http\://arxiv.org/pdf/1906.11129v1:PDF},
  keywords    = {cs.CV, cs.LG, eess.IV, stat.ML},
}

@Article{Haurum2019,
  author      = {Joakim Bruslund Haurum and Chris H. Bahnsen and Thomas B. Moeslund},
  date        = {2019-08-12},
  title       = {Is it Raining Outside? Detection of Rainfall using General-Purpose Surveillance Cameras},
  eprint      = {1908.04034},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {In integrated surveillance systems based on visual cameras, the mitigation of adverse weather conditions is an active research topic. Within this field, rain removal algorithms have been developed that artificially remove rain streaks from images or video. In order to deploy such rain removal algorithms in a surveillance setting, one must detect if rain is present in the scene. In this paper, we design a system for the detection of rainfall by the use of surveillance cameras. We reimplement the former state-of-the-art method for rain detection and compare it against a modern CNN-based method by utilizing 3D convolutions. The two methods are evaluated on our new AAU Visual Rain Dataset (VIRADA) that consists of 215 hours of general-purpose surveillance video from two traffic crossings. The results show that the proposed 3D CNN outperforms the previous state-of-the-art method by a large margin on all metrics, for both of the traffic crossings. Finally, it is shown that the choice of region-of-interest has a large influence on performance when trying to generalize the investigated methods. The AAU VIRADA dataset and our implementation of the two rain detection algorithms are publicly available at https://bitbucket.org/aauvap/aau-virada.},
  file        = {:http\://arxiv.org/pdf/1908.04034v1:PDF},
  keywords    = {cs.CV},
}

@Article{Wang2019c,
  author      = {Hong Wang and Yichen Wu and Minghan Li and Qian Zhao and Deyu Meng},
  date        = {2019-09-18},
  title       = {A Survey on Rain Removal from Video and Single Image},
  eprint      = {1909.08326},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Rain streaks might severely degenerate the performance of video/image processing tasks. The investigations on rain removal from video or a single image has thus been attracting much research attention in the field of computer vision and pattern recognition, and various methods have been proposed against this task in the recent years. However, there is still not a comprehensive survey paper to summarize current rain removal methods and fairly compare their generalization performance, and especially, still not a off-the-shelf toolkit to accumulate recent representative methods for easy performance comparison and capability evaluation. Aiming at this meaningful task, in this study we present a comprehensive review for current rain removal methods for video and a single image. Specifically, these methods are categorized into model-driven and data-driven approaches, and more elaborate branches of each approach are further introduced. Intrinsic capabilities, especially generalization, of representative state-of-the-art methods of each approach have been evaluated and analyzed by experiments implemented on synthetic and real data both visually and quantitatively. Furthermore, we release a comprehensive repository, including direct links to 74 rain removal papers, source codes of 9 methods for video rain removal and 20 ones for single image rain removal, 19 related project pages, 6 synthetic datasets and 4 real ones, and 4 commonly used image quality metrics, to facilitate reproduction and performance comparison of current existing methods for general users. Some limitations and research issues worthy to be further investigated have also been discussed for future research of this direction.},
  file        = {:http\://arxiv.org/pdf/1909.08326v2:PDF},
  keywords    = {eess.IV, cs.CV},
}

@Article{Qin2019,
  author      = {Xu Qin and Zhilin Wang},
  date        = {2019-12-06},
  title       = {NASNet: A Neuron Attention Stage-by-Stage Net for Single Image Deraining},
  eprint      = {1912.03151},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Images captured under complicated rain conditions often suffer from noticeable degradation of visibility. The rain models generally introduce diversity visibility degradation, which includes rain streak, rain drop as well as rain mist. Numerous existing single image deraining methods focus on the only one type rain model, which does not have strong generalization ability. In this paper, we propose a novel end-to-end Neuron Attention Stage-by-Stage Net (NASNet), which can solve all types of rain model tasks efficiently. For one thing, we pay more attention on the Neuron relationship and propose a lightweight Neuron Attention (NA) architectural mechanism. It can adaptively recalibrate neuron-wise feature responses by modelling interdependencies and mutual influence between neurons. Our NA architecture consists of Depthwise Conv and Pointwise Conv, which has slight computation cost and higher performance than SE block by our contrasted experiments. For another, we propose a stage-by-stage unified pattern network architecture, the stage-by-stage strategy guides the later stage by incorporating the useful information in previous stage. We concatenate and fuse stage-level information dynamically by NA module. Extensive experiments demonstrate that our proposed NASNet significantly outperforms the state-of-theart methods by a large margin in terms of both quantitative and qualitative measures on all six public large-scale datasets for three rain model tasks.},
  file        = {:http\://arxiv.org/pdf/1912.03151v2:PDF},
  keywords    = {eess.IV, cs.CV},
}

@Article{Jiang2020,
  author      = {Kui Jiang and Zhongyuan Wang and Peng Yi and Chen Chen and Baojin Huang and Yimin Luo and Jiayi Ma and Junjun Jiang},
  date        = {2020-03-24},
  title       = {Multi-Scale Progressive Fusion Network for Single Image Deraining},
  eprint      = {2003.10985},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain streaks in the air appear in various blurring degrees and resolutions due to different distances from their positions to the camera. Similar rain patterns are visible in a rain image as well as its multi-scale (or multi-resolution) versions, which makes it possible to exploit such complementary information for rain streak representation. In this work, we explore the multi-scale collaborative representation for rain streaks from the perspective of input image scales and hierarchical deep features in a unified framework, termed multi-scale progressive fusion network (MSPFN) for single image rain streak removal. For similar rain streaks at different positions, we employ recurrent calculation to capture the global texture, thus allowing to explore the complementary and redundant information at the spatial dimension to characterize target rain streaks. Besides, we construct multi-scale pyramid structure, and further introduce the attention mechanism to guide the fine fusion of this correlated information from different scales. This multi-scale progressive fusion strategy not only promotes the cooperative representation, but also boosts the end-to-end training. Our proposed method is extensively evaluated on several benchmark datasets and achieves state-of-the-art results. Moreover, we conduct experiments on joint deraining, detection, and segmentation tasks, and inspire a new research direction of vision task-driven image deraining. The source code is available at \url{https://github.com/kuihua/MSPFN}.},
  file        = {:http\://arxiv.org/pdf/2003.10985v2:PDF},
  keywords    = {cs.CV, cs.LG, eess.IV},
}

@Article{Chivers2020,
  author       = {Benedict Delahaye Chivers and John Wallbank and Steven J. Cole and Ondrej Sebek and Simon Stanley and Matthew Fry and Georgios Leontidis},
  date         = {2020-03-30},
  journaltitle = {Journal of Hydrology 2020},
  title        = {Imputation of missing sub-hourly precipitation data in a large sensor network: a machine learning approach},
  doi          = {10.1016/j.jhydrol.2020.125126},
  eprint       = {2004.11123},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  abstract     = {Precipitation data collected at sub-hourly resolution represents specific challenges for missing data recovery by being largely stochastic in nature and highly unbalanced in the duration of rain vs non-rain. Here we present a two-step analysis utilising current machine learning techniques for imputing precipitation data sampled at 30-minute intervals by devolving the task into (a) the classification of rain or non-rain samples, and (b) regressing the absolute values of predicted rain samples. Investigating 37 weather stations in the UK, this machine learning process produces more accurate predictions for recovering precipitation data than an established surface fitting technique utilising neighbouring rain gauges. Increasing available features for the training of machine learning algorithms increases performance with the integration of weather data at the target site with externally sourced rain gauges providing the highest performance. This method informs machine learning models by utilising information in concurrently collected environmental data to make accurate predictions of missing rain data. Capturing complex non-linear relationships from weakly correlated variables is critical for data recovery at sub-hourly resolutions. Such pipelines for data recovery can be developed and deployed for highly automated and near instantaneous imputation of missing values in ongoing datasets at high temporal resolutions.},
  file         = {:http\://arxiv.org/pdf/2004.11123v2:PDF},
  keywords     = {cs.LG, stat.AP, stat.ML},
}

@Article{Peters2002,
  author       = {Ole Peters and Kim Christensen},
  date         = {2002-04-04},
  journaltitle = {Phys. Rev. E, 66, 036120 (2002)},
  title        = {Rain: Relaxations in the sky},
  doi          = {10.1103/PhysRevE.66.036120},
  eprint       = {cond-mat/0204109},
  eprintclass  = {cond-mat.soft},
  eprinttype   = {arXiv},
  abstract     = {We demonstrate how, from the point of view of energy flow through an open system, rain is analogous to many other relaxational processes in Nature such as earthquakes. By identifying rain events as the basic entities of the phenomenon, we show that the number density of rain events per year is inversely proportional to the released water column raised to the power 1.4. This is the rain-equivalent of the Gutenberg-Richter law for earthquakes. The event durations and the waiting times between events are also characterised by scaling regions, where no typical time scale exists. The Hurst exponent of the rain intensity signal $H = 0.76 > 0.5$. It is valid in the temporal range from minutes up to the full duration of the signal of half a year. All of our findings are consistent with the concept of self-organised criticality, which refers to the tendency of slowly driven non-equilibrium systems towards a state of scale free behaviour.},
  file         = {:http\://arxiv.org/pdf/cond-mat/0204109v1:PDF},
  keywords     = {cond-mat.soft, physics.ao-ph},
}

@Article{Deluca2012,
  author      = {Anna Deluca and Alvaro Corral},
  date        = {2012-12-20},
  title       = {Scale Invariant Events and Dry Spells for Medium Resolution Local Rain Data},
  eprint      = {1212.5533},
  eprintclass = {physics.ao-ph},
  eprinttype  = {arXiv},
  abstract    = {We analyze distributions of rain-event sizes, rain-event durations, and dry-spell durations for data obtained from a network of 20 rain gauges scattered in a region of the NW Mediterranean coast. While power-law distributions model the dry-spell durations with a common exponent 1.50 +- 0.05, density analysis is inconclusive for event sizes and event durations, due to finite size effects. However, we present alternative evidence of the existence of scale invariance in these distributions by means of different data collapses of the distributions. These results are in agreement with the expectations from the Self-Organized Criticality paradigm, and demonstrate that scaling properties of rain events and dry spells can also be observed for medium resolution rain data.},
  file        = {:http\://arxiv.org/pdf/1212.5533v1:PDF},
  keywords    = {physics.ao-ph, physics.data-an, physics.geo-ph, stat.AP},
}

@Article{Fang2013,
  author      = {X. Fang and C. Xia and R. Keppens},
  date        = {2013-06-20},
  title       = {Multidimensional modeling of coronal rain dynamics},
  doi         = {10.1088/2041-8205/771/2/L29},
  eprint      = {1306.4759},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {We present the first multidimensional, magnetohydrodynamic simulations which capture the initial formation and the long-term sustainment of the enigmatic coronal rain phenomenon. We demonstrate how thermal instability can induce a spectacular display of in-situ forming blob-like condensations which then start their intimate ballet on top of initially linear force-free arcades. Our magnetic arcades host chromospheric, transition region, and coronal plasma. Following coronal rain dynamics for over 80 minutes physical time, we collect enough statistics to quantify blob widths, lengths, velocity distributions, and other characteristics which directly match with modern observational knowledge. Our virtual coronal rain displays the deformation of blobs into $V$-shaped like features, interactions of blobs due to mostly pressure-mediated levitations, and gives the first views on blobs which evaporate in situ, or get siphoned over the apex of the background arcade. Our simulations pave the way for systematic surveys of coronal rain showers in true multidimensional settings, to connect parametrized heating prescriptions with rain statistics, ultimately allowing to quantify the coronal heating input.},
  file        = {:http\://arxiv.org/pdf/1306.4759v1:PDF},
  keywords    = {astro-ph.SR, 85-08, J.2},
}

@Article{Sun2019,
  author      = {Zhaoyang Sun and Shengwu Xiong and Ryan Wen Liu},
  date        = {2019-02-19},
  title       = {Directional Regularized Tensor Modeling for Video Rain Streaks Removal},
  eprint      = {1902.07090},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Outdoor videos sometimes contain unexpected rain streaks due to the rainy weather, which bring negative effects on subsequent computer vision applications, e.g., video surveillance, object recognition and tracking, etc. In this paper, we propose a directional regularized tensor-based video deraining model by taking into consideration the arbitrary direction of rain streaks. In particular, the sparsity of rain streaks in spatial and derivative domains, the spatiotemporal sparsity and low-rank property of video background are incorporated into the proposed method. Different from many previous methods under the assumption of vertically falling rain streaks, we consider a more realistic assumption that all the rain streaks in a video fall in an approximately similar arbitrary direction. The resulting complicated optimization problem will be effectively solved through an alternating direction method. Comprehensive experiments on both synthetic and realistic datasets have demonstrated the superiority of the proposed deraining method.},
  file        = {:http\://arxiv.org/pdf/1902.07090v1:PDF},
  keywords    = {cs.CV},
}

@Article{Reep2020,
  author       = {Jeffrey W. Reep and Patrick Antolin and Stephen J. Bradshaw},
  date         = {2020-02-18},
  journaltitle = {ApJ, Vol. 890, pp.100, 2020},
  title        = {Electron Beams Cannot Directly Produce Coronal Rain},
  doi          = {10.3847/1538-4357/ab6bdc},
  eprint       = {2002.07669},
  eprintclass  = {astro-ph.SR},
  eprinttype   = {arXiv},
  abstract     = {Coronal rain is ubiquitous in flare loops, forming shortly after the onset of the solar flare. Rain is thought to be caused by a thermal instability, a localized runaway cooling of material in the corona. The models that demonstrate this require extremely long duration heating on the order of the radiative cooling time, localized near the footpoints of the loops. In flares, electron beams are thought to be the primary energy transport mechanism, driving strong footpoint heating during the impulsive phase that causes evaporation, filling and heating flare loops. Electron beams, however, do not act for a long period of time, and even supposing that they did, their heating would not remain localized at the footpoints. With a series of numerical experiments, we show directly that these two issues mean that electron beams are incapable of causing the formation of rain in flare loops. This result suggests that either there is another mechanism acting in flare loops responsible for rain, or that the modeling of the cooling of flare loops is somehow deficient. To adequately describe flares, the standard model must address this issue to account for the presence of coronal rain.},
  file         = {:http\://arxiv.org/pdf/2002.07669v1:PDF},
  keywords     = {astro-ph.SR},
}

@Article{Zhu2020,
  author      = {Honghe Zhu and Cong Wang and Yajie Zhang and Zhixun Su and Guohui Zhao},
  date        = {2020-03-30},
  title       = {Physical Model Guided Deep Image Deraining},
  eprint      = {2003.13242},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Single image deraining is an urgent task because the degraded rainy image makes many computer vision systems fail to work, such as video surveillance and autonomous driving. So, deraining becomes important and an effective deraining algorithm is needed. In this paper, we propose a novel network based on physical model guided learning for single image deraining, which consists of three sub-networks: rain streaks network, rain-free network, and guide-learning network. The concatenation of rain streaks and rain-free image that are estimated by rain streaks network, rain-free network, respectively, is input to the guide-learning network to guide further learning and the direct sum of the two estimated images is constrained with the input rainy image based on the physical model of rainy image. Moreover, we further develop the Multi-Scale Residual Block (MSRB) to better utilize multi-scale information and it is proved to boost the deraining performance. Quantitative and qualitative experimental results demonstrate that the proposed method outperforms the state-of-the-art deraining methods. The source code will be available at \url{https://supercong94.wixsite.com/supercong94}.},
  file        = {:http\://arxiv.org/pdf/2003.13242v1:PDF},
  keywords    = {eess.IV, cs.CV},
}

@Article{Su2021,
  author      = {Zhipeng Su and Yixiong Zhang and Xiao-Ping Zhang and Feng Qi},
  date        = {2021-03-03},
  title       = {Non-local Channel Aggregation Network for Single Image Rain Removal},
  eprint      = {2103.02488},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain streaks showing in images or videos would severely degrade the performance of computer vision applications. Thus, it is of vital importance to remove rain streaks and facilitate our vision systems. While recent convolutinal neural network based methods have shown promising results in single image rain removal (SIRR), they fail to effectively capture long-range location dependencies or aggregate convolutional channel information simultaneously. However, as SIRR is a highly illposed problem, these spatial and channel information are very important clues to solve SIRR. First, spatial information could help our model to understand the image context by gathering long-range dependency location information hidden in the image. Second, aggregating channels could help our model to concentrate on channels more related to image background instead of rain streaks. In this paper, we propose a non-local channel aggregation network (NCANet) to address the SIRR problem. NCANet models 2D rainy images as sequences of vectors in three directions, namely vertical direction, transverse direction and channel direction. Recurrently aggregating information from all three directions enables our model to capture the long-range dependencies in both channels and spaitials locations. Extensive experiments on both heavy and light rain image data sets demonstrate the effectiveness of the proposed NCANet model.},
  file        = {:http\://arxiv.org/pdf/2103.02488v1:PDF},
  keywords    = {cs.CV},
}

@Article{Ishikawa2020,
  author      = {Ryohtaroh T. Ishikawa and Yukio Katsukawa and Patrick Antolin and Shin Toriumi},
  date        = {2020-03-30},
  title       = {Temporal and Spatial Scales in Coronal Rain Revealed by UV Imaging and Spectroscopic Observations},
  doi         = {10.1007/s11207-020-01617-z},
  eprint      = {2003.13214},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {Coronal rain corresponds to cool and dense clumps in the corona accreting towards the solar surface, and is often observed above solar active regions. They are generally thought to be produced by thermal instability in the corona and their lifetime is limited by the time they take to reach the chromosphere. Although the rain usually fragments into smaller clumps while falling down, their specific spatial and temporal scales remain unclear. In addition, the observational signatures of the impact of the rain with the chromosphere have not been clarified yet. In this study, we investigate the time evolution of velocity and intensity of coronal rain above a sunspot by analyzing coronal images obtained by the AIA onboard the SDO satellite as well as the Slit-Jaw Images (SJIs) and spectral data taken by the IRIS satellite. We identify dark and bright threads moving towards the umbra in AIA images and in SJIs, respectively, and co-spatial chromospheric intensity enhancements and redshifts in three IRIS spectra, Mg II k 2796 Angstrom, Si IV 1394 Angstrom, and C II 1336 Angstrom. The intensity enhancements and coronal rain redshifts occur almost concurrently in all the three lines, which clearly demonstrates the causal relationship with coronal rain. Furthermore, we detect bursty intensity variation with a timescale shorter than 1 minute in Mg II k, Si IV and C II spectra, indicating that a length scale of rain clumps is about 2.7 Mm if we multiply the typical time scale of the busty intensity variation at 30 sec by the rain velocity at 90 $\mathrm{km\ s}^{-1}$. Such rapid enhancements in the IRIS lines are excited within a time lag of 5.6 sec limited by the temporal resolution. These temporal and spatial scales may reflect the physical processes responsible for the rain morphology, and are suggestive of instabilities such as Kelvin-Helmholtz.},
  file        = {:http\://arxiv.org/pdf/2003.13214v1:PDF},
  keywords    = {astro-ph.SR},
}

@Article{Antolin2009,
  author       = {P. Antolin and K. Shibata},
  date         = {2009-10-13},
  journaltitle = {Astrophys.J.716:154-166,2010},
  title        = {Coronal rain as a marker for coronal heating mechanisms},
  doi          = {10.1088/0004-637X/716/1/154},
  eprint       = {0910.2383},
  eprintclass  = {astro-ph.SR},
  eprinttype   = {arXiv},
  abstract     = {Reported observations in H-alpha, Ca II H and K or or other chromospheric lines of coronal rain trace back to the days of the Skylab mission. Offering a high contrast in intensity with respect to the background (either bright in emission if observed at the limb, or dark in absorption if observed on disk) these cool blobs are often observed falling down from high coronal heights above active regions. A physical explanation for this spectacular phenomenon has been put forward thanks to numerical simulations of loops with footpoint concentrated heating, a heating scenario in which cool condensations naturally form in the corona. This effect has been termed 'catastrophic cooling' and is the predominant explanation for coronal rain. In this work we further investigate the link between this phenomenon and the heating mechanisms acting in the corona. We start by analyzing observations of coronal rain at the limb in the Ca II H line performed by the SOT instrument on board of the Hinode satellite. We then compare the observations with 1.5-dimensional MHD simulations of loops being heated by small-scale discrete events concentrated towards the footpoints (that could come, for instance, from magnetic reconnection events), and by Alfven waves generated at the photosphere. It is found that if a loop is heated predominantly from Alfven waves coronal rain is inhibited due to the characteristic uniform heating they produce. Hence coronal rain may not only point to the spatial distribution of the heating in coronal loops but also to the agent of the heating itself. We thus propose coronal rain as a marker for coronal heating mechanisms.},
  file         = {:http\://arxiv.org/pdf/0910.2383v1:PDF},
  keywords     = {astro-ph.SR, astro-ph.EP},
}

@Article{Antolin2011,
  author      = {Patrick Antolin and Erwin Verwichte},
  date        = {2011-05-11},
  title       = {Transverse oscillations of loops with coronal rain observed by Hinode/SOT},
  doi         = {10.1088/0004-637X/736/2/121},
  eprint      = {1105.2175},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {The condensations composing coronal rain, falling down along loop-like structures observed in cool chromospheric lines such as H$\alpha$ and \ion{Ca}{2} H, have long been a spectacular phenomenon of the solar corona. However, considered a peculiar sporadic phenomenon, it has not received much attention. This picture is rapidly changing due to recent high resolution observations with instruments such as \textit{Hinode}/SOT, CRISP of \textit{SST} and \textit{SDO}. Furthermore, numerical simulations have shown that coronal rain is a loss of thermal equilibrium of loops linked to footpoint heating. This result has highlighted the importance that coronal rain can play in the field of coronal heating. In this work, we further stress the importance of coronal rain by showing the role it can play in the understanding of the coronal magnetic field topology. We analyze \textit{Hinode}/SOT observations in the \ion{Ca}{2} H line of a loop in which coronal rain puts in evidence in-phase transverse oscillations of multiple strand-like structures. The periods, amplitudes, transverse velocities and phase velocities are calculated, allowing an estimation of the energy flux of the wave and the coronal magnetic field inside the loop through means of coronal seismology. We discuss the possible interpretations of the wave as either standing or propagating torsional Alfv\'en or fast kink waves. An estimate of the plasma beta parameter of the condensations indicates a condition that may allow the often observed separation and elongation processes of the condensations. We also show that the wave pressure from the transverse wave can be responsible for the observed low downward acceleration of coronal rain.},
  file        = {:http\://arxiv.org/pdf/1105.2175v1:PDF},
  keywords    = {astro-ph.SR, astro-ph.EP},
}

@Article{Antolin2012,
  author      = {P. Antolin and M. Carlsson and L. Rouppe van der Voort and E. Verwichte and G. Vissers},
  date        = {2012-02-03},
  title       = {A Sharp Look at Coronal Rain with Hinode/SOT and SST/CRISP},
  eprint      = {1202.0787},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {The tropical wisdom that when it is hot and dense we can expect rain might also apply to the Sun. Indeed, observations and numerical simulations have shown that strong heating at footpoints of loops, as is the case for active regions, puts their coronae out of thermal equilibrium, which can lead to a phenomenon known as catastrophic cooling. Following local pressure loss in the corona, hot plasma locally condenses in these loops and dramatically cools down to chromospheric temperatures. These blobs become bright in H-alpha and Ca II H in time scales of minutes, and their dynamics seem to be subject more to internal pressure changes in the loop rather than to gravity. They thus become trackers of the magnetic field, which results in the spectacular coronal rain that is observed falling down coronal loops. In this work we report on high resolution observations of coronal rain with the Solar Optical Telescope (SOT) on Hinode and CRISP at the Swedish Solar Telescope (SST). A statistical study is performed in which properties such as velocities and accelerations of coronal rain are derived. We show how this phenomenon can constitute a diagnostic tool for the internal physical conditions inside loops. Furthermore, we analyze transverse oscillations of strand-like condensations composing coronal rain falling in a loop, and discuss the possible nature of the wave. This points to the important role that coronal rain can play in the fields of coronal heating and coronal seismology.},
  file        = {:http\://arxiv.org/pdf/1202.0787v1:PDF},
  keywords    = {astro-ph.SR},
}

@Article{Wei2014,
  author       = {Y. Wei and C. M. Cejas and R. Barrois and R. Dreyfus and D. J. Durian},
  date         = {2014-03-13},
  journaltitle = {Physical Review Applied 2, 044004 (2014)},
  title        = {Morphology of rain water channelization in systematically varied model sandy soils},
  doi          = {10.1103/PhysRevApplied.2.044004},
  eprint       = {1403.3220},
  eprintclass  = {cond-mat.soft},
  eprinttype   = {arXiv},
  abstract     = {We visualize the formation of fingered flow in dry model sandy soils under different raining conditions using a quasi-2d experimental set-up, and systematically determine the impact of soil grain diameter and surface wetting property on water channelization phenomenon. The model sandy soils we use are random closely-packed glass beads with varied diameters and surface treatments. For hydrophilic sandy soils, our experiments show that rain water infiltrates into a shallow top layer of soil and creates a horizontal water wetting front that grows downward homogeneously until instabilities occur to form fingered flows. For hydrophobic sandy soils, in contrast, we observe that rain water ponds on the top of soil surface until the hydraulic pressure is strong enough to overcome the capillary repellency of soil and create narrow water channels that penetrate the soil packing. Varying the raindrop impinging speed has little influence on water channel formation. However, varying the rain rate causes significant changes in water infiltration depth, water channel width, and water channel separation. At a fixed raining condition, we combine the effects of grain diameter and surface hydrophobicity into a single parameter and determine its influence on water infiltration depth, water channel width, and water channel separation. We also demonstrate the efficiency of several soil water improvement methods that relate to rain water channelization phenomenon, including pre-wetting sandy soils at different level before rainfall, modifying soil surface flatness, and applying superabsorbent hydrogel particles as soil modifiers.},
  file         = {:http\://arxiv.org/pdf/1403.3220v1:PDF},
  keywords     = {cond-mat.soft},
}

@Article{Jiang2018,
  author      = {Tai-Xiang Jiang and Ting-Zhu Huang and Xi-Le Zhao and Liang-Jian Deng and Yao Wang},
  date        = {2018-03-20},
  title       = {FastDeRain: A Novel Video Rain Streak Removal Method Using Directional Gradient Priors},
  doi         = {10.1109/TIP.2018.2880512},
  eprint      = {1803.07487},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain streak removal is an important issue in outdoor vision systems and has recently been investigated extensively. In this paper, we propose a novel video rain streak removal approach FastDeRain, which fully considers the discriminative characteristics of rain streaks and the clean video in the gradient domain. Specifically, on the one hand, rain streaks are sparse and smooth along the direction of the raindrops, whereas on the other hand, clean videos exhibit piecewise smoothness along the rain-perpendicular direction and continuity along the temporal direction. Theses smoothness and continuity results in the sparse distribution in the different directional gradient domain, respectively. Thus, we minimize 1) the $\ell_1$ norm to enhance the sparsity of the underlying rain streaks, 2) two $\ell_1$ norm of unidirectional Total Variation (TV) regularizers to guarantee the anisotropic spatial smoothness, and 3) an $\ell_1$ norm of the time-directional difference operator to characterize the temporal continuity. A split augmented Lagrangian shrinkage algorithm (SALSA) based algorithm is designed to solve the proposed minimization model. Experiments conducted on synthetic and real data demonstrate the effectiveness and efficiency of the proposed method. According to comprehensive quantitative performance measures, our approach outperforms other state-of-the-art methods especially on account of the running time. The code of FastDeRain can be downloaded at https://github.com/TaiXiangJiang/FastDeRain.},
  file        = {:http\://arxiv.org/pdf/1803.07487v3:PDF},
  keywords    = {cs.CV},
}

@Article{Chen2018a,
  author      = {Jie Chen and Cheen-Hau Tan and Junhui Hou and Lap-Pui Chau and He Li},
  date        = {2018-04-24},
  title       = {Robust Video Content Alignment and Compensation for Clear Vision Through the Rain},
  eprint      = {1804.09555},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Outdoor vision-based systems suffer from atmospheric turbulences, and rain is one of the worst factors for vision degradation. Current rain removal methods show limitations either for complex dynamic scenes, or under torrential rain with opaque occlusions. We propose a novel derain framework which applies superpixel (SP) segmentation to decompose the scene into depth consistent units. Alignment of scene contents are done at the SP level, which proves to be robust against rain occlusion interferences and fast camera motion. Two alignment output tensors, i.e., optimal temporal match tensor and sorted spatial-temporal match tensor, provide informative clues for the location of rain streaks and the occluded background contents. Different classical and novel methods such as Robust Principle Component Analysis and Convolutional Neural Networks are applied and compared for their respective advantages in efficiently exploiting the rich spatial-temporal features provided by the two tensors. Extensive evaluations show that advantage of up to 5dB is achieved on the scene restoration PSNR over state-of-the-art methods, and the advantage is especially obvious with highly complex and dynamic scenes. Visual evaluations show that the proposed framework is not only able to suppress heavy and opaque occluding rain streaks, but also large semi-transparent regional fluctuations and distortions.},
  file        = {:http\://arxiv.org/pdf/1804.09555v1:PDF},
  keywords    = {cs.CV},
}

@Article{Mason2019,
  author       = {E. I. Mason and S. K. Antiochos and N. M. Viall},
  date         = {2019-04-18},
  journaltitle = {2019ApJ...874L..33M},
  title        = {Observations of Solar Coronal Rain in Null Point Topologies},
  doi          = {10.3847/2041-8213/ab0c5d},
  eprint       = {1904.08982},
  eprintclass  = {astro-ph.SR},
  eprinttype   = {arXiv},
  abstract     = {Coronal rain is the well-known phenomenon in which hot plasma high in the Sun's corona undergoes rapid cooling (from > 10^6 K to < 10^4 K), condenses, and falls to the surface. Coronal rain appears frequently in active region coronal loops and is very common in post-flare loops. This Letter presents discovery observations, which show that coronal rain is ubiquitous in the embedded bipole very near a coronal hole boundary. Our observed structures formed when the photospheric decay of active region leading sunspots resulted in a large parasitic polarity embedded in a background unipolar region. We observe coronal rain to appear within the legs of closed loops well under the fan surface, as well as preferentially near separatrices of the resulting coronal topology: the spine lines, null point, and fan surface. We analyze 3 events using SDO Atmospheric Imaging Assembly (AIA) observations in the 304, 171, and 211 {/AA} channels, as well as SDO Helioseismic and Magnetic Imager (HMI) magnetograms. The frequency of rain formation and the ease with which it is observed strongly suggests that this phenomenon is generally present in null-point topologies of this size scale. We argue that these rain events could be explained by the classic process of thermal nonequilibrium or via interchange reconnection at the null; it is also possible that both mechanisms are present. Further studies with higher spatial resolution data and MHD simulations will be required to determine the exact mechanism(s).},
  file         = {:http\://arxiv.org/pdf/1904.08982v1:PDF},
  keywords     = {astro-ph.SR},
}

@Article{Wang2020c,
  author      = {Hong Wang and Qi Xie and Qian Zhao and Deyu Meng},
  date        = {2020-05-04},
  title       = {A Model-driven Deep Neural Network for Single Image Rain Removal},
  eprint      = {2005.01333},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Deep learning (DL) methods have achieved state-of-the-art performance in the task of single image rain removal. Most of current DL architectures, however, are still lack of sufficient interpretability and not fully integrated with physical structures inside general rain streaks. To this issue, in this paper, we propose a model-driven deep neural network for the task, with fully interpretable network structures. Specifically, based on the convolutional dictionary learning mechanism for representing rain, we propose a novel single image deraining model and utilize the proximal gradient descent technique to design an iterative algorithm only containing simple operators for solving the model. Such a simple implementation scheme facilitates us to unfold it into a new deep network architecture, called rain convolutional dictionary network (RCDNet), with almost every network module one-to-one corresponding to each operation involved in the algorithm. By end-to-end training the proposed RCDNet, all the rain kernels and proximal operators can be automatically extracted, faithfully characterizing the features of both rain and clean background layers, and thus naturally lead to its better deraining performance, especially in real scenarios. Comprehensive experiments substantiate the superiority of the proposed network, especially its well generality to diverse testing scenarios and good interpretability for all its modules, as compared with state-of-the-arts both visually and quantitatively. The source codes are available at \url{https://github.com/hongwang01/RCDNet}.},
  file        = {:http\://arxiv.org/pdf/2005.01333v1:PDF},
  keywords    = {eess.IV, cs.CV},
}

@Article{Li2020,
  author      = {Leping Li and Hardi Peter and Lakshmi Pradeep Chitta and Hongqiang Song},
  date        = {2020-11-02},
  title       = {Relation of coronal rain originating from coronal condensations to interchange magnetic reconnection},
  doi         = {10.3847/1538-4357/abc68c},
  eprint      = {2011.00709},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {Using extreme-ultraviolet images, we recently proposed a new and alternative formation mechanism for coronal rain along magnetically open field lines due to interchange magnetic reconnection. In this paper we report coronal rain at chromospheric and transition region temperatures originating from the coronal condensations facilitated by reconnection between open and closed coronal loops. For this, we employ the Interface Region Imaging Spectrograph (IRIS) and the Atmospheric Imaging Assembly (AIA) of the Solar Dynamics Observatory (SDO). Around 2013 October 19, a coronal rain along curved paths was recorded by IRIS over the southeastern solar limb. Related to this, we found reconnection between a system of higher-lying open features and lower-lying closed loops that occurs repeatedly in AIA images. In this process, the higher-lying features form magnetic dips. In response, two sets of newly reconnected loops appear and retract away from the reconnection region. In the dips, seven events of cooling and condensation of coronal plasma repeatedly occur due to thermal instability over several days, from October 18 to 20. The condensations flow downward to the surface as coronal rain, with a mean interval between condensations of 6.6 hr. In the cases where IRIS data were available we found the condensations to cool all the way down to chromospheric temperatures. Based on our observations we suggest that some of the coronal rain events observed at chromospheric temperatures could be explained by the new and alternative scenario for the formation of coronal rain, where the condensation is facilitated by interchange reconnection.},
  file        = {:http\://arxiv.org/pdf/2011.00709v1:PDF},
  keywords    = {astro-ph.SR},
}

@Article{Yue2021,
  author      = {Zongsheng Yue and Jianwen Xie and Qian Zhao and Deyu Meng},
  date        = {2021-03-14},
  title       = {Semi-Supervised Video Deraining with Dynamic Rain Generator},
  eprint      = {2103.07939},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {While deep learning (DL)-based video deraining methods have achieved significant success recently, they still exist two major drawbacks. Firstly, most of them do not sufficiently model the characteristics of rain layers of rainy videos. In fact, the rain layers exhibit strong physical properties (e.g., direction, scale and thickness) in spatial dimension and natural continuities in temporal dimension, and thus can be generally modelled by the spatial-temporal process in statistics. Secondly, current DL-based methods seriously depend on the labeled synthetic training data, whose rain types are always deviated from those in unlabeled real data. Such gap between synthetic and real data sets leads to poor performance when applying them in real scenarios. Against these issues, this paper proposes a new semi-supervised video deraining method, in which a dynamic rain generator is employed to fit the rain layer, expecting to better depict its insightful characteristics. Specifically, such dynamic generator consists of one emission model and one transition model to simultaneously encode the spatially physical structure and temporally continuous changes of rain streaks, respectively, which both are parameterized as deep neural networks (DNNs). Further more, different prior formats are designed for the labeled synthetic and unlabeled real data, so as to fully exploit the common knowledge underlying them. Last but not least, we also design a Monte Carlo EM algorithm to solve this model. Extensive experiments are conducted to verify the superiorities of the proposed semi-supervised deraining model.},
  file        = {:http\://arxiv.org/pdf/2103.07939v1:PDF},
  keywords    = {cs.CV, I.4.3},
}

@Article{Zhang2017,
  author      = {He Zhang and Vishwanath Sindagi and Vishal M. Patel},
  date        = {2017-01-21},
  title       = {Image De-raining Using a Conditional Generative Adversarial Network},
  eprint      = {1701.05957},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Severe weather conditions such as rain and snow adversely affect the visual quality of images captured under such conditions thus rendering them useless for further usage and sharing. In addition, such degraded images drastically affect performance of vision systems. Hence, it is important to solve the problem of single image de-raining/de-snowing. However, this is a difficult problem to solve due to its inherent ill-posed nature. Existing approaches attempt to introduce prior information to convert it into a well-posed problem. In this paper, we investigate a new point of view in addressing the single image de-raining problem. Instead of focusing only on deciding what is a good prior or a good framework to achieve good quantitative and qualitative performance, we also ensure that the de-rained image itself does not degrade the performance of a given computer vision algorithm such as detection and classification. In other words, the de-rained result should be indistinguishable from its corresponding clear image to a given discriminator. This criterion can be directly incorporated into the optimization framework by using the recently introduced conditional generative adversarial networks (GANs). To minimize artifacts introduced by GANs and ensure better visual quality, a new refined loss function is introduced. Based on this, we propose a novel single image de-raining method called Image De-raining Conditional General Adversarial Network (ID-CGAN), which considers quantitative, visual and also discriminative performance into the objective function. Experiments evaluated on synthetic images and real images show that the proposed method outperforms many recent state-of-the-art single image de-raining methods in terms of quantitative and visual performance.},
  file        = {:http\://arxiv.org/pdf/1701.05957v4:PDF},
  keywords    = {cs.CV},
}

@Article{Antolin2015,
  author      = {P. Antolin and G. Vissers and T. M. D. Pereira and L. Rouppe van der Voort and E. Scullion},
  date        = {2015-04-17},
  title       = {The multi-thermal and multi-stranded nature of coronal rain},
  doi         = {10.1088/0004-637X/806/1/81},
  eprint      = {1504.04418},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {In this work, we analyse coordinated observations spanning chromospheric, TR and coronal temperatures at very high resolution which reveal essential characteristics of thermally unstable plasmas. Coronal rain is found to be a highly multi-thermal phenomenon with a high degree of co-spatiality in the multi-wavelength emission. EUV darkening and quasi-periodic intensity variations are found to be strongly correlated to coronal rain showers. Progressive cooling of coronal rain is observed, leading to a height dependence of the emission. A fast-slow two-step catastrophic cooling progression is found, which may reflect the transition to optically thick plasma states. The intermittent and clumpy appearance of coronal rain at coronal heights becomes more continuous and persistent at chromospheric heights just before impact, mainly due to a funnel effect from the observed expansion of the magnetic field. Strong density inhomogeneities on spatial scales of 0.2"-0.5" are found, in which TR to chromospheric temperature transition occurs at the lowest detectable scales. The shape of the distribution of coronal rain widths is found to be independent of temperature with peaks close to the resolution limit of each telescope, ranging from 0.2" to 0.8". However we find a sharp increase of clump numbers at the coolest wavelengths and especially at higher resolution, suggesting that the bulk of the rain distribution remains undetected. Rain clumps appear organised in strands in both chromospheric and TR temperatures, suggesting an important role of thermal instability in the shaping of fundamental loop substructure. We further find structure reminiscent of the MHD thermal mode. Rain core densities are estimated to vary between 2x10^{10} cm^{-3} and 2.5x10^{11} cm^{-3} leading to significant downward mass fluxes per loop of 1-5x10^{9} g s^{-1}, suggesting a major role in the chromosphere-corona mass cycle.},
  file        = {:http\://arxiv.org/pdf/1504.04418v1:PDF},
  keywords    = {astro-ph.SR},
}

@Article{Fu2016,
  author      = {Xueyang Fu and Jiabin Huang and Xinghao Ding and Yinghao Liao and John Paisley},
  date        = {2016-09-07},
  title       = {Clearing the Skies: A deep network architecture for single-image rain removal},
  doi         = {10.1109/TIP.2017.2691802},
  eprint      = {1609.02087},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {We introduce a deep network architecture called DerainNet for removing rain streaks from an image. Based on the deep convolutional neural network (CNN), we directly learn the mapping relationship between rainy and clean image detail layers from data. Because we do not possess the ground truth corresponding to real-world rainy images, we synthesize images with rain for training. In contrast to other common strategies that increase depth or breadth of the network, we use image processing domain knowledge to modify the objective function and improve deraining with a modestly-sized CNN. Specifically, we train our DerainNet on the detail (high-pass) layer rather than in the image domain. Though DerainNet is trained on synthetic data, we find that the learned network translates very effectively to real-world images for testing. Moreover, we augment the CNN framework with image enhancement to improve the visual results. Compared with state-of-the-art single image de-raining methods, our method has improved rain removal and much faster computation time after network training.},
  file        = {:http\://arxiv.org/pdf/1609.02087v2:PDF},
  keywords    = {cs.CV},
}

@Article{Peters2002a,
  author       = {Ole Peters and Christopher Hertlein and Kim Christensen},
  date         = {2002-01-25},
  journaltitle = {Phys. Rev. Lett. 88, 018701-1 (2002)},
  title        = {A Complexity View of Rainfall},
  doi          = {10.1103/PhysRevLett.88.018701},
  eprint       = {cond-mat/0201468},
  eprintclass  = {cond-mat.stat-mech},
  eprinttype   = {arXiv},
  abstract     = {We show that rain events are analogous to a variety of nonequilibrium relaxation processes in Nature such as earthquakes and avalanches. Analysis of high-resolution rain data reveals that power laws describe the number of rain events versus size and number of droughts versus duration. In addition, the accumulated water column displays scale-less fluctuations. These statistical properties are the fingerprints of a self-organized critical process and may serve as a benchmark for models of precipitation and atmospheric processes.},
  file         = {:http\://arxiv.org/pdf/cond-mat/0201468v1:PDF},
  keywords     = {cond-mat.stat-mech, cond-mat.soft, physics.ao-ph},
}

@Article{Dickman2003,
  author       = {Ronald Dickman},
  date         = {2003-11-17},
  journaltitle = {Brazilian Journal of Physics 34, 337 (2004)},
  title        = {Fractal rain distributions and chaotic advection},
  eprint       = {physics/0311083},
  eprintclass  = {physics.ao-ph},
  eprinttype   = {arXiv},
  abstract     = {Localized rain events have been found to follow power-law distributions over several decades, suggesting parallels between precipitation and seismic activity [O. Peters et al., PRL 88, 018701 (2002)]. Similar power laws can be generated by treating raindrops as passive tracers advected by the velocity field of a two-dimensional system of point vortices [R. Dickman, PRL 90, 108701 (2003)]. Here I review observational and theoretical aspects of fractal rain distributions and chaotic advection, and present new results on tracer distributions in the vortex model.},
  file         = {:http\://arxiv.org/pdf/physics/0311083v1:PDF},
  keywords     = {physics.ao-ph, cond-mat.other, physics.flu-dyn},
}

@Article{Falkovich2004,
  author      = {G. Falkovich and M. G. Stepanov and M. Vucelja},
  date        = {2004-11-22},
  title       = {Rain initiation time in turbulent warm clouds},
  eprint      = {physics/0411201},
  eprintclass = {physics.ao-ph},
  eprinttype  = {arXiv},
  abstract    = {We present a mean-field model that describes droplet growth due to condensation and collisions and droplet loss due to fallout. The model allows for an effective numerical simulation. We study how the rain initiation time depends on different parameters. We also present a simple model that allows one to estimate the rain initiation time for turbulent clouds with an inhomogeneous concentration of cloud condensation nuclei. In particular, we show that over-seeding even a part of a cloud by small hygroscopic nuclei one can substantially delay the onset of precipitation.},
  file        = {:http\://arxiv.org/pdf/physics/0411201v1:PDF},
  keywords    = {physics.ao-ph, nlin.CD, physics.geo-ph},
}

@Article{Wilkinson2011,
  author      = {Michael Wilkinson},
  date        = {2011-06-01},
  title       = {Ostwald ripening and the kinetics of rain initiation},
  eprint      = {1106.0334},
  eprintclass = {physics.ao-ph},
  eprinttype  = {arXiv},
  abstract    = {A central problem in cloud physics is understanding the kinetics of the growth of water droplets. It is believed that there exists a 'condensation-coalescence bottleneck' in the growth of intermediate size droplets. Here the Lifshitz-Slezov theory of Ostwald ripening is applied to the kinetics of the growth of rain drops. The analysis shows that kinetic barriers to rain initiation are not significant. It is also shown that Ostwald ripening can greatly enhance the apparent collision efficiency of droplets falling under gravity.},
  file        = {:http\://arxiv.org/pdf/1106.0334v2:PDF},
  keywords    = {physics.ao-ph},
}

@Article{Antolin2012a,
  author      = {P. Antolin and G. Vissers and L. Rouppe van der Voort},
  date        = {2012-03-09},
  title       = {On-disk coronal rain},
  doi         = {10.1007/s11207-012-9979-7},
  eprint      = {1203.2077},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {Small and elongated, cool and dense blob-like structures are being reported with high resolution telescopes in physically different regions throughout the solar atmosphere. Their detection and the understanding of their formation, morphology and thermodynamical characteristics can provide important information on their hosting environment, especially concerning the magnetic field, whose understanding constitutes a major problem in solar physics. An example of such blobs is coronal rain, a phenomenon of thermal non- equilibrium observed in active region loops, which consists of cool and dense chromospheric blobs falling along loop-like paths from coronal heights. So far, only off-limb coronal rain has been observed and few reports on the phenomenon exist. In the present work, several datasets of on-disk H{\alpha} observations with the CRisp Imaging SpectroPolarimeter (CRISP) at the Swedish 1-m Solar Telescope (SST) are analyzed. A special family of on-disk blobs is selected for each dataset and a statistical analysis is carried out on their dynamics, morphology and temperatures. All characteristics present distributions which are very similar to reported coronal rain statistics. We discuss possible interpretations considering other similar blob-like structures reported so far and show that a coronal rain interpretation is the most likely one. Their chromospheric nature and the projection effects (which eliminate all direct possibility of height estimation) on one side, and their small sizes, fast dynamics, and especially, their faint character (offering low contrast with the background intensity) on the other side, are found as the main causes for the absence until now of the detection of this on-disk coronal rain counterpart.},
  file        = {:http\://arxiv.org/pdf/1203.2077v1:PDF},
  keywords    = {astro-ph.SR},
}

@Article{Wang2019d,
  author      = {Yinglong Wang and Haokui Zhang and Yu Liu and Qinfeng Shi and Bing Zeng},
  date        = {2019-10-09},
  title       = {Gradient Information Guided Deraining with A Novel Network and Adversarial Training},
  eprint      = {1910.03839},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {In recent years, deep learning based methods have made significant progress in rain-removing. However, the existing methods usually do not have good generalization ability, which leads to the fact that almost all of existing methods have a satisfied performance on removing a specific type of rain streaks, but may have a relatively poor performance on other types of rain streaks. In this paper, aiming at removing multiple types of rain streaks from single images, we propose a novel deraining framework (GRASPP-GAN), which has better generalization capacity. Specifically, a modified ResNet-18 which extracts the deep features of rainy images and a revised ASPP structure which adapts to the various shapes and sizes of rain streaks are composed together to form the backbone of our deraining network. Taking the more prominent characteristics of rain streaks in the gradient domain into consideration, a gradient loss is introduced to help to supervise our deraining training process, for which, a Sobel convolution layer is built to extract the gradient information flexibly. To further boost the performance, an adversarial learning scheme is employed for the first time to train the proposed network. Extensive experiments on both real-world and synthetic datasets demonstrate that our method outperforms the state-of-the-art deraining methods quantitatively and qualitatively. In addition, without any modifications, our proposed framework also achieves good visual performance on dehazing.},
  file        = {:http\://arxiv.org/pdf/1910.03839v1:PDF},
  keywords    = {cs.CV, eess.IV},
}

@Article{Kohutova2019,
  author      = {P. Kohutova and E. Verwichte and C. Froment},
  date        = {2019-10-17},
  title       = {Formation of coronal rain triggered by impulsive heating associated with magnetic reconnection},
  doi         = {10.1051/0004-6361/201936253},
  eprint      = {1910.07746},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {Coronal rain consists of cool plasma condensations formed in coronal loops as a result of thermal instability. The standard models of coronal rain formation assume that the heating is quasi-steady and localised at the coronal loop footpoints. We present an observation of magnetic reconnection in the corona and the associated impulsive heating triggering formation of coronal rain condensations. We analyse combined SDO/AIA and IRIS observations of a coronal rain event following a reconnection between threads of a low-lying prominence flux rope and surrounding coronal field lines. The reconnection of the twisted flux rope and open field lines leads to a release of magnetic twist. Evolution of the emission of one of the coronal loops involved in the reconnection process in different AIA bandpasses suggests that the loop becomes thermally unstable and is subject to the formation of coronal rain condensations following the reconnection and that the associated heating is localised in the upper part of the loop leg. We conclude that in addition to the standard models of thermally unstable coronal loops with heating localised exclusively in the footpoints, thermal instability and subsequent formation of condensations can be triggered by the impulsive heating associated with magnetic reconnection occurring anywhere along a magnetic field line.},
  file        = {:http\://arxiv.org/pdf/1910.07746v1:PDF},
  keywords    = {astro-ph.SR},
}

@Article{Wang2020d,
  author      = {Cong Wang and Xiaoying Xing and Zhixun Su and Junyang Chen},
  date        = {2020-08-03},
  title       = {DCSFN: Deep Cross-scale Fusion Network for Single Image Rain Removal},
  eprint      = {2008.00767},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Rain removal is an important but challenging computer vision task as rain streaks can severely degrade the visibility of images that may make other visions or multimedia tasks fail to work. Previous works mainly focused on feature extraction and processing or neural network structure, while the current rain removal methods can already achieve remarkable results, training based on single network structure without considering the cross-scale relationship may cause information drop-out. In this paper, we explore the cross-scale manner between networks and inner-scale fusion operation to solve the image rain removal task. Specifically, to learn features with different scales, we propose a multi-sub-networks structure, where these sub-networks are fused via a crossscale manner by Gate Recurrent Unit to inner-learn and make full use of information at different scales in these sub-networks. Further, we design an inner-scale connection block to utilize the multi-scale information and features fusion way between different scales to improve rain representation ability and we introduce the dense block with skip connection to inner-connect these blocks. Experimental results on both synthetic and real-world datasets have demonstrated the superiority of our proposed method, which outperforms over the state-of-the-art methods. The source code will be available at https://supercong94.wixsite.com/supercong94.},
  file        = {:http\://arxiv.org/pdf/2008.00767v1:PDF},
  keywords    = {cs.CV},
}

@Article{Wang2020e,
  author      = {Hong Wang and Zongsheng Yue and Qi Xie and Qian Zhao and Yefeng Zheng and Deyu Meng},
  date        = {2020-08-08},
  title       = {From Rain Generation to Rain Removal},
  eprint      = {2008.03580},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {For the single image rain removal (SIRR) task, the performance of deep learning (DL)-based methods is mainly affected by the designed deraining models and training datasets. Most of current state-of-the-art focus on constructing powerful deep models to obtain better deraining results. In this paper, to further improve the deraining performance, we novelly attempt to handle the SIRR task from the perspective of training datasets by exploring a more efficient way to synthesize rainy images. Specifically, we build a full Bayesian generative model for rainy image where the rain layer is parameterized as a generator with the input as some latent variables representing the physical structural rain factors, e.g., direction, scale, and thickness. To solve this model, we employ the variational inference framework to approximate the expected statistical distribution of rainy image in a data-driven manner. With the learned generator, we can automatically and sufficiently generate diverse and non-repetitive training pairs so as to efficiently enrich and augment the existing benchmark datasets. User study qualitatively and quantitatively evaluates the realism of generated rainy images. Comprehensive experiments substantiate that the proposed model can faithfully extract the complex rain distribution that not only helps significantly improve the deraining performance of current deep single image derainers, but also largely loosens the requirement of large training sample pre-collection for the SIRR task.},
  file        = {:http\://arxiv.org/pdf/2008.03580v2:PDF},
  keywords    = {cs.CV},
}

@Article{Wang2020f,
  author      = {Rong Wang and Yu Mei and Xiangzhu Meng and Jianjun Ma},
  date        = {2020-09-04},
  title       = {Secrecy Performance of Terahertz Wireless Links in Rain and Snow},
  doi         = {10.1016/j.nancom.2021.100350},
  eprint      = {2009.03031},
  eprintclass = {physics.app-ph},
  eprinttype  = {arXiv},
  abstract    = {Wireless communication technique at terahertz (THz) frequencies is regarded as the most potential candidate for future wireless networks due to its wider frequency bandwidth and higher data capacity when compared to that employing radio frequency (RF) and millimeter wave (mmWave). Besides, a THz link can achieve higher security at physical layer when it propagates in clear weather due to its higher directionality, which reduces the possibility of eavesdropping attacks. However, under adverse weather conditions (such as water fog, dust fog, rain and snow), the link degradation due to weather particles and gaseous molecules will affect the link secrecy performance seriously. In this work, we present theoretical investigations on physical layer security of a point-to-point THz link in rain and snow with a potential eavesdropper locating outside of the legitimate link path. Signal degradation due to rain/snow, gaseous attenuation and beam divergence are included in a theoretical model to estimate the link performance. Secrecy capacity of the link with carriers at 140, 220 and 340 GHz is calculated and compared. Insecure regions are also presented and the secrecy performance is analyzed. We find that the THz link suffers least eavesdropping attacks in rain and the maximum data transmission rate decreases for higher carrier frequencies in rain and snow.},
  file        = {:http\://arxiv.org/pdf/2009.03031v2:PDF},
  keywords    = {physics.app-ph, eess.SP},
}

@Article{Park2020,
  author      = {Yeachan Park and Myeongho Jeon and Junho Lee and Myungjoo Kang},
  date        = {2020-09-29},
  title       = {MARA-Net: Single Image Deraining Network with Multi-level connections and Adaptive Regional Attentions},
  eprint      = {2009.13990},
  eprintclass = {cs.CV},
  eprinttype  = {arXiv},
  abstract    = {Removing rain streaks from single images is an important problem in various computer vision tasks because rain streaks can degrade outdoor images and reduce their visibility. While recent convolutional neural network-based deraining models have succeeded in capturing rain streaks effectively, difficulties in recovering the details in rain-free images still remain. In this paper, we present a multi-level connection and adaptive regional attention network (MARA-Net) to properly restore the original background textures in rainy images. The first main idea is a multi-level connection design that repeatedly connects multi-level features of the encoder network to the decoder network. Multi-level connections encourage the decoding process to use the feature information of all levels. Channel attention is considered in multi-level connections to learn which level of features is important in the decoding process of the current level. The second main idea is a wide regional non-local block (WRNL). As rain streaks primarily exhibit a vertical distribution, we divide the grid of the image into horizontally-wide patches and apply a non-local operation to each region to explore the rich rain-free background information. Experimental results on both synthetic and real-world rainy datasets demonstrate that the proposed model significantly outperforms existing state-of-the-art models. Furthermore, the results of the joint deraining and segmentation experiment prove that our model contributes effectively to other vision tasks.},
  file        = {:http\://arxiv.org/pdf/2009.13990v3:PDF},
  keywords    = {cs.CV},
}

@Article{Boettcher2001,
  author      = {A. Boettcher},
  date        = {2001-01-01},
  title       = {On the determinant formulas by Borodin, Okounkov, Baik, Deift, and Rains},
  eprint      = {math/0101008},
  eprintclass = {math.FA},
  eprinttype  = {arXiv},
  abstract    = {We give alternative proofs to (block case versions of) some formulas for Toeplitz and Fredholm determinants established recently by the authors of the title. Our proof of the Borodin-Okounkov formula is very short and direct. The proof of the Baik-Deift-Rains formulas is based on standard manipulations with Wiener-Hopf factorizations.},
  file        = {:http\://arxiv.org/pdf/math/0101008v1:PDF},
  keywords    = {math.FA, math.CO, 47B35},
}

@Article{Liu2010,
  author      = {Zheng Liu and Xunya Jiang},
  date        = {2010-09-08},
  title       = {Rain model for the transmission spectra of one dimensional disorder system},
  eprint      = {1009.1447},
  eprintclass = {cond-mat.dis-nn},
  eprinttype  = {arXiv},
  abstract    = {We imitate the spectrum character of one-dimensional disorder system with our new rain model. It has been shown that the transmission spectrum can be approximately characterized by the model, which include some coupled lorentzian transmission peaks on the spectrum and embody the coupling of the local modes in the disorder system.},
  file        = {:http\://arxiv.org/pdf/1009.1447v1:PDF},
  keywords    = {cond-mat.dis-nn},
}

@Article{Antolin2011a,
  author      = {Patrick Antolin and Luc Rouppe van der Voort},
  date        = {2011-12-03},
  title       = {Observing the fine structure of loops through high resolution spectroscopic observations of coronal rain with the CRISP instrument at the Swedish Solar Telescope},
  doi         = {10.1088/0004-637X/745/2/152},
  eprint      = {1112.0656},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {We present here one of the first high resolution spectroscopic observations of coronal rain, performed with the CRISP instrument at the Swedish Solar Telescope. This work constitutes the first attempt to assess the importance of coronal rain in the understanding of the coronal magnetic field in active regions. A large statistical set is obtained in which dynamics (total velocities and accelerations), shapes (lengths and widths), trajectories (angles of fall) and thermodynamic properties (temperatures) of the condensations are derived. Specifically, we find that coronal rain is composed of small and dense chromospheric cores with average widths and lengths of 310 km and 710 km respectively, average temperatures below 7000 K, displaying a broad distribution of falling speeds with an average of 70 km/s and accelerations largely below the effective gravity along loops. Through estimates of the ion-neutral coupling in the blobs we show that coronal rain acts as a tracer of the coronal magnetic field, thus supporting the multi-strand loop scenario, and acts as a probe of the local thermodynamic conditions in loops. We further find that the cooling in neighboring strands occurs simultaneously in general suggesting a similar thermodynamic evolution among strands, which can be explained by a common footpoint heating process. Constraints for coronal heating models of loops are thus provided. Estimates of the fraction of coronal volume with coronal rain give values between 7% and 30%. Estimates of the occurrence time of the phenomenon in loops set times between 5 and 20 hours, implying that coronal rain may be a common phenomenon, in agreement with the frequent observations of cool downflows in EUV lines. The coronal mass drain rate in the form of coronal rain is estimated to be on the order of 5x10^9 g/s, a significant quantity compared to the estimate of mass flux into the corona from spicules.},
  file        = {:http\://arxiv.org/pdf/1112.0656v1:PDF},
  keywords    = {astro-ph.SR},
}

@Article{Wilkinson2015,
  author       = {Michael Wilkinson},
  date         = {2015-06-09},
  journaltitle = {Phys. Rev. Lett. 116, 018501 (2016)},
  title        = {Large deviations and rain showers},
  doi          = {10.1103/PhysRevLett.116.018501},
  eprint       = {1506.02807},
  eprintclass  = {physics.ao-ph},
  eprinttype   = {arXiv},
  abstract     = {Rainfall from ice-free cumulus clouds requires collisions of large numbers of microscopic droplets to create every raindrop. The onset of rain showers can be surprisingly rapid, much faster than the mean time required for a single collision. Large-deviation theory is used to explain this observation.},
  file         = {:http\://arxiv.org/pdf/1506.02807v1:PDF},
  keywords     = {physics.ao-ph},
}

@Article{Block2019,
  author       = {Louis Block and James Keesling and Brian Raines and Sonja Stimac},
  date         = {2019-03-18},
  journaltitle = {Topology and its Applications 156 (2009) 2417-2425},
  title        = {Homeomorphisms of unimodal inverse limit spaces with a non-recurrent postcritical point},
  eprint       = {1903.07524},
  eprintclass  = {math.DS},
  eprinttype   = {arXiv},
  abstract     = {In this paper we show that the group of automorphisms of a non-recurrent tent map inverse limit is very simple by demonstrating that every homeomorphism of such a space is isotopic to a power of the induced shift homeomorphism.},
  file         = {:http\://arxiv.org/pdf/1903.07524v2:PDF},
  keywords     = {math.DS},
}

@Article{Louis2003,
  author      = {Godfrey Louis and A. Santhosh Kumar},
  date        = {2003-12-29},
  title       = {New biology of red rain extremophiles prove cometary panspermia},
  eprint      = {astro-ph/0312639},
  eprintclass = {astro-ph},
  eprinttype  = {arXiv},
  abstract    = {This paper reports the extraordinary biology of the microorganisms from the mysterious red rain of Kerala, India. These chemosynthetic organisms grow optimally at an extreme high temperature of 300 degrees C in hydrothermal conditions and can metabolize inorganic and organic compounds including hydrocarbons. Stages found in their life cycle show reproduction by a special multiple fission process and the red cells found in the red rain are identified as the resting spores of these microbes. While these extreme hyperthermophiles contain proteins, our study shows the absence of DNA in these organisms, indicating a new primitive domain of life with alternate thermostable genetics. This new biology proves our earlier hypothesis that these microbes are of extraterrestrial origin and also supports our earlier argument that the mysterious red rain of Kerala is due to the cometary delivery of the red spores into the stratosphere above Kerala.},
  file        = {:http\://arxiv.org/pdf/astro-ph/0312639v1:PDF},
  keywords    = {astro-ph},
}

@Article{Audenaert2002,
  author       = {K. Audenaert and B. De Moor and K. G. H. Vollbrecht and R. F. Werner},
  date         = {2002-04-24},
  journaltitle = {Phys. Rev. A 66, 032310 (2002)},
  title        = {Asymptotic Relative Entropy of Entanglement for Orthogonally Invariant States},
  doi          = {10.1103/PhysRevA.66.032310},
  eprint       = {quant-ph/0204143},
  eprintclass  = {quant-ph},
  eprinttype   = {arXiv},
  abstract     = {For a special class of bipartite states we calculate explicitly the asymptotic relative entropy of entanglement $E_R^\infty$ with respect to states having a positive partial transpose (PPT). This quantity is an upper bound to distillable entanglement. The states considered are invariant under rotations of the form $O\otimes O$, where $O$ is any orthogonal matrix. We show that in this case $E_R^\infty$ is equal to another upper bound on distillable entanglement, constructed by Rains. To perform these calculations, we have introduced a number of new results that are interesting in their own right: (i) the Rains bound is convex and continuous; (ii) under some weak assumption, the Rains bound is an upper bound to $E_R^\infty$; (iii) for states for which the relative entropy of entanglement $E_R$ is additive, the Rains bound is equal to $E_R$.},
  file         = {:http\://arxiv.org/pdf/quant-ph/0204143v1:PDF},
  keywords     = {quant-ph},
}

@Article{Wei2013,
  author       = {Y. Wei and D. J. Durian},
  date         = {2013-10-23},
  journaltitle = {European Physical Journal E 37, 97 (2014)},
  title        = {Rain water transport and storage in a model sandy soil with hydrogel particle additives},
  doi          = {10.1140/epje/i2014-14097-x},
  eprint       = {1310.6258},
  eprintclass  = {cond-mat.soft},
  eprinttype   = {arXiv},
  abstract     = {We study rain water infiltration and drainage in a dry model sandy soil with superabsorbent hydrogel particle additives by measuring the mass of retained water for non-ponding rainfall using a self-built 3D laboratory set-up. In the pure model sandy soil, the retained water curve measurements indicate that instead of a stable horizontal wetting front that grows downward uniformly, a narrow fingered flow forms under the top layer of water-saturated soil. This rain water channelization phenomenon not only further reduces the available rain water in the plant root zone, but also affects the efficiency of soil additives, such as superabsorbent hydrogel particles. Our studies show that the shape of the retained water curve for a soil packing with hydrogel particle additives strongly depends on the location and the concentration of the hydrogel particles in the model sandy soil. By carefully choosing the particle size and distribution methods, we may use the swollen hydrogel particles to modify the soil pore structure, to clog or extend the water channels in sandy soils, or to build water reservoirs in the plant root zone.},
  file         = {:http\://arxiv.org/pdf/1310.6258v2:PDF},
  keywords     = {cond-mat.soft, nlin.PS},
}

@Article{Sun2016,
  author       = {Ying Sun and Michael L. Stein},
  date         = {2016-02-09},
  journaltitle = {Annals of Applied Statistics 2015, Vol. 9, No. 4, 2110-2132},
  title        = {A stochastic space-time model for intermittent precipitation occurrences},
  doi          = {10.1214/15-AOAS875},
  eprint       = {1602.02902},
  eprintclass  = {stat.AP},
  eprinttype   = {arXiv},
  abstract     = {Modeling a precipitation field is challenging due to its intermittent and highly scale-dependent nature. Motivated by the features of high-frequency precipitation data from a network of rain gauges, we propose a threshold space-time $t$ random field (tRF) model for 15-minute precipitation occurrences. This model is constructed through a space-time Gaussian random field (GRF) with random scaling varying along time or space and time. It can be viewed as a generalization of the purely spatial tRF, and has a hierarchical representation that allows for Bayesian interpretation. Developing appropriate tools for evaluating precipitation models is a crucial part of the model-building process, and we focus on evaluating whether models can produce the observed conditional dry and rain probabilities given that some set of neighboring sites all have rain or all have no rain. These conditional probabilities show that the proposed space-time model has noticeable improvements in some characteristics of joint rainfall occurrences for the data we have considered.},
  file         = {:http\://arxiv.org/pdf/1602.02902v1:PDF},
  keywords     = {stat.AP},
}

@Article{Cavaleri2015,
  author      = {Luigi Cavaleri and Luciana Bertotti and Jean-Raymond Bidlot},
  date        = {2015-03-10},
  title       = {Waving in the rain},
  eprint      = {1503.03031},
  eprintclass = {physics.ao-ph},
  eprinttype  = {arXiv},
  abstract    = {We consider the effect of rain on wind wave generation and dissipation. Rain falling on a wavy surface may have a marked tendency to dampen the shorter waves in the tail of the spectrum, the related range increasing with the rain rate. Following the coupling between meteorological and wave models, we derive that on the whole this should imply stronger wind and higher waves in the most energetic part of the spectrum. This is supported by numerical experiments. However, a verification based on the comparison between operational model results and measured data suggests that the opposite is true. This leads to a keen analysis of the overall process, in particular on the role of the tail of the spectrum in modulating the wind input and the white-capping. We suggest that the relationship between white-capping and generation by wind is deeper and more implicative than presently generally assumed.},
  file        = {:http\://arxiv.org/pdf/1503.03031v1:PDF},
  keywords    = {physics.ao-ph, 86A10},
}

@Article{Sharma2017,
  author      = {Sandeep Sharma and Nitu Kumari},
  date        = {2017-04-28},
  title       = {Modeling the impact of rain on population exposed to air pollution},
  eprint      = {1705.01895},
  eprintclass = {physics.ao-ph},
  eprinttype  = {arXiv},
  abstract    = {Environmental pollution, comprising of air, water and soil have emerged as a serious problem in past two decades. The air pollution is caused by contamination of air due to various natural and anthropogenic activities. The growing air pollution has diverse adverse effects on human health and other living species. However, a significant reduction in the concentration of air pollutants has been observed during the rainy season. Recently, a number of studies have been performed to understand the mechanism of removal of air pollutants due to the rain. These studies have found that rain is helpful in removing many air pollutants from the environment. In this paper, we proposed a mathematical model to investigate the role of rain in removal of air pollutants and its subsequent impacts on human population.},
  file        = {:http\://arxiv.org/pdf/1705.01895v1:PDF},
  keywords    = {physics.ao-ph},
}

@Article{Nissen2019,
  author      = {Silas Boye Nissen and Jan O. Haerter},
  date        = {2019-11-28},
  title       = {How weakened cold pools open for convective self-aggregation},
  eprint      = {1911.12849},
  eprintclass = {physics.ao-ph},
  eprinttype  = {arXiv},
  abstract    = {In radiative-convective equilibrium (RCE) simulations, convective self-aggregation (CSA) is the spontaneous organization into segregated cloudy and cloud-free regions. Evidence exists for how CSA is stabilized, but how it arises favorably on large domains is not settled. Using large-eddy simulations (LES), we link the spatial organization emerging from the interaction of cold pools (CPs) to CSA. We systematically weaken simulated rain evaporation to reduce maximal CP radii, $R_{\text{max}}$, and find reducing $R_{\text{max}}$ causes CSA to occur earlier. We further identify a typical rain cell generation time and a minimum radius, $R_{\text{min}}$, around a given rain cell, within which the formation of subsequent rain cells is suppressed. Incorporating $R_{\text{min}}$ and $R_{\text{max}}$, we propose a toy model that captures how CSA arises earlier on large domains: when two CPs of radii $r_{i,j}\in[R_{\text{min}},R_{\text{max}}]$ collide, they form a new convective event. These findings imply that interactions between CPs may explain the initial stages of CSA.},
  file        = {:http\://arxiv.org/pdf/1911.12849v3:PDF},
  keywords    = {physics.ao-ph},
}

@Article{Fu2020,
  author      = {Jun Fu and Jianfeng Xu and Kazuyuki Tasaka and Zhibo Chen},
  date        = {2020-06-01},
  title       = {Residual Squeeze-and-Excitation Network for Fast Image Deraining},
  eprint      = {2006.00757},
  eprintclass = {eess.IV},
  eprinttype  = {arXiv},
  abstract    = {Image deraining is an important image processing task as rain streaks not only severely degrade the visual quality of images but also significantly affect the performance of high-level vision tasks. Traditional methods progressively remove rain streaks via different recurrent neural networks. However, these methods fail to yield plausible rain-free images in an efficient manner. In this paper, we propose a residual squeeze-and-excitation network called RSEN for fast image deraining as well as superior deraining performance compared with state-of-the-art approaches. Specifically, RSEN adopts a lightweight encoder-decoder architecture to conduct rain removal in one stage. Besides, both encoder and decoder adopt a novel residual squeeze-and-excitation block as the core of feature extraction, which contains a residual block for producing hierarchical features, followed by a squeeze-and-excitation block for channel-wisely enhancing the resulted hierarchical features. Experimental results demonstrate that our method can not only considerably reduce the computational complexity but also significantly improve the deraining performance compared with state-of-the-art methods.},
  file        = {:http\://arxiv.org/pdf/2006.00757v1:PDF},
  keywords    = {eess.IV, cs.CV, cs.LG},
}

@Article{Scullion2016,
  author      = {E. Scullion and L. Rouppe Van Der Voort and P. Antolin and S. Wedemeyer and G. Vissers and E. P. Kontar and P. Gallagher},
  date        = {2016-10-28},
  title       = {Observing the formation of flare-driven coronal rain},
  doi         = {10.3847/1538-4357/833/2/184},
  eprint      = {1610.09255},
  eprintclass = {astro-ph.SR},
  eprinttype  = {arXiv},
  abstract    = {Flare-driven coronal rain can manifest from rapidly cooled plasma condensations near coronal loop-tops in thermally unstable post-flare arcades. We detect 5 phases that characterise the post-flare decay: heating, evaporation, conductive cooling dominance for ~120 s, radiative / enthalpy cooling dominance for ~4700 s and finally catastrophic cooling occurring within 35-124 s leading to rain strands with s periodicity of 55-70 s. We find an excellent agreement between the observations and model predictions of the dominant cooling timescales and the onset of catastrophic cooling. At the rain formation site we detect co-moving, multi-thermal rain clumps that undergo catastrophic cooling from ~1 MK to ~22000 K. During catastrophic cooling the plasma cools at a maximum rate of 22700 K s-1 in multiple loop-top sources. We calculated the density of the EUV plasma from the DEM of the multi-thermal source employing regularised inversion. Assuming a pressure balance, we estimate the density of the chromospheric component of rain to be 9.21x10^11 +-1.76x10^11 cm-3 which is comparable with quiescent coronal rain densities. With up to 8 parallel strands in the EUV loop cross section, we calculate the mass loss rate from the post-flare arcade to be as much as 1.98x10^12 +/-4.95x10^11 g s-1. Finally, we reveal a close proximity between the model predictions of 10^5.8 K and the observed properties between 10^5.9 K and 10^6.2 K, that defines the temperature onset of catastrophic cooling. The close correspondence between the observations and numerical models suggests that indeed acoustic waves (with a sound travel time of 68 s) could play an important role in redistributing energy and sustaining the enthalpy-based radiative cooling.},
  file        = {:http\://arxiv.org/pdf/1610.09255v1:PDF},
  keywords    = {astro-ph.SR},
}

@Article{Saha2018,
  author      = {Prashanta Saha and Upulee Kanewala},
  date        = {2018-02-20},
  title       = {Fault Detection Effectiveness of Source Test Case Generation Strategies for Metamorphic Testing},
  eprint      = {1802.07361},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Metamorphic testing is a well known approach to tackle the oracle problem in software testing. This technique requires the use of source test cases that serve as seeds for the generation of follow-up test cases. Systematic design of test cases is crucial for the test quality. Thus, source test case generation strategy can make a big impact on the fault detection effectiveness of metamorphic testing. Most of the previous studies on metamorphic testing have used either random test data or existing test cases as source test cases. There has been limited research done on systematic source test case generation for metamorphic testing. This paper provides a comprehensive evaluation on the impact of source test case generation techniques on the fault finding effectiveness of metamorphic testing. We evaluated the effectiveness of line coverage, branch coverage, weak mutation and random test generation strategies for source test case generation. The experiments are conducted with 77 methods from 4 open source code repositories. Our results show that by systematically creating source test cases, we can significantly increase the fault finding effectiveness of metamorphic testing. Further, in this paper we introduce a simple metamorphic testing tool called "METtester" that we use to conduct metamorphic testing on these methods.},
  file        = {:http\://arxiv.org/pdf/1802.07361v1:PDF},
  keywords    = {cs.SE},
}

@Article{Zhu2019,
  author      = {Hong Zhu and Ian Bayley and Dongmei Liu and Xiaoyu Zheng},
  date        = {2019-12-20},
  title       = {Morphy: A Datamorphic Software Test Automation Tool},
  eprint      = {1912.09881},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {This paper presents an automated tool called Morphy for datamorphic testing. It classifies software test artefacts into test entities and test morphisms, which are mappings on testing entities. In addition to datamorphisms, metamorphisms and seed test case makers, Morphy also employs a set of other test morphisms including test case metrics and filters, test set metrics and filters, test result analysers and test executers to realise test automation. In particular, basic testing activities can be automated by invoking test morphisms. Test strategies can be realised as complex combinations of test morphisms. Test processes can be automated by recording, editing and playing test scripts that invoke test morphisms and strategies. Three types of test strategies have been implemented in Morphy: datamorphism combination strategies, cluster border exploration strategies and strategies for test set optimisation via genetic algorithms. This paper focuses on the datamorphism combination strategies by giving their definitions and implementation algorithms. The paper also illustrates their uses for testing both traditional software and AI applications with three case studies.},
  file        = {:http\://arxiv.org/pdf/1912.09881v1:PDF},
  keywords    = {cs.SE, cs.AI},
}

@Article{Sanchez2016,
  author      = {Jimi Sanchez},
  date        = {2016-06-01},
  title       = {A Review of Pair-wise Testing},
  eprint      = {1606.00288},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {In software testing, the large size of the input domain makes exhaustively testing the inputs a daunting and often impossible task. Pair-wise testing is a popular approach to combinatorial testing problems. This paper reviews Pair-wise testing and its history, strengths, weaknesses, and tools for generating test cases.},
  file        = {:http\://arxiv.org/pdf/1606.00288v1:PDF},
  keywords    = {cs.SE},
}

@Article{Wu2007,
  author       = {Cheng-Wen Wu},
  date         = {2007-10-25},
  journaltitle = {Dans Design, Automation and Test in Europe - DATE'05, Munich : Allemagne (2005)},
  title        = {SOC Testing Methodology and Practice},
  eprint       = {0710.4669},
  eprintclass  = {cs.AR},
  eprinttype   = {arXiv},
  abstract     = {On a commercial digital still camera (DSC) controller chip we practice a novel SOC test integration platform, solving real problems in test scheduling, test IO reduction, timing of functional test, scan IO sharing, embedded memory built-in self-test (BIST), etc. The chip has been fabricated and tested successfully by our approach. Test results justify that short test integration cost, short test time, and small area overhead can be achieved. To support SOC testing, a memory BIST compiler and an SOC testing integration system have been developed.},
  file         = {:http\://arxiv.org/pdf/0710.4669v1:PDF},
  keywords     = {cs.AR},
}

@Article{Kawai2020,
  author      = {Tetsuya Kawai and Masayuki Uchida},
  date        = {2020-10-26},
  title       = {Adaptive testing method for ergodic diffusion processes based on high frequency data},
  eprint      = {2010.13410},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {We consider parametric tests for multidimensional ergodic diffusions based on high frequency data. We propose two-step testing method for diffusion parameters and drift parameters. To construct test statistics of the tests, we utilize the adaptive estimator and provide three types of test statistics: likelihood ratio type test, Wald type test and Rao's score type test. It is proved that these test statistics converge in distribution to the chi-squared distribution under null hypothesis and have consistency of the tests against alternatives. Moreover, these test statistics converge in distribution to the non-central chi-squared distribution under local alternatives. We also give some simulation studies of the behavior of the three types of test statistics.},
  file        = {:http\://arxiv.org/pdf/2010.13410v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Aslan2002,
  author      = {B. Aslan and G. Zech},
  date        = {2002-07-31},
  title       = {Comparison of different goodness-of-fit tests},
  eprint      = {math/0207300},
  eprintclass = {math.PR},
  eprinttype  = {arXiv},
  abstract    = {Various distribution free goodness-of-fit test procedures have been extracted from literature. We present two new binning free tests, the univariate three-region-test and the multivariate energy test. The power of the selected tests with respect to different slowly varying distortions of experimental distributions are investigated. None of the tests is optimum for all distortions. The energy test has high power in many applications and is superior to the chi^2 test.},
  file        = {:http\://arxiv.org/pdf/math/0207300v1:PDF},
  keywords    = {math.PR},
}

@Article{Mrkvicka2015,
  author      = {Tom Mrkvika and Mari Myllymki and Ute Hahn},
  date        = {2015-06-04},
  title       = {Multiple Monte Carlo Testing with Applications in Spatial Point Processes},
  eprint      = {1506.01646},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {The rank envelope test (Myllym\"aki et al., Global envelope tests for spatial processes, arXiv:1307.0239 [stat.ME]) is proposed as a solution to multiple testing problem for Monte Carlo tests. Three different situations are recognized: 1) a few univariate Monte Carlo tests, 2) a Monte Carlo test with a function as the test statistic, 3) several Monte Carlo tests with functions as test statistics. The rank test has correct (global) type I error in each case and it is accompanied with a $p$-value and with a graphical interpretation which shows which subtest or which distances of the used test function(s) lead to the rejection at the prescribed significance level of the test. Examples of null hypothesis from point process and random set statistics are used to demonstrate the strength of the rank envelope test. The examples include goodness-of-fit test with several test functions, goodness-of-fit test for one group of point patterns, comparison of several groups of point patterns, test of dependence of components in a multi-type point pattern, and test of Boolean assumption for random closed sets.},
  file        = {:http\://arxiv.org/pdf/1506.01646v1:PDF},
  keywords    = {stat.ME},
}

@Article{Morciniec2016,
  author      = {Tobias Morciniec and Andreas Podelski},
  date        = {2016-12-13},
  title       = {A Logical Approach to Generating Test Plans},
  eprint      = {1612.04351},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {During the execution of a test plan, a test manager may decide to drop a test case if its result can be inferred from already executed test cases. We show that it is possible to automatically generate a test plan to exploit the potential to justifiably drop a test case and thus reduce the number of test cases. Our approach uses Boolean formulas to model the mutual dependencies between test results. The algorithm to generate a test plan comes with the formal guarantee of optimality with regards to the inference of the result of a test case from already executed test cases.},
  file        = {:http\://arxiv.org/pdf/1612.04351v1:PDF},
  keywords    = {cs.SE},
}

@Article{Iwasaki2019,
  author      = {Atsushi Iwasaki},
  date        = {2019-08-20},
  title       = {Independent Randomness Tests based on the Orthogonalized Non-overlapping Template Matching Test},
  eprint      = {1908.07145},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {In general, randomness tests included in a test suite are not independent of each other. This renders it difficult to fix a rational criterion through the whole test suite with an explicit significance level. In this paper, we focus on the Non-overlapping Template Matching Test, which is a randomness test included in the NIST statistical test suite. The test uses a parameter called "template" and we can consider a test item for each template. We investigate dependency between two test items by deriving the joint probability density function of the two p-values and propose a transformation to make multi test items independent of each other.},
  file        = {:http\://arxiv.org/pdf/1908.07145v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Ceyhan2008,
  author      = {Elvan Ceyhan},
  date        = {2008-07-26},
  title       = {On the Use of Nearest Neighbor Contingency Tables for Testing Spatial Segregation},
  eprint      = {0807.4236},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {For two or more classes (or types) of points, nearest neighbor contingency tables (NNCTs) are constructed using nearest neighbor (NN) frequencies and are used in testing spatial segregation of the classes. Pielou's test of independence, Dixon's cell-specific, class-specific, and overall tests are the tests based on NNCTs (i.e., they are NNCT-tests). These tests are designed and intended for use under the null pattern of random labeling (RL) of completely mapped data. However, it has been shown that Pielou's test is not appropriate for testing segregation against the RL pattern while Dixon's tests are. In this article, we compare Pielou's and Dixon's NNCT-tests; introduce the one-sided versions of Pielou's test; extend the use of NNCT-tests for testing complete spatial randomness (CSR) of points from two or more classes (which is called \emph{CSR independence}, henceforth). We assess the finite sample performance of the tests by an extensive Monte Carlo simulation study and demonstrate that Dixon's tests are also appropriate for testing CSR independence; but Pielou's test and the corresponding one-sided versions are liberal for testing CSR independence or RL. Furthermore, we show that Pielou's tests are only appropriate when the NNCT is based on a random sample of (base, NN) pairs. We also prove the consistency of the tests under their appropriate null hypotheses. Moreover, we investigate the edge (or boundary) effects on the NNCT-tests and compare the buffer zone and toroidal edge correction methods for these tests. We illustrate the tests on a real life and an artificial data set.},
  file        = {:http\://arxiv.org/pdf/0807.4236v1:PDF},
  keywords    = {stat.ME, stat.AP},
}

@Article{Feng2018,
  author      = {Long Feng},
  date        = {2018-12-27},
  title       = {Power Comparison between High Dimensional t-Test, Sign, and Signed Rank Tests},
  eprint      = {1812.10625},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we propose a power comparison between high dimensional t-test, sign and signed rank test for the one sample mean test. We show that the high dimensional signed rank test is superior to a high dimensional t test, but inferior to a high dimensional sign test.},
  file        = {:http\://arxiv.org/pdf/1812.10625v1:PDF},
  keywords    = {stat.ME},
}

@Article{Steimle2019,
  author      = {Markus Steimle and Till Menzel and Markus Maurer},
  date        = {2019-05-22},
  title       = {A Method for Classifying Test Bench Configurations in a Scenario-Based Test Approach for Automated Vehicles},
  eprint      = {1905.09018},
  eprintclass = {eess.SP},
  eprinttype  = {arXiv},
  abstract    = {The introduction of automated vehicles demands a way to prove their safe operation. However, validating the safety of automated vehicles is still an unsolved problem. While the scenario-based test approach seems to provide a possible solution, it requires the execution of a high amount of test cases. Several test benches, from actual test vehicles to partly or fully simulated environments, are available, but choosing the optimal test bench, e.g. in terms of required execution time and costs, is a difficult task. Every test bench provides different elements, e.g. simulation models which can be used for test case execution. The composition of elements at a specific test bench is called test bench configuration. This test bench configuration determines the actual performance of a test bench and, therefore, whether the run of a particular test case provides valid test case results with respect to the intended purpose, e.g. a safety validation. For an effective and efficient test case execution, a method is required to assign test cases to the most appropriate test bench configuration. Therefore, it is indispensable to have a method to classify test bench configurations in a clear and reproducible manner. In this paper, we propose a method for classifying test benches and test bench configurations and illustrate the classification method with some examples. The classification method serves as a basis for a systematic assignment of test cases to test bench configurations which allows for an effective and efficient test case execution.},
  file        = {:http\://arxiv.org/pdf/1905.09018v1:PDF},
  keywords    = {eess.SP},
}

@Article{Kim2004,
  author      = {Song-Ju Kim and Ken Umeno and Akio Hasegawa},
  date        = {2004-01-27},
  title       = {Corrections of the NIST Statistical Test Suite for Randomness},
  eprint      = {nlin/0401040},
  eprintclass = {nlin.CD},
  eprinttype  = {arXiv},
  abstract    = {It is well known that the NIST statistical test suite was used for the evaluation of AES candidate algorithms. We have found that the test setting of Discrete Fourier Transform test and Lempel-Ziv test of this test suite are wrong. We give four corrections of mistakes in the test settings. This suggests that re-evaluation of the test results should be needed.},
  file        = {:http\://arxiv.org/pdf/nlin/0401040v1:PDF},
  keywords    = {nlin.CD},
}

@Article{Kattumannil2020,
  author      = {Sudheesh Kattumannil},
  date        = {2020-01-22},
  title       = {A new goodness of fit test for normal distribution based on Stein's characterization},
  eprint      = {2001.07932},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {We develop a new non-parametric test for testing normal distribution using Stein's characterization. We study asymptotic properties of the test statistic. We also develop jackknife empirical likelihood ratio test for testing normality. Using Monte Carlo simulation study, we evaluate the finite sample performance of the proposed JEL based test. Finally, we illustrate our test procedure using two real data.},
  file        = {:http\://arxiv.org/pdf/2001.07932v2:PDF},
  keywords    = {math.ST, stat.TH, 62G10, 62G20},
}

@Article{Enoiu2018,
  author      = {Eduard Enoiu and Mirgita Frasheri},
  date        = {2018-02-12},
  title       = {Test Agents: Adaptive, Autonomous and Intelligent Test Cases},
  eprint      = {1802.03921},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Growth of software size, lack of resources to perform regression testing, and failure to detect bugs faster have seen increased reliance on continuous integration and test automation. Even with greater hardware and software resources dedicated to test automation, software testing is faced with enormous challenges, resulting in increased dependence on complex mechanisms for automated test case selection and prioritization as part of a continuous integration framework. These mechanisms are currently using simple entities called test cases that are concretely realized as executable scripts. Our key idea is to provide test cases with more reasoning, adaptive behavior and learning capabilities by using the concepts of intelligent software agents. We refer to such test cases as test agents. The model that underlie a test agent is capable of flexible and autonomous actions in order to meet overall testing objectives. Our goal is to increase the decentralization of regression testing by letting test agents to know for themselves when they should be executing, how they should update their purpose, and when they should interact with each other. In this paper, we envision software test agents that display such adaptive autonomous behavior. Emerging developments and challenges regarding the use of test agents are explored-in particular, new research that seeks to use adaptive autonomous agents in software testing.},
  file        = {:http\://arxiv.org/pdf/1802.03921v1:PDF},
  keywords    = {cs.SE},
}

@Article{Betka2021,
  author      = {Maik Betka and Stefan Wagner},
  date        = {2021-03-15},
  title       = {Extreme mutation testing in practice: An industrial case study},
  eprint      = {2103.08480},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Mutation testing is used to evaluate the effectiveness of test suites. In recent years, a promising variation called extreme mutation testing emerged that is computationally less expensive. It identifies methods where their functionality can be entirely removed, and the test suite would not notice it, despite having coverage. These methods are called pseudo-tested. In this paper, we compare the execution and analysis times for traditional and extreme mutation testing and discuss what they mean in practice. We look at how extreme mutation testing impacts current software development practices and discuss open challenges that need to be addressed to foster industry adoption. For that, we conducted an industrial case study consisting of running traditional and extreme mutation testing in a large software project from the semiconductor industry that is covered by a test suite of more than 11,000 unit tests. In addition to that, we did a qualitative analysis of 25 pseudo-tested methods and interviewed two experienced developers to see how they write unit tests and gathered opinions on how useful the findings of extreme mutation testing are. Our results include execution times, scores, numbers of executed tests and mutators, reasons why methods are pseudo-tested, and an interview summary. We conclude that the shorter execution and analysis times are well noticeable in practice and show that extreme mutation testing supplements writing unit tests in conjunction with code coverage tools. We propose that pseudo-tested code should be highlighted in code coverage reports and that extreme mutation testing should be performed when writing unit tests rather than in a decoupled session. Future research should investigate how to perform extreme mutation testing while writing unit tests such that the results are available fast enough but still meaningful.},
  file        = {:http\://arxiv.org/pdf/2103.08480v1:PDF},
  keywords    = {cs.SE, D.2.5},
}

@Article{Haq2021,
  author      = {Fitash Ul Haq and Donghwan Shin and Shiva Nejati and Lionel Briand},
  date        = {2021-01-26},
  title       = {Can Offline Testing of Deep Neural Networks Replace Their Online Testing?},
  eprint      = {2101.11118},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {We distinguish two general modes of testing for Deep Neural Networks (DNNs): Offline testing where DNNs are tested as individual units based on test datasets obtained independently from the DNNs under test, and online testing where DNNs are embedded into a specific application environment and tested in a closed-loop mode in interaction with the application environment. Typically, DNNs are subjected to both types of testing during their development life cycle where offline testing is applied immediately after DNN training and online testing follows after offline testing and once a DNN is deployed within a specific application environment. In this paper, we study the relationship between offline and online testing. Our goal is to determine how offline testing and online testing differ or complement one another and if we can use offline testing results to run fewer tests during online testing to reduce the testing cost. Though these questions are generally relevant to all autonomous systems, we study them in the context of automated driving systems where, as study subjects, we use DNNs automating end-to-end controls of steering functions of self-driving vehicles. Our results show that offline testing is more optimistic than online testing as many safety violations identified by online testing could not be identified by offline testing, while large prediction errors generated by offline testing always led to severe safety violations detectable by online testing. Further, we cannot use offline testing results to run fewer tests during online testing in practice since we are not able to identify specific situations where offline testing could be as accurate as online testing in identifying safety requirement violations.},
  file        = {:http\://arxiv.org/pdf/2101.11118v1:PDF},
  keywords    = {cs.SE, cs.LG},
}

@Article{Yunus2007,
  author      = {Rossita M Yunus and Shahjahan Khan},
  date        = {2007-10-10},
  title       = {Increasing power of the test through pre-test - a robust method},
  eprint      = {0710.1919},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {This paper develops robust test procedures for testing the intercept of a simple regression model when it is \textit{apriori} suspected that the slope has a specified value. Defining unrestricted test (UT), restricted test (RT) and pre-test test (PTT) corresponding to the unrestricted (UE), restricted (RE), and preliminary test estimators (PTE) in the estimation case, the M-estimation methodology is used to formulate the M-tests and derive their asymptotic power functions. Analytical and graphical comparisons of the three tests are obtained by studying the power functions with respect to size and power of the tests. It is shown that PTT achieves a reasonable dominance over the others asymptotically.},
  file        = {:http\://arxiv.org/pdf/0710.1919v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Kada2008,
  author       = {Masaru Kada and Harumichi Nishimura and Tomoyuki Yamakami},
  date         = {2008-09-11},
  journaltitle = {J. Phys. A: Math. Theor. 41 (2008) 395309},
  title        = {The Efficiency of Quantum Identity Testing of Multiple States},
  doi          = {10.1088/1751-8113/41/39/395309},
  eprint       = {0809.2037},
  eprintclass  = {quant-ph},
  eprinttype   = {arXiv},
  abstract     = {We examine two quantum operations, the Permutation Test and the Circle Test, which test the identity of n quantum states. These operations naturally extend the well-studied Swap Test on two quantum states. We first show the optimality of the Permutation Test for any input size n as well as the optimality of the Circle Test for three input states. In particular, when n=3, we present a semi-classical protocol, incorporated with the Swap Test, which approximates the Circle Test efficiently. Furthermore, we show that, with help of classical preprocessing, a single use of the Circle Test can approximate the Permutation Test efficiently for an arbitrary input size n.},
  file         = {:http\://arxiv.org/pdf/0809.2037v1:PDF},
  keywords     = {quant-ph},
}

@Article{Hikima2021,
  author      = {Yasunari Hikima and Atsushi Iwasaki and Ken Umeno},
  date        = {2021-03-19},
  title       = {The reference distributions of Maurer's universal statistical test and its improved tests},
  eprint      = {2103.10660},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Maurer's universal statistical test can widely detect non-randomness of given sequences. Coron proposed an improved test, and further Yamamoto and Liu proposed a new test based on Coron's test. These tests use normal distributions as their reference distributions, but the soundness has not been theoretically discussed so far. Additionally, Yamamoto and Liu's test uses an experimental value as the variance of its reference distribution. In this paper, we theoretically derive the variance of the reference distribution of Yamamoto and Liu's test and prove that the true reference distribution of Coron's test converges to a normal distribution in some sense. We can apply the proof to the other tests with small changes.},
  file        = {:http\://arxiv.org/pdf/2103.10660v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Xu2019,
  author      = {Tianyin Xu and Owolabi Legunsen},
  date        = {2019-05-29},
  title       = {Configuration Testing: Testing Configuration Values as Code and with Code},
  eprint      = {1905.12195},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {This paper proposes configuration testing--evaluating configuration values (to be deployed) by exercising the code that uses the values and assessing the corresponding program behavior. We advocate that configuration values should be systematically tested like software code and that configuration testing should be a key reliability engineering practice for preventing misconfigurations from production deployment. The essential advantage of configuration testing is to put the configuration values (to be deployed) in the context of the target software program under test. In this way, the dynamic effects of configuration values and the impact of configuration changes can be observed during testing. Configuration testing overcomes the fundamental limitations of de facto approaches to combatting misconfigurations, namely configuration validation and software testing--the former is disconnected from code logic and semantics, while the latter can hardly cover all possible configuration values and their combinations. Our preliminary results show the effectiveness of configuration testing in capturing real-world misconfigurations. We present the principles of writing new configuration tests and the promises of retrofitting existing software tests to be configuration tests. We discuss new adequacy and quality metrics for configuration testing. We also explore regression testing techniques to enable incremental configuration testing during continuous integration and deployment in modern software systems.},
  file        = {:http\://arxiv.org/pdf/1905.12195v2:PDF},
  keywords    = {cs.SE},
}

@Article{Lu2019,
  author      = {Zeng-Hua Lu},
  date        = {2019-11-12},
  title       = {Extended MinP Tests of Multiple Hypotheses},
  eprint      = {1911.04696},
  eprintclass = {econ.EM},
  eprinttype  = {arXiv},
  abstract    = {Much empirical research in economics and finance involves simultaneously testing multiple hypotheses. This paper proposes extended MinP (EMinP) tests by expanding the minimand set of the MinP test statistic to include the $p$% -value of a global test such as a likelihood ratio test. We show that, compared with MinP tests, EMinP tests may considerably improve the global power in rejecting the intersection of all individual hypotheses. Compared with closed tests EMinP tests have the computational advantage by sharing the benefit of the stepdown procedure of MinP tests and can have a better global power over the tests used to construct closed tests. Furthermore, we argue that EMinP tests may be viewed as a tool to prevent data snooping when two competing tests that have distinct global powers are exploited. Finally, the proposed tests are applied to an empirical application on testing the effects of exercise.},
  file        = {:http\://arxiv.org/pdf/1911.04696v1:PDF},
  keywords    = {econ.EM},
}

@Article{Wildandyawan2020,
  author      = {Adrian Wildandyawan and Yasuharu Nishi},
  date        = {2020-02-12},
  title       = {Object-based Metamorphic Testing through Image Structuring},
  eprint      = {2002.07046},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Testing software is often costly due to the need of mass-producing test cases and providing a test oracle for it. This is often referred to as the oracle problem. One method that has been proposed in order to alleviate the oracle problem is metamorphic testing. Metamorphic testing produces new test cases by altering an existing test case, and uses the metamorphic relation between the inputs and the outputs of the System Under Test (SUT) to predict the expected outputs of the produced test cases. Metamorphic testing has often been used for image processing software, where changes are applied to the image's attributes to create new test cases with annotations that are the same as the original image. We refer to this existing method as the image-based metamorphic testing. In this research, we propose an object-based metamorphic testing and a composite metamorphic testing which combines different metamorphic testing approaches to relatively increase test coverage.},
  file        = {:http\://arxiv.org/pdf/2002.07046v1:PDF},
  keywords    = {cs.LG, cs.SE},
}

@Article{Bures2018,
  author      = {Miroslav Bures and Bestoun S. Ahmed},
  date        = {2018-02-22},
  title       = {Employment of Multiple Algorithms for Optimal Path-based Test Selection Strategy},
  doi         = {10.1016/j.infsof.2019.06.006},
  eprint      = {1802.08005},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Executing various sequences of system functions in a system under test represents one of the primary techniques in software testing. The natural way to create effective, consistent and efficient test sequences is to model the system under test and employ an algorithm to generate the tests that satisfy a defined test coverage criterion. Several criteria of test set optimality can be defined. In addition, to optimize the test set from an economic viewpoint, the priorities of the various parts of the system model under test must be defined. Using this prioritization, the test cases exercise the high priority parts of the system under test more intensely than those with low priority. Evidence from the literature and our observations confirm that finding a universal algorithm that produces an optimal test set for all test coverage and test set optimality criteria is a challenging task. Moreover, for different individual problem instances, different algorithms provide optimal results. In this paper, we present a path-based strategy to perform optimal test selection. The strategy first employs a set of current algorithms to generate test sets; then, it assesses the optimality of each test set by the selected criteria, and finally, chooses the optimal test set. The experimental results confirm the validity and usefulness of this strategy. For individual instances of 50 system under test models, different algorithms provided optimal results; these results varied by the required test coverage level, the size of the priority parts of the model, and the selected test set optimality criteria.},
  file        = {:http\://arxiv.org/pdf/1802.08005v1:PDF},
  keywords    = {cs.SE},
}

@Article{Bures2019,
  author      = {Miroslav Bures and Bestoun S. Ahmed and Kamal Z. Zamli},
  date        = {2019-03-20},
  title       = {Prioritized Process Test: An Alternative to Current Process Testing Strategies},
  eprint      = {1903.08531},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Testing processes and workflows in information and Internet of Things systems is a major part of the typical software testing effort. Consistent and efficient path-based test cases are desired to support these tests. Because certain parts of software system workflows have a higher business priority than others, this fact has to be involved in the generation of test cases. In this paper, we propose a Prioritized Process Test (PPT), which is a model-based test case generation algorithm that represents an alternative to currently established algorithms that use directed graphs and test requirements to model the system under test. The PPT accepts a directed multigraph as a model to express priorities, and edge weights are used instead of test requirements. To determine the test-coverage level of test cases, a test-depth-level concept is used. We compared the presented PPT with five alternatives (i.e., the Process Cycle Test, a naive reduction of test set created by the Process Cycle Test, Brute Force algorithm, Set-covering Based Solution and Matching-based Prefix Graph Solution) for edge coverage and edge-pair coverage. To assess the optimality of the path-based test cases produced by these strategies, we used fourteen metrics based on the properties of these test cases and 59 models that were created for three real-world systems. For all edge coverage, the PPT produced more optimal test cases than the alternatives in terms of the majority of the metrics. For edge-pair coverage, the PPT strategy yielded similar results to those of the alternatives. Thus, the PPT strategy is an applicable alternative, as it reflects both the required test coverage level and the business priority in parallel.},
  file        = {:http\://arxiv.org/pdf/1903.08531v1:PDF},
  keywords    = {cs.SE},
}

@Article{Kayes2013,
  author      = {Imrul Kayes and Jacob Chakareski},
  date        = {2013-11-17},
  title       = {ComReg: A Complex Network Approach to Prioritize Test Cases for Regression Testing},
  eprint      = {1311.4176},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Regression testing is performed to provide confidence that changes in a part of software do not affect other parts of the software. An execution of all existing test cases is the best way to re-establish this confidence. However, regression testing is an expensive process---there might be insufficient resources (e.g., time, workforce) to allow for the re-execution of all test cases. Regression test prioritization techniques attempt to re-order a regression test suite based on some criteria so that highest priority test cases are executed earlier. In this study, we want to prioritize test cases for regression testing based on the dependency network of faults. In software testing, it is common that some faults are consequences of other faults (leading faults). Moreover, dependent faults can be removed if and only if the leading faults have been removed. Our goal is to prioritize test cases so that test cases that exposed leading faults (the most central faults in the fault dependency network) in the system testing phase, are executed first in regression testing. We present ComReg, a test case prioritization technique based on the dependency network of faults. We model a fault dependency network as a directed graph and identify leading faults to prioritize test cases for regression testing. We use a centrality aggregation technique which considers six network representative centrality metrics to identify leading faults in the fault dependency network. We also discuss the use of fault communities to select an arbitrary percentage of the test cases from a prioritized regression test suite. We conduct a case study that evaluates the effectiveness and applicability of the proposed method.},
  file        = {:http\://arxiv.org/pdf/1311.4176v3:PDF},
  keywords    = {cs.SE},
}

@Article{Cohen2009,
  author      = {Ernie Cohen},
  date        = {2009-10-06},
  title       = {Pessimistic Testing},
  eprint      = {0910.0996},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {We propose a new approach to testing conformance to a nondeterministic specification, in which testing proceeds only as long as increased test coverage is guaranteed.},
  file        = {:http\://arxiv.org/pdf/0910.0996v1:PDF},
  keywords    = {cs.SE, cs.GT, D.2.5},
}

@Article{Isabella2012,
  author       = {A. Isabella and Emi Retna},
  date         = {2012-02-21},
  journaltitle = {International Journal of Software Engineering & Applications (IJSEA), Vol.3, No.1, January 2012},
  title        = {Study Paper on Test Case generation for GUI Based Testing},
  eprint       = {1202.4527},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {With the advent of WWW and outburst in technology and software development, testing the software became a major concern. Due to the importance of the testing phase in a software development life cycle, testing has been divided into graphical user interface (GUI) based testing, logical testing, integration testing, etc.GUI Testing has become very important as it provides more sophisticated way to interact with the software. The complexity of testing GUI increased over time. The testing needs to be performed in a way that it provides effectiveness, efficiency, increased fault detection rate and good path coverage. To cover all use cases and to provide testing for all possible (success/failure) scenarios the length of the test sequence is considered important. Intent of this paper is to study some techniques used for test case generation and process for various GUI based software applications.},
  file         = {:http\://arxiv.org/pdf/1202.4527v1:PDF},
  keywords     = {cs.SE},
}

@Article{Bassil2012,
  author      = {Youssef Bassil},
  date        = {2012-03-24},
  title       = {Distributed, Cross-Platform, and Regression Testing Architecture for Service-Oriented Architecture},
  eprint      = {1203.5403},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {As per leading IT experts, today's large enterprises are going through business transformations. They are adopting service-based IT models such as SOA to develop their enterprise information systems and applications. In fact, SOA is an integration of loosely-coupled interoperable components, possibly built using heterogeneous software technologies and hardware platforms. As a result, traditional testing architectures are no more adequate for verifying and validating the quality of SOA systems and whether they are operating to specifications. This paper first discusses the various state-of-the-art methods for testing SOA applications, and then it proposes a novel automated, distributed, cross-platform, and regression testing architecture for SOA systems. The proposed testing architecture consists of several testing units which include test engine, test code generator, test case generator, test executer, and test monitor units. Experiments conducted showed that the proposed testing architecture managed to use parallel agents to test heterogeneous web services whose technologies were incompatible with the testing framework. As future work, testing non-functional aspects of SOA applications are to be investigated so as to allow the testing of such properties as performance, security, availability, and scalability.},
  file        = {:http\://arxiv.org/pdf/1203.5403v1:PDF},
  keywords    = {cs.SE},
}

@Article{Groce2017,
  author      = {Alex Groce and Josie Holmes},
  date        = {2017-11-05},
  title       = {Provenance and Pseudo-Provenance for Seeded Learning-Based Automated Test Generation},
  eprint      = {1711.01661},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {Many methods for automated software test generation, including some that explicitly use machine learning (and some that use ML more broadly conceived) derive new tests from existing tests (often referred to as seeds). Often, the seed tests from which new tests are derived are manually constructed, or at least simpler than the tests that are produced as the final outputs of such test generators. We propose annotation of generated tests with a provenance (trail) showing how individual generated tests of interest (especially failing tests) derive from seed tests, and how the population of generated tests relates to the original seed tests. In some cases, post-processing of generated tests can invalidate provenance information, in which case we also propose a method for attempting to construct "pseudo-provenance" describing how the tests could have been (partly) generated from seeds.},
  file        = {:http\://arxiv.org/pdf/1711.01661v2:PDF},
  keywords    = {stat.ML, cs.SE},
}

@Article{Erlenhov2020,
  author      = {Linda Erlenhov and Francisco Gomes de Oliveira Neto and Martin Chukaleski and Samer Daknache},
  date        = {2020-04-21},
  title       = {Challenges and guidelines on designing test cases for test bots},
  doi         = {10.1145/3387940.3391535},
  eprint      = {2004.10143},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Test bots are automated testing tools that autonomously and periodically run a set of test cases that check whether the system under test meets the requirements set forth by the customer. The automation decreases the amount of time a development team spends on testing. As development projects become larger, it is important to focus on improving the test bots by designing more effective test cases because otherwise time and usage costs can increase greatly and misleading conclusions from test results might be drawn, such as false positives in the test execution. However, literature currently lacks insights on how test case design affects the effectiveness of test bots. This paper uses a case study approach to investigate those effects by identifying challenges in designing tests for test bots. Our results include guidelines for test design schema for such bots that support practitioners in overcoming the challenges mentioned by participants during our study.},
  file        = {:http\://arxiv.org/pdf/2004.10143v1:PDF},
  keywords    = {cs.SE},
}

@Article{Cheng2021,
  author      = {Xiwei Cheng and Sidharth Jaggi and Qiaoqiao Zhou},
  date        = {2021-02-20},
  title       = {Generalized Non-adaptive Group Testing},
  eprint      = {2102.10256},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  abstract    = {In the problem of classical group testing one aims to identify a small subset (of expected size $d$) diseased individuals/defective items in a large population (of size $n$) based on a minimal number of suitably-designed group non-adaptive tests on subsets of items, where the test outcome is governed by an "OR" function, i.e., the test outcome is positive iff the given test contains at least one defective item. Motivated by physical considerations we consider a generalized scenario (that includes as special cases multiple other group-testing-like models in the literature) wherein the test outcome is governed by an arbitrary $\textit{monotone}$ (stochastic) test function $f(\cdot)$, with the test outcome being positive with probability $f(x)$, where $x$ is the number of defectives tested in that pool. For any monotone test function $f(\cdot)$ we present a non-adaptive generalized group-testing scheme that identifies all defective items with high probability. Our scheme requires at most ${\cal O}(d^2\log(n))$ tests for any monotone test function $f(\cdot)$, and at most ${\cal O}(d\log(n))$ in the physically relevant sub-class of $\textit{sensitive}$ test functions (and hence is information-theoretically order-optimal for this sub-class).},
  file        = {:http\://arxiv.org/pdf/2102.10256v1:PDF},
  keywords    = {cs.IT, math.IT},
}

@Article{Tsumura2009,
  author      = {Yu Tsumura},
  date        = {2009-12-29},
  title       = {Primality tests for 2^kn-1 using elliptic curves},
  eprint      = {0912.5279},
  eprintclass = {math.NT},
  eprinttype  = {arXiv},
  abstract    = {We propose some primality tests for 2^kn-1, where k, n in Z, k>= 2 and n odd. There are several tests depending on how big n is. These tests are proved using properties of elliptic curves. Essentially, the new primality tests are the elliptic curve version of the Lucas-Lehmer-Riesel primality test. Note:An anonymous referee suggested that Benedict H. Gross already proved the same result about a primality test for Mersenne primes using elliptic curve.},
  file        = {:http\://arxiv.org/pdf/0912.5279v1:PDF},
  keywords    = {math.NT, 11Y11; 11Y05},
}

@Article{Polivaev2012,
  author       = {Dimitry Polivaev},
  date         = {2012-02-28},
  journaltitle = {EPTCS 80, 2012, pp. 103-114},
  title        = {Rule-based Test Generation with Mind Maps},
  doi          = {10.4204/EPTCS.80.8},
  eprint       = {1202.6125},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {This paper introduces basic concepts of rule based test generation with mind maps, and reports experiences learned from industrial application of this technique in the domain of smart card testing by Giesecke & Devrient GmbH over the last years. It describes the formalization of test selection criteria used by our test generator, our test generation architecture and test generation framework.},
  file         = {:http\://arxiv.org/pdf/1202.6125v1:PDF},
  keywords     = {cs.SE},
}

@Article{Dorman2014,
  author      = {A. M. Dorman},
  date        = {2014-08-30},
  title       = {Computerized Multi Microphone Test System},
  eprint      = {1409.0117},
  eprintclass = {cs.SD},
  eprinttype  = {arXiv},
  abstract    = {An acoustic testing approach based on the concept of a microphone sensor surrounding the product under test is proposed. Microphone signals are processed simultaneously by a test system computer, according to the objective of the test. The spatial and frequency domain selectivity features of this method are examined. Sound-spatial visualization algorithm is observed. A test system design based on the concept of a microphone surrounding the tested product has the potential to improve distortion measurement accuracy.},
  file        = {:http\://arxiv.org/pdf/1409.0117v1:PDF},
  keywords    = {cs.SD},
}

@Article{Ryabko2006,
  author      = {Boris Ryabko and Jaakko Astola},
  date        = {2006-02-25},
  title       = {Universal Codes as a Basis for Time Series Testing},
  eprint      = {cs/0602084},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  abstract    = {We suggest a new approach to hypothesis testing for ergodic and stationary processes. In contrast to standard methods, the suggested approach gives a possibility to make tests, based on any lossless data compression method even if the distribution law of the codeword lengths is not known. We apply this approach to the following four problems: goodness-of-fit testing (or identity testing), testing for independence, testing of serial independence and homogeneity testing and suggest nonparametric statistical tests for these problems. It is important to note that practically used so-called archivers can be used for suggested testing.},
  file        = {:http\://arxiv.org/pdf/cs/0602084v1:PDF},
  keywords    = {cs.IT, math.IT},
}

@Article{McKague2016,
  author      = {Matthew McKague},
  date        = {2016-09-30},
  title       = {Self-testing in parallel with CHSH},
  eprint      = {1609.09584},
  eprintclass = {quant-ph},
  eprinttype  = {arXiv},
  abstract    = {Self-testing allows classical referees to verify the quantum behaviour of some untrusted devices. Recently we developed a framework for building large self-tests by repeating a smaller self-test many times in parallel. However, the framework did not apply to the CHSH test, which tests a maximally entangled pair of qubits. CHSH is the most well known and widely used test of this type. Here we extend the parallel self-testing framework to build parallel CHSH self-tests for any number of pairs of maximally entangled qubits. Our construction achieves an error bound which is polynomial in the number of tested qubit pairs.},
  file        = {:http\://arxiv.org/pdf/1609.09584v4:PDF},
  keywords    = {quant-ph},
}

@Article{Cuparic2018,
  author      = {Marija Cupari and Bojana Miloevi and Marko Obradovi},
  date        = {2018-09-20},
  title       = {New $L^2$-type exponentiality tests},
  eprint      = {1809.07585},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {We introduce new consistent and scale-free goodness-of-fit tests for the exponential distribution based on Puri-Rubin characterization. For the construction of test statistics we employ weighted $L^2$ distance between $V$-empirical Laplace transforms of random variables that appear in the characterization. The resulting test statistics are degenerate V-statistics with estimated parameters. We compare our tests, in terms of the Bahadur efficiency, to the likelihood ratio test, as well as some recent characterization based goodness-of-fit tests for the exponential distribution. We also compare the powers of our tests to the powers of some recent and classical exponentiality tests. In both criteria, our tests are shown to be strong and outperform most of their competitors.},
  file        = {:http\://arxiv.org/pdf/1809.07585v1:PDF},
  keywords    = {stat.ME, 62G10, 62G20},
}

@Article{Charbachi2017,
  author      = {Peter Charbachi and Linus Eklund and Eduard Enoiu},
  date        = {2017-06-06},
  title       = {Can Pairwise Testing Perform Comparably to Manually Handcrafted Testing Carried Out by Industrial Engineers?},
  eprint      = {1706.01636},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Testing is an important activity in engineering of industrial software. For such software, testing is usually performed manually by handcrafting test suites based on specific design techniques and domain-specific experience. To support developers in testing, different approaches for producing good test suites have been proposed. In the last couple of years combinatorial testing has been explored with the goal of automatically combining the input values of the software based on a certain strategy. Pairwise testing is a combinatorial technique used to generate test suites by varying the values of each pair of input parameters to a system until all possible combinations of those parameters are created. There is some evidence suggesting that these kinds of techniques are efficient and relatively good at detecting software faults. Unfortunately, there is little experimental evidence on the comparison of these combinatorial testing techniques with, what is perceived as, rigorous manually handcrafted testing. In this study we compare pairwise test suites with test suites created manually by engineers for 45 industrial programs. The test suites were evaluated in terms of fault detection, code coverage and number of tests. The results of this study show that pairwise testing, while useful for achieving high code coverage and fault detection for the majority of the programs, is almost as effective in terms of fault detection as manual testing. The results also suggest that pairwise testing is just as good as manual testing at fault detection for 64% of the programs.},
  file        = {:http\://arxiv.org/pdf/1706.01636v1:PDF},
  keywords    = {cs.SE},
}

@Article{Kampmann2019,
  author      = {Alexander Kampmann and Andreas Zeller},
  date        = {2019-06-04},
  title       = {Bridging the Gap between Unit Test Generation and System Test Generation},
  eprint      = {1906.01463},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Common test generators fall into two categories. Generating test inputs at the unit level is fast, but can lead to false alarms when a function is called with inputs that would not occur in a system context. If a generated input at the system level causes a failure, this is a true alarm, as the input could also have come from the user or a third party; but system testing is much slower. In this paper, we introduce the concept of a test generation bridge, which joins the accuracy of system testing with the speed of unit testing. A Test Generation Bridge allows to combine an arbitrary system test generator with an arbitrary unit test generator. It does so by carving parameterized unit tests from system (test) executions. These unit tests run in a context recorded from the system test, but individual parameters are left free for the unit test generator to systematically explore. This allows symbolic test generators such as KLEE to operate on individual functions in the recorded system context. If the test generator detects a failure, we lift the failure-inducing parameter back to the system input; if the failure can be reproduced at the system level, it is reported as a true alarm. Our BASILISK prototype can extract and test units out of complex systems such as a Web/Python/SQLite/C stack; in its evaluation, it achieves a higher coverage than a state-of-the-art system test generator.},
  file        = {:http\://arxiv.org/pdf/1906.01463v1:PDF},
  keywords    = {cs.SE},
}

@Article{Junior2020,
  author      = {Nildo Silva Junior and Larissa Rocha and Luana Almeida Martins and Ivan Machado},
  date        = {2020-03-12},
  title       = {A survey on test practitioners' awareness of test smells},
  eprint      = {2003.05613},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Developing test code may be a time-consuming task that usually requires much effort and cost, especially when it is done manually. Besides, during this process, developers and testers are likely to adopt bad design choices, which may lead to the introduction of the so-called test smells in test code. Test smells are bad solutions to either implement or design test code. As the test code with test smells increases in size, these tests might become more complex, and as a consequence, much harder to understand and evolve correctly. Therefore, test smells may have a negative impact on the quality and maintenance of test code and may also harm the whole software testing activities. In this context, this study aims to understand whether test professionals non-intentionally insert test smells. We carried out an expert survey to analyze the usage frequency of a set of test smells. Sixty professionals from different companies participated in the survey. We selected 14 widely studied smells from the literature, which are also implemented in existing test smell detection tools. The yielded results indicate that experienced professionals introduce test smells during their daily programming tasks, even when they are using standardized practices from their companies, and not only for their personal assumptions. Another relevant evidence was that developers' professional experience can not be considered as a root-cause for the insertion of test smells in test code.},
  file        = {:http\://arxiv.org/pdf/2003.05613v1:PDF},
  keywords    = {cs.SE},
}

@Article{Tufano2020,
  author      = {Michele Tufano and Dawn Drain and Alexey Svyatkovskiy and Shao Kun Deng and Neel Sundaresan},
  date        = {2020-09-11},
  title       = {Unit Test Case Generation with Transformers},
  eprint      = {2009.05617},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Automated Unit Test Case generation has been the focus of extensive literature within the research community. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult to read or understand for developers. In this paper we propose AthenaTest, an approach that aims at generating unit test cases by learning from real-world, developer-written test cases. Our approach relies on a state-of-the-art sequence-to-sequence transformer model which is able to write useful test cases for a given method under test (i.e., focal method). We also introduce methods2test - the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 630k test cases mined from 70k open-source repositories hosted on GitHub. We use this dataset to train a transformer model to translate focal methods into the corresponding test cases. We evaluate the ability of our model in generating test cases using natural language processing as well as code-specific criteria. First, we assess the quality of the translation compared to the target test case, then we analyze properties of the test case such as syntactic correctness and number and variety of testing APIs (e.g., asserts). We execute the test cases, collect test coverage information, and compare them with test cases generated by EvoSuite and GPT-3. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated test cases.},
  file        = {:http\://arxiv.org/pdf/2009.05617v1:PDF},
  keywords    = {cs.SE, cs.CL, cs.LG},
}

@Article{Chen2020,
  author      = {Junjie Chen and Ming Yan and Zan Wang and Yuning Kang and Zhuo Wu},
  date        = {2020-10-10},
  title       = {Deep Neural Network Test Coverage: How Far Are We?},
  eprint      = {2010.04946},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {DNN testing is one of the most effective methods to guarantee the quality of DNN. In DNN testing, many test coverage metrics have been proposed to measure test effectiveness, including structural coverage and non-structural coverage (which are classified according to whether considering which structural elements are covered during testing). Those test coverage metrics are proposed based on the assumption: they are correlated with test effectiveness (i.e., the generation of adversarial test inputs or the error-revealing capability of test inputs in DNN testing studies). However, it is still unknown whether the assumption is tenable. In this work, we conducted the first extensive study to systematically validate the assumption by controlling for the size of test sets. In the study, we studied seven typical test coverage metrics based on 9 pairs of datasets and models with great diversity (including four pairs that have never been used to evaluate these test coverage metrics before). The results demonstrate that the assumption fails for structural coverage in general but holds for non-structural coverage on more than half of subjects, indicating that measuring the difference of DNN behaviors between test inputs and training data is more promising than measuring which structural elements are covered by test inputs for measuring test effectiveness. Even so, the current non-structural coverage metrics still can be improved from several aspects such as unfriendly parameters and unstable performance. That indicates that although a lot of test coverage metrics have been proposed before, there is still a lot of room for improvement of measuring test effectiveness in DNN testing, and our study has pointed out some promising directions.},
  file        = {:http\://arxiv.org/pdf/2010.04946v2:PDF},
  keywords    = {cs.SE},
}

@Article{Maragathavalli2011,
  author      = {P. Maragathavalli},
  date        = {2011-03-01},
  title       = {Search-based software test data generation using evolutionary computation},
  eprint      = {1103.0125},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Search-based Software Engineering has been utilized for a number of software engineering activities. One area where Search-Based Software Engineering has seen much application is test data generation. Evolutionary testing designates the use of metaheuristic search methods for test case generation. The search space is the input domain of the test object, with each individual or potential solution, being an encoded set of inputs to that test object. The fitness function is tailored to find test data for the type of test that is being undertaken. Evolutionary Testing (ET) uses optimizing search techniques such as evolutionary algorithms to generate test data. The effectiveness of GA-based testing system is compared with a Random testing system. For simple programs both testing systems work fine, but as the complexity of the program or the complexity of input domain grows, GA-based testing system significantly outperforms Random testing.},
  file        = {:http\://arxiv.org/pdf/1103.0125v1:PDF},
  keywords    = {cs.SE},
}

@Article{Panda2019,
  author      = {Sambit Panda and Cencheng Shen and Ronan Perry and Jelle Zorn and Antoine Lutz and Carey E. Priebe and Joshua T. Vogelstein},
  date        = {2019-10-20},
  title       = {Nonparametric MANOVA via Independence Testing},
  eprint      = {1910.08883},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {The $k$-sample testing problem tests whether or not $k$ groups of data points are sampled from the same distribution. Multivariate analysis of variance (MANOVA) is currently the gold standard for $k$-sample testing but makes strong, often inappropriate, parametric assumptions. Moreover, independence testing and $k$-sample testing are tightly related, and there are many nonparametric multivariate independence tests with strong theoretical and empirical properties, including distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion (Hsic). We prove that universally consistent independence tests achieve universally consistent $k$-sample testing and that $k$-sample statistics like Energy and Maximum Mean Discrepancy (MMD) are exactly equivalent to Dcorr. Empirically evaluating these tests for $k$-sample scenarios demonstrates that these nonparametric independence tests typically outperform MANOVA, even for Gaussian distributed settings. Finally, we extend these non-parametric $k$-sample testing procedures to perform multiway and multilevel tests. Thus, we illustrate the existence of many theoretically motivated and empirically performant $k$-sample tests. A Python package with all independence and k-sample tests called hyppo is available from https://hyppo.neurodata.io/.},
  file        = {:http\://arxiv.org/pdf/1910.08883v2:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Bertolotti2020,
  author      = {Paolo Bertolotti and Ali Jadbabaie},
  date        = {2020-12-04},
  title       = {Network Group Testing},
  eprint      = {2012.02847},
  eprintclass = {stat.AP},
  eprinttype  = {arXiv},
  abstract    = {We consider the problem of identifying infected individuals in a population of size $N$. Group testing provides an approach to test the entire population using significantly fewer than $N$ tests when infection prevalence is low. The original and most commonly utilized form of group testing, called Dorfman testing, treats each individual's infection probability as independent and homogenous. However, as communicable diseases spread from individual to individual through underlying social networks, an individual's network location affects their infection probability. In this work, we utilize network information to improve group testing. Specifically, we group individuals by community and demonstrate the performance gain over Dorfman testing. After introducing a network and epidemic model, we derive the number of tests used under network grouping. We prove the expected number of tests is upper bounded by Dorfman testing. In addition, we demonstrate network grouping successfully achieves the theoretical lower bound for two-stage testing procedures when networks have strong community structure. On the other hand, network grouping is equivalent to Dorfman testing when networks have no structure. We end by demonstrating network grouping outperforms Dorfman testing in the scenario of a university testing its population for COVID-19 cases.},
  file        = {:http\://arxiv.org/pdf/2012.02847v1:PDF},
  keywords    = {stat.AP},
}

@Article{Romano2021,
  author      = {Alan Romano and Zihe Song and Sampath Grandhi and Wei Yang and Weihang Wang},
  date        = {2021-03-03},
  title       = {An Empirical Analysis of UI-based Flaky Tests},
  eprint      = {2103.02669},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Flaky tests have gained attention from the research community in recent years and with good reason. These tests lead to wasted time and resources, and they reduce the reliability of the test suites and build systems they affect. However, most of the existing work on flaky tests focus exclusively on traditional unit tests. This work ignores UI tests that have larger input spaces and more diverse running conditions than traditional unit tests. In addition, UI tests tend to be more complex and resource-heavy, making them unsuited for detection techniques involving rerunning test suites multiple times. In this paper, we perform a study on flaky UI tests. We analyze 235 flaky UI test samples found in 62 projects from both web and Android environments. We identify the common underlying root causes of flakiness in the UI tests, the strategies used to manifest the flaky behavior, and the fixing strategies used to remedy flaky UI tests. The findings made in this work can provide a foundation for the development of detection and prevention techniques for flakiness arising in UI tests.},
  file        = {:http\://arxiv.org/pdf/2103.02669v1:PDF},
  keywords    = {cs.SE},
}

@Article{Peruma2021,
  author      = {Anthony Peruma and Christian D. Newman},
  date        = {2021-03-17},
  title       = {On the Distribution of "Simple Stupid Bugs" in Unit Test Files: An Exploratory Study},
  eprint      = {2103.09388},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {A key aspect of ensuring the quality of a software system is the practice of unit testing. Through unit tests, developers verify the correctness of production source code, thereby verifying the system's intended behavior under test. However, unit test code is subject to issues, ranging from bugs in the code to poor test case design (i.e., test smells). In this study, we compare and contrast the occurrences of a type of single-statement-bug-fix known as "simple stupid bugs" (SStuBs) in test and non-test (i.e., production) files in popular open-source Java Maven projects. Our results show that SStuBs occur more frequently in non-test files than in test files, with most fix-related code associated with assertion statements in test files. Further, most test files exhibiting SStuBs also exhibit test smells. We envision our findings enabling tool vendors to better support developers in improving the maintenance of test suites.},
  file        = {:http\://arxiv.org/pdf/2103.09388v1:PDF},
  keywords    = {cs.SE},
}

@Article{Swain2012,
  author      = {Ranjita Kumari Swain and Prafulla Kumar Behera and Durga Prasad Mohapatra},
  date        = {2012-06-02},
  title       = {Generation and Optimization of Test cases for Object-Oriented Software Using State Chart Diagram},
  eprint      = {1206.0373},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {The process of testing any software system is an enormous task which is time consuming and costly. The time and required effort to do sufficient testing grow, as the size and complexity of the software grows, which may cause overrun of the project budget, delay in the development of software system or some test cases may not be covered. During SDLC (software development life cycle), generally the software testing phase takes around 40-70% of the time and cost. State-based testing is frequently used in software testing. Test data generation is one of the key issues in software testing. A properly generated test suite may not only locate the errors in a software system, but also help in reducing the high cost associated with software testing. It is often desired that test data in the form of test sequences within a test suite can be automatically generated to achieve required test coverage. This paper proposes an optimization approach to test data generation for the state-based software testing. In this paper, first state transition graph is derived from state chart diagram. Then, all the required information are extracted from the state chart diagram. Then, test cases are generated. Lastly, a set of test cases are minimized by calculating the node coverage for each test case. It is also determined that which test cases are covered by other test cases. The advantage of our test generation technique is that it optimizes test coverage by minimizing time and cost. The proposed test data generation scheme generates test cases which satisfy transition path coverage criteria, path coverage criteria and action coverage criteria. A case study on Automatic Ticket Machine (ATM) has been presented to illustrate our approach.},
  file        = {:http\://arxiv.org/pdf/1206.0373v1:PDF},
  keywords    = {cs.SE},
}

@Article{Jeevarathinam2010,
  author       = {Mrs. R. Jeevarathinam and Dr. Antony Selvadoss Thanamani},
  date         = {2010-02-10},
  journaltitle = {International Journal of Computer Science and Information Security, IJCSIS, Vol. 7, No. 1, pp. 190-195, January 2010, USA},
  title        = {Test Case Generation using Mutation Operators and Fault Classification},
  eprint       = {1002.2197},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Software testing is the important phase of software development process. But, this phase can be easily missed by software developers because of their limited time to complete the project. Since, software developers finish their software nearer to the delivery time; they dont get enough time to test their program by creating effective test cases. . One of the major difficulties in software testing is the generation of test cases that satisfy the given adequacy criterion Moreover, creating manual test cases is a tedious work for software developers in the final rush hours. A new approach which generates test cases can help the software developers to create test cases from software specifications in early stage of software development (before coding) and as well as from program execution traces from after software development (after coding). Heuristic techniques can be applied for creating quality test cases. Mutation testing is a technique for testing software units that has great potential for improving the quality of testing, and to assure the high reliability of software. In this paper, a mutation testing based test cases generation technique has been proposed to generate test cases from program execution trace, so that the test cases can be generated after coding. The paper details about the mutation testing implementation to generate test cases. The proposed algorithm has been demonstrated for an example.},
  file         = {:http\://arxiv.org/pdf/1002.2197v1:PDF},
  keywords     = {cs.SE},
}

@Article{Haramoto2019,
  author      = {Hiroshi Haramoto},
  date        = {2019-12-23},
  title       = {Study on upper limit of sample sizes for a two-level test in NIST SP800-22},
  eprint      = {1912.10602},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {NIST SP800-22 is one of the most widely used statistical testing tools for pseudorandom number generators (PRNGs). This tool consists of 15 tests (one-level tests) and two additional tests (two-level tests). Each one-level test provides one or more $p$-values. The two-level tests measure the uniformity of the obtained $p$-values for a fixed one-level test. One of the two-level tests categorizes the $p$-values into ten intervals of equal length, and apply a chi-squared goodness-of-fit test. This two-level test is often more powerful than one-level tests, but sometimes it rejects even good PRNGs when the sample size at the second level is too large, since it detects approximation errors in the computation of $p$-values. In this paper, we propose a practical upper limit of the sample size in this two-level test, for each of six tests appeared in SP800-22. These upper limits are derived by the chi-squared discrepancy between the distribution of the approximated $p$-values and the uniform distribution $U(0, 1)$. We also computed a "risky" sample size at the second level for each one-level test. Our experiments show that the two-level test with the proposed upper limit gives appropriate results, while using the risky size often rejects even good PRNGs. We also propose another improvement: to use the exact probability for the ten categories in the computation of goodness-of-fit at the two-level test. This allows us to increase the sample size at the second level, and would make the test more sensitive than the NIST's recommending usage.},
  file        = {:http\://arxiv.org/pdf/1912.10602v3:PDF},
  keywords    = {stat.ME, cs.CR, 65C10, 65C60, 68N30},
}

@Article{Berrahou2012,
  author      = {Noureddine Berrahou and Lahcen Douge},
  date        = {2012-11-07},
  title       = {Bahadur efficiency of nonparametric test for independence based on $L_1$-error},
  eprint      = {1211.1725},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {We introduce new test statistic to test the independence of two multi-dimensional random variables. Based on the $L_1$-distance and the historgram density estimation method, the test is compared via Bahadur relative efficiency to several tests available in the literature. It arises that our test reaches better performances than a number of usual tests among whom we cite the Kolmogorov-Smirnov test. Beforehand, large deviation result is stated for the associated statistic. The local asymptotic optimality relative to the test is also studied.},
  file        = {:http\://arxiv.org/pdf/1211.1725v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Chakraborty2014,
  author      = {Anirvan Chakraborty and Probal Chaudhuri},
  date        = {2014-03-02},
  title       = {A Wilcoxon-Mann-Whitney type test for infinite dimensional data},
  eprint      = {1403.0201},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {The Wilcoxon-Mann-Whitney test is a robust competitor of the t-test in the univariate setting. For finite dimensional multivariate data, several extensions of the Wilcoxon-Mann-Whitney test have been shown to have better performance than Hotelling's $T^{2}$ test for many non-Gaussian distributions of the data. In this paper, we study a Wilcoxon-Mann-Whitney type test based on spatial ranks for data in infinite dimensional spaces. We demonstrate the performance of this test using some real and simulated datasets. We also investigate the asymptotic properties of the proposed test and compare the test with a wide range of competing tests.},
  file        = {:http\://arxiv.org/pdf/1403.0201v1:PDF},
  keywords    = {stat.ME},
}

@Article{Zhang2015,
  author      = {Xianyang Zhang},
  date        = {2015-09-28},
  title       = {Testing High Dimensional Mean Under Sparsity},
  eprint      = {1509.08444},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Motivated by the likelihood ratio test under the Gaussian assumption, we develop a maximum sum-of-squares test for conducting hypothesis testing on high dimensional mean vector. The proposed test which incorporates the dependence among the variables is designed to ease the computational burden and to maximize the asymptotic power in the likelihood ratio test. A simulation-based approach is developed to approximate the sampling distribution of the test statistic. The validity of the testing procedure is justified under both the null and alternative hypotheses. We further extend the main results to the two sample problem without the equal covariance assumption. Numerical results suggest that the proposed test can be more powerful than some existing alternatives.},
  file        = {:http\://arxiv.org/pdf/1509.08444v2:PDF},
  keywords    = {stat.ME},
}

@Article{Schepsmeier2013,
  author      = {Ulf Schepsmeier},
  date        = {2013-09-23},
  title       = {Efficient goodness-of-fit tests in multi-dimensional vine copula models},
  eprint      = {1309.5808},
  eprintclass = {stat.CO},
  eprinttype  = {arXiv},
  abstract    = {We introduce a new goodness-of-fit test for regular vine (R-vine) copula models, a flexible class of multivariate copulas based on a pair-copula construction (PCC). The test arises from the information matrix ratio. The corresponding test statistic is derived and its asymptotic normality is proven. The test's power is investigated and compared to 14 other goodness-of-fit tests, adapted from the bivariate copula case, in a high dimensional setting. The extensive simulation study shows the excellent performance with respect to size and power as well as the superiority of the information matrix ratio based test against most other goodness-of-fit tests. The best performing tests are applied to a portfolio of stock indices and their related volatility indices validating different R-vine specifications.},
  file        = {:http\://arxiv.org/pdf/1309.5808v1:PDF},
  keywords    = {stat.CO},
}

@Article{Petr2016,
  author      = {Koldanov Petr and Koldanov Alexander and Kalyagin Valeriy and Pardalos Panos},
  date        = {2016-10-02},
  title       = {Uniformly most powerful unbiased test for conditional independence in Gaussian graphical model},
  eprint      = {1610.00316},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Model selection for Gaussian concentration graph is based on multiple testing of pairwise conditional independence. In practical applications partial correlation tests are widely used. However it is not known whether partial correlation test is uniformly most powerful for pairwise conditional independence testing. This question is answered in the paper. Uniformly most powerful unbiased test of Neymann structure is obtained. It turns out, that this test can be reduced to usual partial correlation test. It implies that partial correlation test is uniformly most powerful unbiased one.},
  file        = {:http\://arxiv.org/pdf/1610.00316v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Iwasaki2017,
  author      = {Atsushi Iwasaki and Ken Umeno},
  date        = {2017-08-28},
  title       = {A new randomness test solving problems of Discrete Fourier Transform Test},
  eprint      = {1708.08218},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Discrete Fourier Transform Test (DFTT), which is a randomness test included in NIST SP800-22, has a problem. It is that theoretical reference distribution of the test statistic has not been derived. In this paper, we propose a new test using variance of power spectrum as the test statistic, whose reference distribution can be theoretically derived. The purpose of DFTT is to detect periodic features and that of the proposed test is the same. We make some experiments and show that the proposed test has stronger detection power than DFTT.},
  file        = {:http\://arxiv.org/pdf/1708.08218v1:PDF},
  keywords    = {stat.ME},
}

@Article{Poth2019,
  author       = {Alexander Poth and Quirin Beck and Andreas Riel},
  date         = {2019-06-07},
  journaltitle = {EuroAsiaSPI${}^2$ 2019, Sep 2019, Edinburgh, United Kingdom},
  title        = {Artificial Intelligence helps making Quality Assurance processes leaner},
  eprint       = {1906.02970},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still works fine in the new release. In many projects, a significant part of these regression tests are not automated and therefore executed manually. Manual tests are expensive and time intensive [1], which is why often only a relevant subset of all possible regression tests are executed in order to safe time and money. Depending on the software process, different approaches can be used to identify the right set of regression tests. The source code file level is a frequent entry point for this identification [2]. Advanced approaches combine different file level methods [3]. To handle black-box tests, methods like [4] or [5] can be used for test case prioritiza-tion. To decide which tests can be skipped, a relevance ranking of the tests in a regression test suite is needed. Based on the relevance a test is in or out of the regression test set for a specific release. This decision is a task of the test manager supported by experts. The task can be time-consuming in case of big (often a 4-to 5-digit number) regression test suites because the selection is specific to each release. Trends are going to continuous prioritization [6], which this work wants to support with the presented ML based approach for black box regression test case prioritization. Any regression test selection is made upon release specific changes. Changes can be new or deleted code based on refactoring or implementation of new features. But also changes on externals systems which are connected by interfaces have to be considered},
  file         = {:http\://arxiv.org/pdf/1906.02970v1:PDF},
  keywords     = {cs.SE},
}

@Article{Zhang2019,
  author      = {Jie M. Zhang and Mark Harman and Lei Ma and Yang Liu},
  date        = {2019-06-19},
  title       = {Machine Learning Testing: Survey, Landscapes and Horizons},
  eprint      = {1906.10742},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.},
  file        = {:http\://arxiv.org/pdf/1906.10742v2:PDF},
  keywords    = {cs.LG, cs.AI, cs.SE, stat.ML},
}

@Article{Campbell2020,
  author      = {Harlan Campbell},
  date        = {2020-04-03},
  title       = {Equivalence testing for standardized effect sizes in linear regression},
  eprint      = {2004.01757},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we introduce equivalence testing procedures for standardized effect sizes in a linear regression. We show how to define valid hypotheses and calculate p-values for these tests. Such tests are necessary to confirm the lack of a meaningful association between an outcome and predictors. A simulation study is conducted to examine type I error rates and statistical power. We also compare using equivalence testing as part of a frequentist testing scheme with an alternative Bayesian testing approach. The results indicate that the proposed equivalence test is a potentially useful tool for "testing the null."},
  file        = {:http\://arxiv.org/pdf/2004.01757v4:PDF},
  keywords    = {stat.ME},
}

@Article{Saha2020,
  author      = {Prashanta Saha and Upulee Kanewala},
  date        = {2020-04-18},
  title       = {Improving The Effectiveness of Automatically Generated Test Suites Using Metamorphic Testing},
  doi         = {10.1145/3387940.3392253},
  eprint      = {2004.08518},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Automated test generation has helped to reduce the cost of software testing. However, developing effective test oracles for these automatically generated test inputs is a challenging task. Therefore, most automated test generation tools use trivial oracles that reduce the fault detection effectiveness of these automatically generated test cases. In this work, we provide results of an empirical study showing that utilizing metamorphic relations can increase the fault detection effectiveness of automatically generated test cases.},
  file        = {:http\://arxiv.org/pdf/2004.08518v1:PDF},
  keywords    = {cs.SE},
}

@Article{Aldridge2020,
  author      = {Matthew Aldridge},
  date        = {2020-05-06},
  title       = {Conservative two-stage group testing},
  eprint      = {2005.06617},
  eprintclass = {stat.AP},
  eprinttype  = {arXiv},
  abstract    = {Inspired by applications in testing for COVID-19, we consider a variant of two-stage group testing we call "conservative" two-stage testing, where every item declared to be defective must be definitively confirmed by being tested by itself in the second stage. We study this in the linear regime where the prevalence is fixed while the number of items is large. We study various nonadaptive test designs for the first stage, and derive a new lower bound for the total number of tests required. We find that a first-stage design with constant tests per item and constant items per test due to Broder and Kumar (arXiv:2004.01684) is extremely close to optimal. Simulations back up the theoretical results.},
  file        = {:http\://arxiv.org/pdf/2005.06617v1:PDF},
  keywords    = {stat.AP, cs.IT, math.IT},
}

@Article{Modonato2020,
  author      = {Matteo Modonato},
  date        = {2020-05-19},
  title       = {Combining Dynamic Symbolic Execution, Machine Learning and Search-Based Testing to Automatically Generate Test Cases for Classes},
  eprint      = {2005.09317},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {This article discusses a new technique to automatically generate test cases for object oriented programs. At the state of the art, the problem of generating adequate sets of complete test cases has not been satisfactorily solved yet. There are various techniques to automatically generate test cases (random testing, search-based testing, etc.) but each one has its own weaknesses. This article proposes an approach that distinctively combines dynamic symbolic execution, search-based testing and machine learning, to efficiently generate thorough class-level test suites. The preliminary data obtained carrying out some experiments confirm that we are going in the right direction.},
  file        = {:http\://arxiv.org/pdf/2005.09317v1:PDF},
  keywords    = {cs.SE},
}

@Article{Kodialam2020,
  author      = {Arjun Kodialam},
  date        = {2020-08-24},
  title       = {Efficient Detection Of Infected Individuals using Two Stage Testing},
  eprint      = {2008.10741},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Group testing is an efficient method for testing a large population to detect infected individuals. In this paper, we consider an efficient adaptive two stage group testing scheme. Using a straightforward analysis, we characterize the efficiency of several two stage group testing algorithms. We determine how to pick the parameters of the tests optimally for three schemes with different types of randomization, and show that the performance of two stage testing depends on the type of randomization employed. Seemingly similar randomization procedures lead to different expected number of tests to detect all infected individuals, we determine what kinds of randomization are necessary to achieve optimal performance. We further show that in the optimal setting, our testing scheme is robust to errors in the input parameters.},
  file        = {:http\://arxiv.org/pdf/2008.10741v1:PDF},
  keywords    = {stat.ME, cs.LG},
}

@Article{Pierson2020,
  author      = {Emma Pierson},
  date        = {2020-11-02},
  title       = {Assessing racial inequality in COVID-19 testing with Bayesian threshold tests},
  eprint      = {2011.01179},
  eprintclass = {stat.AP},
  eprinttype  = {arXiv},
  abstract    = {There are racial disparities in the COVID-19 test positivity rate, suggesting that minorities may be under-tested. Here, drawing on the literature on statistically assessing racial disparities in policing, we 1) illuminate a statistical flaw, known as infra-marginality, in using the positivity rate as a metric for assessing racial disparities in under-testing; 2) develop a new type of Bayesian threshold test to measure disparities in COVID-19 testing and 3) apply the test to measure racial disparities in testing thresholds in a real-world COVID-19 dataset.},
  file        = {:http\://arxiv.org/pdf/2011.01179v1:PDF},
  keywords    = {stat.AP},
}

@Article{Schieferdecker2012,
  author       = {Ina Schieferdecker and Juergen Grossmann and Martin Schneider},
  date         = {2012-02-28},
  journaltitle = {EPTCS 80, 2012, pp. 1-12},
  title        = {Model-Based Security Testing},
  doi          = {10.4204/EPTCS.80.1},
  eprint       = {1202.6118},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Security testing aims at validating software system requirements related to security properties like confidentiality, integrity, authentication, authorization, availability, and non-repudiation. Although security testing techniques are available for many years, there has been little approaches that allow for specification of test cases at a higher level of abstraction, for enabling guidance on test identification and specification as well as for automated test generation. Model-based security testing (MBST) is a relatively new field and especially dedicated to the systematic and efficient specification and documentation of security test objectives, security test cases and test suites, as well as to their automated or semi-automated generation. In particular, the combination of security modelling and test generation approaches is still a challenge in research and of high interest for industrial applications. MBST includes e.g. security functional testing, model-based fuzzing, risk- and threat-oriented testing, and the usage of security test patterns. This paper provides a survey on MBST techniques and the related models as well as samples of new methods and tools that are under development in the European ITEA2-project DIAMONDS.},
  file         = {:http\://arxiv.org/pdf/1202.6118v1:PDF},
  keywords     = {cs.SE},
}

@Article{Singh2012,
  author      = {Amandeep Singh and Balwinder Singh},
  date        = {2012-05-09},
  title       = {Microcontroller Based Testing of Digital IP-Core},
  doi         = {10.5121/vlsic.2012.3205},
  eprint      = {1205.1866},
  eprintclass = {cs.AR},
  eprinttype  = {arXiv},
  abstract    = {Testing core based System on Chip is a challenge for the test engineers. To test the complete SOC at one time with maximum fault coverage, test engineers prefer to test each IP-core separately. At speed testing using external testers is more expensive because of gigahertz processor. The purpose of this paper is to develop cost efficient and flexible test methodology for testing digital IP-cores . The prominent feature of the approach is to use microcontroller to test IP-core. The novel feature is that there is no need of test pattern generator and output response analyzer as microcontroller performs the function of both. This approach has various advantages such as at speed testing, low cost, less area overhead and greater flexibility since most of the testing process is based on software.},
  file        = {:http\://arxiv.org/pdf/1205.1866v1:PDF},
  keywords    = {cs.AR},
}

@Article{Ceyhan2012,
  author      = {Elvan Ceyhan},
  date        = {2012-06-08},
  title       = {New Cell-Specific and Overall Tests of Spatial Interaction Based on Nearest Neighbor Contingency Tables},
  eprint      = {1206.1850},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Spatial interaction patterns such as segregation and association can be tested using nearest neighbor contingency tables (NNCTs). We introduce new cell-specific (or pairwise) and overall segregation tests and determine their asymptotic distributions. In particular, we demonstrate that cell-specific tests enjoy asymptotic normality, while overall tests have chi-square distributions asymptotically. We also perform an extensive Monte Carlo simulation study to compare the finite sample performance of the tests in terms of empirical size and power. In addition to the cell-specific tests as post-hoc tests for overall tests, we discuss one-class-versus-rest type of NNCT-tests after an overall test yields significant interaction. We also introduce the concepts of total, strong, and partial segregation/association to label levels of these patterns. We compare these new tests with the existing NNCT-tests in literature with simulations as well and illustrate the NNCT-tests on an ecological data set.},
  file        = {:http\://arxiv.org/pdf/1206.1850v2:PDF},
  keywords    = {stat.ME, stat.AP, 62H11, 62M30, 62G10, 62P12},
}

@Article{Schrammel2013,
  author      = {Peter Schrammel and Tom Melham and Daniel Kroening},
  date        = {2013-06-14},
  title       = {Chaining Test Cases for Reactive System Testing (extended version)},
  eprint      = {1306.3882},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Testing of synchronous reactive systems is challenging because long input sequences are often needed to drive them into a state at which a desired feature can be tested. This is particularly problematic in on-target testing, where a system is tested in its real-life application environment and the time required for resetting is high. This paper presents an approach to discovering a test case chain---a single software execution that covers a group of test goals and minimises overall test execution time. Our technique targets the scenario in which test goals for the requirements are given as safety properties. We give conditions for the existence and minimality of a single test case chain and minimise the number of test chains if a single test chain is infeasible. We report experimental results with a prototype tool for C code generated from Simulink models and compare it to state-of-the-art test suite generators.},
  file        = {:http\://arxiv.org/pdf/1306.3882v2:PDF},
  keywords    = {cs.SE, cs.SY, D.2.4; D.2.5; F.3.1; F.2.2; C.3},
}

@Article{F|ilipov2015,
  author      = {Stefan M. F|ilipov and Ivan D. Gospodinov},
  date        = {2015-11-30},
  title       = {Meeting an absolute test information target with optimal number of test items via Grand Canonical Monte Carlo simulation},
  eprint      = {1511.09216},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {This work studies IRT-based Automated Test Assembly (ATA) of multiple test forms (tests) that meet an absolute target information function, i.e. selecting from an item bank only the tests that have information functions that are at a small distance away from the target. The authors introduce the quantities multiplicity of tests and probability of selecting a test with particular number of items N and distance E from the target. A Grand Canonical Monte Carlo test-assembly algorithm is proposed that selects tests according to this probability. The algorithm allows N to vary during the simulation. This work demonstrates that the number of tests that meet the target depends strongly on N. The algorithm is capable of finding tests with small values of E and various values of N depending on the need of the test constructor. Most importantly, it can determine the optimal N for which a maximal number of tests with certain specified small E exists.},
  file        = {:http\://arxiv.org/pdf/1511.09216v1:PDF},
  keywords    = {math.NA},
}

@Article{Walkinshaw2016,
  author      = {Neil Walkinshaw and Gordon Fraser},
  date        = {2016-08-10},
  title       = {Uncertainty-Driven Black-Box Test Data Generation},
  eprint      = {1608.03181},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {We can never be certain that a software system is correct simply by testing it, but with every additional successful test we become less uncertain about its correctness. In absence of source code or elaborate specifications and models, tests are usually generated or chosen randomly. However, rather than randomly choosing tests, it would be preferable to choose those tests that decrease our uncertainty about correctness the most. In order to guide test generation, we apply what is referred to in Machine Learning as "Query Strategy Framework": We infer a behavioural model of the system under test and select those tests which the inferred model is "least certain" about. Running these tests on the system under test thus directly targets those parts about which tests so far have failed to inform the model. We provide an implementation that uses a genetic programming engine for model inference in order to enable an uncertainty sampling technique known as "query by committee", and evaluate it on eight subject systems from the Apache Commons Math framework and JodaTime. The results indicate that test generation using uncertainty sampling outperforms conventional and Adaptive Random Testing.},
  file        = {:http\://arxiv.org/pdf/1608.03181v1:PDF},
  keywords    = {cs.SE},
}

@Article{Niedermayr2016,
  author       = {Rainer Niedermayr and Elmar Juergens and Stefan Wagner},
  date         = {2016-11-22},
  journaltitle = {Proceedings of the International Workshop on Continuous Software Evolution and Delivery (CSED '16). ACM, 2016},
  title        = {Will My Tests Tell Me If I Break This Code?},
  doi          = {10.1145/2896941.2896944},
  eprint       = {1611.07163},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Automated tests play an important role in software evolution because they can rapidly detect faults introduced during changes. In practice, code-coverage metrics are often used as criteria to evaluate the effectiveness of test suites with focus on regression faults. However, code coverage only expresses which portion of a system has been executed by tests, but not how effective the tests actually are in detecting regression faults. Our goal was to evaluate the validity of code coverage as a measure for test effectiveness. To do so, we conducted an empirical study in which we applied an extreme mutation testing approach to analyze the tests of open-source projects written in Java. We assessed the ratio of pseudo-tested methods (those tested in a way such that faults would not be detected) to all covered methods and judged their impact on the software project. The results show that the ratio of pseudo-tested methods is acceptable for unit tests but not for system tests (that execute large portions of the whole system). Therefore, we conclude that the coverage metric is only a valid effectiveness indicator for unit tests.},
  file         = {:http\://arxiv.org/pdf/1611.07163v1:PDF},
  keywords     = {cs.SE},
}

@Article{Wang2014,
  author      = {Chao Wang and Qing Zhao and Chen-Nee Chuah},
  date        = {2014-07-08},
  title       = {Optimal Nested Test Plan for Combinatorial Quantitative Group Testing},
  eprint      = {1407.2283},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  abstract    = {We consider the quantitative group testing problem where the objective is to identify defective items in a given population based on results of tests performed on subsets of the population. Under the quantitative group testing model, the result of each test reveals the number of defective items in the tested group. The minimum number of tests achievable by nested test plans was established by Aigner and Schughart in 1985 within a minimax framework. The optimal nested test plan offering this performance, however, was not obtained. In this work, we establish the optimal nested test plan in closed form. This optimal nested test plan is also order optimal among all test plans as the population size approaches infinity. Using heavy-hitter detection as a case study, we show via simulation examples orders of magnitude improvement of the group testing approach over two prevailing sampling-based approaches in detection accuracy and counter consumption. Other applications include anomaly detection and wideband spectrum sensing in cognitive radio systems.},
  file        = {:http\://arxiv.org/pdf/1407.2283v5:PDF},
  keywords    = {cs.IT, math.IT, math.OC},
}

@Article{Guo2016,
  author      = {Jia Guo and Bu Zhou and Jin-Ting Zhang},
  date        = {2016-09-14},
  title       = {A Supremum-Norm Based Test for the Equality of Several Covariance Functions},
  eprint      = {1609.04232},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we propose a new test for the equality of several covariance functions for functional data. Its test statistic is taken as the supremum value of the sum of the squared differences between the estimated individual covariance functions and the pooled sample covariance function, hoping to obtain a more powerful test than some existing tests for the same testing problem. The asymptotic random expression of this test statistic under the null hypothesis is obtained. To approximate the null distribution of the proposed test statistic, we describe a parametric bootstrap method and a non-parametric bootstrap method. The asymptotic random expression of the proposed test is also studied under a local alternative and it is shown that the proposed test is root-$n$ consistent. Intensive simulation studies are conducted to demonstrate the finite sample performance of the proposed test and it turns out that the proposed test is indeed more powerful than some existing tests when functional data are highly correlated. The proposed test is illustrated with three real data examples.},
  file        = {:http\://arxiv.org/pdf/1609.04232v1:PDF},
  keywords    = {stat.ME, stat.AP, stat.CO},
}

@Article{Dwarakanath2018,
  author      = {Anurag Dwarakanath and Aruna Jankiti},
  date        = {2018-09-22},
  title       = {Minimum Number of Test Paths for Prime Path and other Structural Coverage Criteria},
  eprint      = {1809.08446},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {The software system under test can be modeled as a graph comprising of a set of vertices, (V) and a set of edges, (E). Test Cases are Test Paths over the graph meeting a particular test criterion. In this paper, we present a method to achieve the minimum number of Test Paths needed to cover different structural coverage criteria. Our method can accommodate Prime Path, Edge-Pair, Simple & Complete Round Trip, Edge and Node coverage criteria. Our method obtains the optimal solution by transforming the graph into a flow graph and solving the minimum flow problem. We present an algorithm for the minimum flow problem that matches the best known solution complexity of $O(|V| |E|)$. Our method is evaluated through two sets of tests. In the first, we test against graphs representing actual software. In the second test, we create random graphs of varying complexity. In each test we measure the number of Test Paths, the length of Test Paths, the lower bound on minimum number of Test Paths and the execution time.},
  file        = {:http\://arxiv.org/pdf/1809.08446v1:PDF},
  keywords    = {cs.SE},
}

@Article{Roach2018,
  author      = {Jeffrey Roach and William Valdar},
  date        = {2018-08-30},
  title       = {Permutation tests of non-exchangeable null models},
  eprint      = {1808.10483},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Generalizations to the permutation test are introduced to allow for situations in which the null model is not exchangeable. It is shown that the generalized permutation tests are exact, and a partial converse: that any test function that is exact on all probability densities coincides with a generalized permutation test on a particular region, is established. A most powerful generalized permutation test is derived in closed form. Approximations to the most powerful generalized permutation test are proposed to reduce the computational burden required to compute the complete test. In particular, an explicit form for the approximate test is derived in terms of a multinomial Bernstein polynomial approximation, and its convergence to the most powerful generalized permutation test is demonstrated. In the case where the determination of p-values is of greater interest than testing of hypotheses, two approaches to estimation of significance are analyzed. Bounds on the deviation from significance of the exact most powerful test are given in terms of sample size. For both estimators, as sample size approaches infinity, the estimator converges to the significance of the most powerful generalized permutation test under mild conditions. Applications of generalized permutation testing to linear mixed models are provided.},
  file        = {:http\://arxiv.org/pdf/1808.10483v1:PDF},
  keywords    = {stat.ME},
}

@Article{Spieker2019,
  author       = {Helge Spieker and Arnaud Gotlieb},
  date         = {2019-10-01},
  journaltitle = {Journal of Systems and Software (JSS) Vol. 165 (2020) 110574},
  title        = {Adaptive Metamorphic Testing with Contextual Bandits},
  doi          = {10.1016/j.jss.2020.110574},
  eprint       = {1910.00262},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Metamorphic Testing is a software testing paradigm which aims at using necessary properties of a system-under-test, called metamorphic relations, to either check its expected outputs, or to generate new test cases. Metamorphic Testing has been successful to test programs for which a full oracle is not available or to test programs for which there are uncertainties on expected outputs such as learning systems. In this article, we propose Adaptive Metamorphic Testing as a generalization of a simple yet powerful reinforcement learning technique, namely contextual bandits, to select one of the multiple metamorphic relations available for a program. By using contextual bandits, Adaptive Metamorphic Testing learns which metamorphic relations are likely to transform a source test case, such that it has higher chance to discover faults. We present experimental results over two major case studies in machine learning, namely image classification and object detection, and identify weaknesses and robustness boundaries. Adaptive Metamorphic Testing efficiently identifies weaknesses of the tested systems in context of the source test case.},
  file         = {:http\://arxiv.org/pdf/1910.00262v3:PDF},
  keywords     = {cs.SE},
}

@Article{Felderer2019,
  author      = {Michael Felderer and Ina Schieferdecker},
  date        = {2019-12-24},
  title       = {A taxonomy of risk-based testing},
  eprint      = {1912.11519},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Software testing has often to be done under severe pressure due to limited resources and a challenging time schedule facing the demand to assure the fulfillment of the software requirements. In addition, testing should unveil those software defects that harm the mission-critical functions of the software. Risk-based testing uses risk (re-)assessments to steer all phases of the test process in order to optimize testing efforts and limit risks of the software-based system. Due to its importance and high practical relevance several risk-based testing approaches were proposed in academia and industry. This paper presents a taxonomy of risk-based testing providing a framework to understand, categorize, assess, and compare risk-based testing approaches to support their selection and tailoring for specific purposes. The taxonomy is aligned with the consideration of risks in all phases of the test process and consists of the top-level classes risk drivers, risk assessment, and risk-based test process. The taxonomy of risk-based testing has been developed by analyzing the work presented in available publications on risk-based testing. Afterwards, it has been applied to the work on risk-based testing presented in this special section of the International Journal on Software Tools for Technology Transfer.},
  file        = {:http\://arxiv.org/pdf/1912.11519v1:PDF},
  keywords    = {cs.SE},
}

@Article{Ziegler2020,
  author      = {Gabriel Ziegler},
  date        = {2020-12-21},
  title       = {Binary Classification Tests, Imperfect Standards, and Ambiguous Information},
  eprint      = {2012.11215},
  eprintclass = {econ.EM},
  eprinttype  = {arXiv},
  abstract    = {New binary classification tests are often evaluated relative to a pre-established test. For example, rapid Antigen tests for the detection of SARS-CoV-2 are assessed relative to more established PCR tests. In this paper, I argue that the new test can be described as producing ambiguous information when the pre-established is imperfect. This allows for a phenomenon called dilation -- an extreme form of non-informativeness. As an example, I present hypothetical test data satisfying the WHO's minimum quality requirement for rapid Antigen tests which leads to dilation. The ambiguity in the information arises from a missing data problem due to imperfection of the established test: the joint distribution of true infection and test results is not observed. Using results from Copula theory, I construct the (usually non-singleton) set of all these possible joint distributions, which allows me to assess the new test's informativeness. This analysis leads to a simple sufficient condition to make sure that a new test is not a dilation. I illustrate my approach with applications to data from three COVID-19 related tests. Two rapid Antigen tests satisfy my sufficient condition easily and are therefore informative. However, less accurate procedures, like chest CT scans, may exhibit dilation.},
  file        = {:http\://arxiv.org/pdf/2012.11215v3:PDF},
  keywords    = {econ.EM},
}

@Article{Feldt2015,
  author      = {Robert Feldt and Simon Poulding and David Clark and Shin Yoo},
  date        = {2015-06-10},
  title       = {Test Set Diameter: Quantifying the Diversity of Sets of Test Cases},
  eprint      = {1506.03482},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {A common and natural intuition among software testers is that test cases need to differ if a software system is to be tested properly and its quality ensured. Consequently, much research has gone into formulating distance measures for how test cases, their inputs and/or their outputs differ. However, common to these proposals is that they are data type specific and/or calculate the diversity only between pairs of test inputs, traces or outputs. We propose a new metric to measure the diversity of sets of tests: the test set diameter (TSDm). It extends our earlier, pairwise test diversity metrics based on recent advances in information theory regarding the calculation of the normalized compression distance (NCD) for multisets. An advantage is that TSDm can be applied regardless of data type and on any test-related information, not only the test inputs. A downside is the increased computational time compared to competing approaches. Our experiments on four different systems show that the test set diameter can help select test sets with higher structural and fault coverage than random selection even when only applied to test inputs. This can enable early test design and selection, prior to even having a software system to test, and complement other types of test automation and analysis. We argue that this quantification of test set diversity creates a number of opportunities to better understand software quality and provides practical ways to increase it.},
  file        = {:http\://arxiv.org/pdf/1506.03482v1:PDF},
  keywords    = {cs.SE},
}

@Article{Gaboardi2016,
  author      = {Marco Gaboardi and Hyun woo Lim and Ryan Rogers and Salil Vadhan},
  date        = {2016-02-07},
  title       = {Differentially Private Chi-Squared Hypothesis Testing: Goodness of Fit and Independence Testing},
  eprint      = {1602.03090},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Hypothesis testing is a useful statistical tool in determining whether a given model should be rejected based on a sample from the population. Sample data may contain sensitive information about individuals, such as medical information. Thus it is important to design statistical tests that guarantee the privacy of subjects in the data. In this work, we study hypothesis testing subject to differential privacy, specifically chi-squared tests for goodness of fit for multinomial data and independence between two categorical variables. We propose new tests for goodness of fit and independence testing that like the classical versions can be used to determine whether a given model should be rejected or not, and that additionally can ensure differential privacy. We give both Monte Carlo based hypothesis tests as well as hypothesis tests that more closely follow the classical chi-squared goodness of fit test and the Pearson chi-squared test for independence. Crucially, our tests account for the distribution of the noise that is injected to ensure privacy in determining significance. We show that these tests can be used to achieve desired significance levels, in sharp contrast to direct applications of classical tests to differentially private contingency tables which can result in wildly varying significance levels. Moreover, we study the statistical power of these tests. We empirically show that to achieve the same level of power as the classical non-private tests our new tests need only a relatively modest increase in sample size.},
  file        = {:http\://arxiv.org/pdf/1602.03090v2:PDF},
  keywords    = {math.ST, cs.CR, stat.TH},
}

@Article{Gontscharuk2016,
  author       = {Veronika Gontscharuk and Sandra Landwehr and Helmut Finner},
  date         = {2016-03-17},
  journaltitle = {Bernoulli 2016, Vol. 22, No. 3, 1331-1363},
  title        = {Goodness of fit tests in terms of local levels with special emphasis on higher criticism tests},
  doi          = {10.3150/14-BEJ694},
  eprint       = {1603.05461},
  eprintclass  = {math.ST},
  eprinttype   = {arXiv},
  abstract     = {Instead of defining goodness of fit (GOF) tests in terms of their test statistics, we present an alternative method by introducing the concept of local levels, which indicate high or low local sensitivity of a test. Local levels can act as a starting point for the construction of new GOF tests. We study the behavior of local levels when applied to some well-known GOF tests such as Kolmogorov-Smirnov (KS) tests, higher criticism (HC) tests and tests based on phi-divergences. The main focus is on a rigorous characterization of the asymptotic behavior of local levels of the original HC tests which leads to several further asymptotic results for local levels of other GOF tests including GOF tests with equal local levels. While local levels of KS tests, which are related to the central range, are asymptotically strictly larger than zero, all local levels of HC tests converge to zero as the sample size increases. Consequently, there exists no asymptotic level $\alpha$ GOF test such that all local levels are asymptotically bounded away from zero. Finally, by means of numerical computations we compare classical KS and HC tests to a GOF test with equal local levels.},
  file         = {:http\://arxiv.org/pdf/1603.05461v1:PDF},
  keywords     = {math.ST, stat.TH},
}

@Article{Coppola2017,
  author      = {Riccardo Coppola and Maurizio Morisio and Marco Torchiano},
  date        = {2017-11-09},
  title       = {Scripted GUI Testing of Android Apps: A Study on Diffusion, Evolution and Fragility},
  eprint      = {1711.03565},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Background. Evidence suggests that mobile applications are not thoroughly tested as their desktop counterparts. In particular GUI testing is generally limited. Like web-based applications, mobile apps suffer from GUI test fragility, i.e. GUI test classes failing due to minor modifications in the GUI, without the application functionalities being altered. Aims. The objective of our study is to examine the diffusion of GUI testing on Android, and the amount of changes required to keep test classes up to date, and in particular the changes due to GUI test fragility. We define metrics to characterize the modifications and evolution of test classes and test methods, and proxies to estimate fragility-induced changes. Method. To perform our experiments, we selected six widely used open-source tools for scripted GUI testing of mobile applications previously described in the literature. We have mined the repositories on GitHub that used those tools, and computed our set of metrics. Results. We found that none of the considered GUI testing frameworks achieved a major diffusion among the open-source Android projects available on GitHub. For projects with GUI tests, we found that test suites have to be modified often, specifically 5\%-10\% of developers' modified LOCs belong to tests, and that a relevant portion (60\% on average) of such modifications are induced by fragility. Conclusions. Fragility of GUI test classes constitute a relevant concern, possibly being an obstacle for developers to adopt automated scripted GUI tests. This first evaluation and measure of fragility of Android scripted GUI testing can constitute a benchmark for developers, and the basis for the definition of a taxonomy of fragility causes, and actionable guidelines to mitigate the issue.},
  file        = {:http\://arxiv.org/pdf/1711.03565v1:PDF},
  keywords    = {cs.SE},
}

@Article{Chen2020a,
  author      = {T. Y. Chen and S. C. Cheung and S. M. Yiu},
  date        = {2020-02-28},
  title       = {Metamorphic Testing: A New Approach for Generating Next Test Cases},
  eprint      = {2002.12543},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {In software testing, a set of test cases is constructed according to some predefined selection criteria. The software is then examined against these test cases. Three interesting observations have been made on the current artifacts of software testing. Firstly, an error-revealing test case is considered useful while a successful test case which does not reveal software errors is usually not further investigated. Whether these successful test cases still contain useful information for revealing software errors has not been properly studied. Secondly, no matter how extensive the testing has been conducted in the development phase, errors may still exist in the software [5]. These errors, if left undetected, may eventually cause damage to the production system. The study of techniques for uncovering software errors in the production phase is seldom addressed in the literature. Thirdly, as indicated by Weyuker in [6], the availability of test oracles is pragmatically unattainable in most situations. However, the availability of test oracles is generally assumed in conventional software testing techniques. In this paper, we propose a novel test case selection technique that derives new test cases from the successful ones. The selection aims at revealing software errors that are possibly left undetected in successful test cases which may be generated using some existing strategies. As such, the proposed technique augments the effectiveness of existing test selection strategies. The technique also helps uncover software errors in the production phase and can be used in the absence of test oracles.},
  file        = {:http\://arxiv.org/pdf/2002.12543v1:PDF},
  keywords    = {cs.SE},
}

@Article{OliveiraNeto2020,
  author      = {Francisco Gomes de Oliveira Neto and Felix Dobslaw and Robert Feldt},
  date        = {2020-10-19},
  title       = {Using mutation testing to measure behavioural test diversity},
  doi         = {10.1109/ICSTW50294.2020.00051},
  eprint      = {2010.09144},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Diversity has been proposed as a key criterion to improve testing effectiveness and efficiency.It can be used to optimise large test repositories but also to visualise test maintenance issues and raise practitioners' awareness about waste in test artefacts and processes. Even though these diversity-based testing techniques aim to exercise diverse behavior in the system under test (SUT), the diversity has mainly been measured on and between artefacts (e.g., inputs, outputs or test scripts). Here, we introduce a family of measures to capture behavioural diversity (b-div) of test cases by comparing their executions and failure outcomes. Using failure information to capture the SUT behaviour has been shown to improve effectiveness of history-based test prioritisation approaches. However, history-based techniques require reliable test execution logs which are often not available or can be difficult to obtain due to flaky tests, scarcity of test executions, etc. To be generally applicable we instead propose to use mutation testing to measure behavioral diversity by running the set of test cases on various mutated versions of the SUT. Concretely, we propose two specific b-div measures (based on accuracy and Matthew's correlation coefficient, respectively) and compare them with artefact-based diversity (a-div) for prioritising the test suites of 6 different open-source projects. Our results show that our b-div measures outperform a-div and random selection in all of the studied projects. The improvement is substantial with an average increase in average percentage of faults detected (APFD) of between 19% to 31% depending on the size of the subset of prioritised tests.},
  file        = {:http\://arxiv.org/pdf/2010.09144v1:PDF},
  keywords    = {cs.SE},
}

@Article{Martin2020,
  author      = {Nirian Martn},
  date        = {2020-12-28},
  title       = {Rao's Score Tests on Correlation Matrices},
  eprint      = {2012.14238},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Even though the Rao's score tests are classical tests, such as the likelihood ratio tests, their application has been avoided until now in a multivariate framework, in particular high-dimensional setting. We consider they could play an important role for testing high-dimensional data, but currently the classical Rao's score tests for an arbitrary but fixed dimension remain being still not very well-known for tests on correlation matrices of multivariate normal distributions. In this paper, we illustrate how to create Rao's score tests, focussed on testing correlation matrices, showing their asymptotic distribution. Based on Basu et al. (2021), we do not only develop the classical Rao's score tests, but also their robust version, Rao's $\beta$-score tests. Despite of tedious calculations, their strength is the final simple expression, which is valid for any arbitrary but fixed dimension. In addition, we provide basic formulas for creating easily other tests, either for other variants of correlation tests or for location or variability parameters. We perform a simulation study with high-dimensional data and the results are compared to those of the likelihood ratio test with a variety of distributions, either pure and contaminated. The study shows that the classical Rao's score test for correlation matrices seems to work properly not only under multivariate normality but also under other multivariate distributions. Under perturbed distributions, the Rao's $\beta$-score tests outperform any classical test.},
  file        = {:http\://arxiv.org/pdf/2012.14238v2:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Madeja2021,
  author      = {Matej Madeja and Jaroslav Porubn and Michaela Bakov and Mat Sulr and Jn Juhr and Sergej Chodarev and Filip Gurb},
  date        = {2021-02-23},
  title       = {Automating Test Case Identification in Open Source Projects on GitHub},
  eprint      = {2102.11678},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Software testing is one of the very important Quality Assurance (QA) components. A lot of researchers deal with the testing process in terms of tester motivation and how tests should or should not be written. However, it is not known from the recommendations how the tests are actually written in real projects. In this paper the following was investigated: (i) the denotation of the test word in different natural languages; (ii) whether the test word correlates with the presence of test cases; and (iii) what testing frameworks are mostly used. The analysis was performed on 38 GitHub open source repositories thoroughly selected from the set of 4.3M GitHub projects. We analyzed 20,340 test cases in 803 classes manually and 170k classes using an automated approach. The results show that: (i) there exists weak correlation (r = 0.655) between the word test and test cases presence in a class; (ii) the proposed algorithm using static file analysis correctly detected 95\% of test cases; (iii) 15\% of the analyzed classes used main() function whose represent regular Java programs that test the production code without using any third-party framework. The identification of such tests is very low due to implementation diversity. The results may be leveraged to more quickly identify and locate test cases in a repository, to understand practices in customized testing solutions and to mine tests to improve program comprehension in the future.},
  file        = {:http\://arxiv.org/pdf/2102.11678v1:PDF},
  keywords    = {cs.SE, 68-04, D.2.5; D.2.3},
}

@Article{Langovoy2007,
  author       = {Mikhail Langovoy},
  date         = {2007-07-05},
  journaltitle = {Inverse Problems 24 (2008) 025028 17pp},
  title        = {Data-driven efficient score tests for deconvolution problems},
  doi          = {10.1088/0266-5611/24/2/025028},
  eprint       = {0707.0861},
  eprintclass  = {math.ST},
  eprinttype   = {arXiv},
  abstract     = {We consider testing statistical hypotheses about densities of signals in deconvolution models. A new approach to this problem is proposed. We constructed score tests for the deconvolution with the known noise density and efficient score tests for the case of unknown density. The tests are incorporated with model selection rules to choose reasonable model dimensions automatically by the data. Consistency of the tests is proved.},
  file         = {:http\://arxiv.org/pdf/0707.0861v1:PDF},
  keywords     = {math.ST, stat.AP, stat.TH},
}

@Article{Kadry2011,
  author       = {Seifedine Kadry},
  date         = {2011-11-23},
  journaltitle = {International Journal of Security and Its Applications Vol. 5 No. 3, July, 2011},
  title        = {A New Proposed Technique to Improve Software Regression Testing Cost},
  eprint       = {1111.5640},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {In this article, we describe the regression test process to test and verify the changes made on software. A developed technique use the automation test based on decision tree and test selection process in order to reduce the testing cost is given. The developed technique is applied to a practical case and the result show its improvement.},
  file         = {:http\://arxiv.org/pdf/1111.5640v1:PDF},
  keywords     = {cs.SE},
}

@Article{Koldanov2016,
  author      = {Petr A. Koldanov and Alexander P. Koldanov and Panos Pardalos},
  date        = {2016-04-23},
  title       = {Multiple testing with optimal individual tests in Gaussian graphical model selection},
  eprint      = {1604.06874},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Gaussian Graphical Model selection problem is considered. Concentration graph is identified by multiple decision procedure based on individual tests. Optimal unbiased individual tests are constructed. It is shown that optimal tests are equivalent to sample partial correlation tests. Associated multiple decision procedure is compared with standard procedure.},
  file        = {:http\://arxiv.org/pdf/1604.06874v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Neumann2019,
  author      = {Andr Neumann and Thorsten Dickhaus},
  date        = {2019-03-27},
  title       = {Non-parametric Archimedean generator estimation with implications for multiple testing},
  eprint      = {1903.11371},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {In multiple testing, the family-wise error rate can be bounded under some conditions by the copula of the test statistics. Assuming that this copula is Archimedean, we consider two non-parametric Archimedean generator estimators. More specifically, we use the non-parametric estimator from Genest et al. (2011) and a slight modification thereof. In simulations, we compare the resulting multiple tests with the Bonferroni test and the multiple test derived from the true generator as baselines.},
  file        = {:http\://arxiv.org/pdf/1903.11371v1:PDF},
  keywords    = {stat.ME, 62J15},
}

@Article{Pav2019,
  author      = {Steven Pav},
  date        = {2019-11-11},
  title       = {A post hoc test on the Sharpe ratio},
  eprint      = {1911.04090},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {We describe a post hoc test for the Sharpe ratio, analogous to Tukey's test for pairwise equality of means. The test can be applied after rejection of the hypothesis that all population Signal-Noise ratios are equal. The test is applicable under a simple correlation structure among asset returns. Simulations indicate the test maintains nominal type I rate under a wide range of conditions and is moderately powerful under reasonable alternatives.},
  file        = {:http\://arxiv.org/pdf/1911.04090v1:PDF},
  keywords    = {stat.ME, q-fin.PM, 91G70, G.3},
}

@Article{Haq2019,
  author      = {Fitash Ul Haq and Donghwan Shin and Shiva Nejati and Lionel Briand},
  date        = {2019-11-28},
  title       = {Comparing Offline and Online Testing of Deep Neural Networks: An Autonomous Car Case Study},
  eprint      = {1912.00805},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {There is a growing body of research on developing testing techniques for Deep Neural Networks (DNN). We distinguish two general modes of testing for DNNs: Offline testing where DNNs are tested as individual units based on test datasets obtained independently from the DNNs under test, and online testing where DNNs are embedded into a specific application and tested in a close-loop mode in interaction with the application environment. In addition, we identify two sources for generating test datasets for DNNs: Datasets obtained from real-life and datasets generated by simulators. While offline testing can be used with datasets obtained from either sources, online testing is largely confined to using simulators since online testing within real-life applications can be time-consuming, expensive and dangerous. In this paper, we study the following two important questions aiming to compare test datasets and testing modes for DNNs: First, can we use simulator-generated data as a reliable substitute to real-world data for the purpose of DNN testing? Second, how do online and offline testing results differ and complement each other? Though these questions are generally relevant to all autonomous systems, we study them in the context of automated driving systems where, as study subjects, we use DNNs automating end-to-end control of cars' steering actuators. Our results show that simulator-generated datasets are able to yield DNN prediction errors that are similar to those obtained by testing DNNs with real-life datasets. Further, offline testing is more optimistic than online testing as many safety violations identified by online testing could not be identified by offline testing, while large prediction errors generated by offline testing always led to severe safety violations detectable by online testing.},
  file        = {:http\://arxiv.org/pdf/1912.00805v1:PDF},
  keywords    = {cs.LG, cs.SE},
}

@Article{Strandberg2020,
  author      = {Per Erik Strandberg and Thomas J Ostrand and Elaine J Weyuker and Wasif Afzal and Daniel Sundmark},
  date        = {2020-05-14},
  title       = {Intermittently Failing Tests in the Embedded Systems Domain},
  eprint      = {2005.06826},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Software testing is sometimes plagued with intermittently failing tests and finding the root causes of such failing tests is often difficult. This problem has been widely studied at the unit testing level for open source software, but there has been far less investigation at the system test level, particularly the testing of industrial embedded systems. This paper describes our investigation of the root causes of intermittently failing tests in the embedded systems domain, with the goal of better understanding, explaining and categorizing the underlying faults. The subject of our investigation is a currently-running industrial embedded system, along with the system level testing that was performed. We devised and used a novel metric for classifying test cases as intermittent. From more than a half million test verdicts, we identified intermittently and consistently failing tests, and identified their root causes using multiple sources. We found that about 1-3% of all test cases were intermittently failing. From analysis of the case study results and related work, we identified nine factors associated with test case intermittence. We found that a fix for a consistently failing test typically removed a larger number of failures detected by other tests than a fix for an intermittent test. We also found that more effort was usually needed to identify fixes for intermittent tests than for consistent tests. An overlap between root causes leading to intermittent and consistent tests was identified. Many root causes of intermittence are the same in industrial embedded systems and open source software. However, when comparing unit testing to system level testing, especially for embedded systems, we observed that the test environment itself is often the cause of intermittence.},
  file        = {:http\://arxiv.org/pdf/2005.06826v1:PDF},
  keywords    = {cs.SE},
}

@Article{Qayed2020,
  author      = {Abdullah Qayed and Dong Han},
  date        = {2020-08-21},
  title       = {Homogeneity Test of Several High-Dimensional Covariance Matrices for Stationary Processes under Non-normality},
  eprint      = {2008.09259},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {This article presents a homogeneity test for testing the equality of several high-dimensional covariance matrices for stationary processes with ignoring the assumption of normality. We give the asymptotic distribution of the proposed test. The simulation illustrates that the proposed test has perfect performance. Moreover, the power of the test can approach any high probability uniformly on a set of covariance matrices.},
  file        = {:http\://arxiv.org/pdf/2008.09259v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Xuan2015,
  author      = {Jifeng Xuan and Benoit Cornu and Matias Martinez and Benoit Baudry and Lionel Seinturier and Martin Monperrus},
  date        = {2015-06-05},
  title       = {Dynamic Analysis can be Improved with Automatic Test Suite Refactoring},
  eprint      = {1506.01883},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Context: Developers design test suites to automatically verify that software meets its expected behaviors. Many dynamic analysis techniques are performed on the exploitation of execution traces from test cases. However, in practice, there is only one trace that results from the execution of one manually-written test case. Objective: In this paper, we propose a new technique of test suite refactoring, called B-Refactoring. The idea behind B-Refactoring is to split a test case into small test fragments, which cover a simpler part of the control flow to provide better support for dynamic analysis. Method: For a given dynamic analysis technique, our test suite refactoring approach monitors the execution of test cases and identifies small test cases without loss of the test ability. We apply B-Refactoring to assist two existing analysis tasks: automatic repair of if-statements bugs and automatic analysis of exception contracts. Results: Experimental results show that test suite refactoring can effectively simplify the execution traces of the test suite. Three real-world bugs that could previously not be fixed with the original test suite are fixed after applying B-Refactoring; meanwhile, exception contracts are better verified via applying B-Refactoring to original test suites. Conclusions: We conclude that applying B-Refactoring can effectively improve the purity of test cases. Existing dynamic analysis tasks can be enhanced by test suite refactoring.},
  file        = {:http\://arxiv.org/pdf/1506.01883v1:PDF},
  keywords    = {cs.SE},
}

@Article{Srinivasan2018,
  author      = {Madhusudan Srinivasan and Morteza Pourreza Shahri and Upulee Kanewala and Indika Kahanda},
  date        = {2018-02-20},
  title       = {Quality Assurance of Bioinformatics Software: A Case Study of Testing a Biomedical Text Processing Tool Using Metamorphic Testing},
  eprint      = {1802.07354},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Bioinformatics software plays a very important role in making critical decisions within many areas including medicine and health care. However, most of the research is directed towards developing tools, and little time and effort is spent on testing the software to assure its quality. In testing, a test oracle is used to determine whether a test is passed or failed during testing, and unfortunately, for much of bioinformatics software, the exact expected outcomes are not well defined. Thus, the main challenge associated with conducting systematic testing on bioinformatics software is the oracle problem. Metamorphic testing (MT) is a technique used to test programs that face the oracle problem. MT uses metamorphic relations (MRs) to determine whether a test has passed or failed and specifies how the output should change according to a specific change made to the input. In this work, we use MT to test LingPipe, a tool for processing text using computational linguistics, often used in bioinformatics for bio-entity recognition from biomedical literature. First, we identify a set of MRs for testing any bio-entity recognition program. Then we develop a set of test cases that can be used to test LingPipe's bio-entity recognition functionality using these MRs. To evaluate the effectiveness of this testing process, we automatically generate a set of faulty versions of LingPipe. According to our analysis of the experimental results, we observe that our MRs can detect the majority of these faulty versions, which shows the utility of this testing technique for quality assurance of bioinformatics software.},
  file        = {:http\://arxiv.org/pdf/1802.07354v1:PDF},
  keywords    = {cs.SE},
}

@Article{Ahmad2019,
  author      = {Azeem Ahmad and Ola Leifler and Kristian Sandahl},
  date        = {2019-06-03},
  title       = {Empirical Analysis of Factors and their Effect on Test Flakiness - Practitioners' Perceptions},
  eprint      = {1906.00673},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Developers always wish to ensure that their latest changes to the code base do not break existing functionality. If test cases fail, they expect these failures to be connected to the submitted changes. Unfortunately, a flaky test can be the reason for a test failure. Developers spend time to relate possible test failures to the submitted changes only to find out that the cause for these failures is test flakiness. The dilemma of an identification of the real failures or flaky test failures affects developers' perceptions about what is test flakiness. Prior research on test flakiness has been limited to test smells and tools to detect test flakiness. In this paper, we have conducted a multiple case study with four different industries in Scandinavia to understand practitioners' perceptions about test flakiness and how this varies between industries. We observed that there are little differences in how the practitioners perceive test flakiness. We identified 23 factors that are perceived to affect test flakiness. These perceived factors are categorized as 1) Software test quality, 2) Software Quality, 3) Actual Flaky test and 4) Company-specific factors. We have studied the nature of effects such as whether factors increase, decrease or affect the ability to detect test flakiness. We validated our findings with different participants of the 4 companies to avoid biases. The average agreement rate of the identified factors and their effects are 86% and 86% respectively, among participants.},
  file        = {:http\://arxiv.org/pdf/1906.00673v1:PDF},
  keywords    = {cs.SE},
}

@Article{Gonzalez2020,
  author      = {Danielle Gonzalez and Michael Rath and Mehdi Mirakhorli},
  date        = {2020-06-25},
  title       = {Did You Remember to Test Your Tokens?},
  doi         = {10.1145/3379597.3387471},
  eprint      = {2006.14553},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Authentication is a critical security feature for confirming the identity of a system's users, typically implemented with help from frameworks like Spring Security. It is a complex feature which should be robustly tested at all stages of development. Unit testing is an effective technique for fine-grained verification of feature behaviors that is not widely-used to test authentication. Part of the problem is that resources to help developers unit test security features are limited. Most security testing guides recommend test cases in a "black box" or penetration testing perspective. These resources are not easily applicable to developers writing new unit tests, or who want a security-focused perspective on coverage. In this paper, we address these issues by applying a grounded theory-based approach to identify common (unit) test cases for token authentication through analysis of 481 JUnit tests exercising Spring Security-based authentication implementations from 53 open source Java projects. The outcome of this study is a developer-friendly unit testing guide organized as a catalog of 53 test cases for token authentication, representing unique combinations of 17 scenarios, 40 conditions, and 30 expected outcomes learned from the data set in our analysis. We supplement the test guide with common test smells to avoid. To verify the accuracy and usefulness of our testing guide, we sought feedback from selected developers, some of whom authored unit tests in our dataset.},
  file        = {:http\://arxiv.org/pdf/2006.14553v1:PDF},
  keywords    = {cs.SE},
}

@Article{Jaszkiewicz2020,
  author      = {Andrzej Jaszkiewicz},
  date        = {2020-11-30},
  title       = {Modified Dorfman procedure for pool tests with dilution -- COVID-19 case study},
  eprint      = {2012.00673},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {The outbreak of the global COVID-19 pandemic results in unprecedented demand for fast and efficient testing of large numbers of patients for the presence of SARS-CoV-2 coronavirus. Beside technical improvements of the cost and speed of individual tests, pool testing may be used to improve efficiency and throughput of a population test. Dorfman pool testing procedure is one of the best known and studied methods of this kind. This procedure is, however, based on unrealistic assumptions that the pool test has perfect sensitivity and the only objective is to minimize the number of tests, and is not well adapted to the case of imperfect pool tests. We propose and analyze a simple modification of this procedure in which test of a pool with negative result is independently repeated up to several times. The proposed procedure is evaluated in a computational study using recent data about dilution effect for SARS-CoV-2 PCR tests, showing that the proposed approach significantly reduces the number of false negatives with a relatively small increase of the number of tests, especially for small prevalence rates. For example, for prevalence rate 0.001 the number of tests could be reduced to 22.1% of individual tests, increasing the expected number of false negatives by no more than 1%, and to 16.8% of individual tests increasing the expected number of false negatives by no more than 10%. At the same time, a similar reduction of the expected number of tests in the standard Dorfman procedure would yield 675% and 821% increase of the expected number of false negatives, respectively. This makes the proposed procedure an interesting choice for screening tests in the case of diseases like COVID-19.},
  file        = {:http\://arxiv.org/pdf/2012.00673v2:PDF},
  keywords    = {stat.ME, stat.AP},
}