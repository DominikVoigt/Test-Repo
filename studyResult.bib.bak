% Encoding: UTF-8

@Article{Saha2018,
  author      = {Prashanta Saha and Upulee Kanewala},
  date        = {2018-02-20},
  title       = {Fault Detection Effectiveness of Source Test Case Generation Strategies for Metamorphic Testing},
  eprint      = {1802.07361},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Metamorphic testing is a well known approach to tackle the oracle problem in software testing. This technique requires the use of source test cases that serve as seeds for the generation of follow-up test cases. Systematic design of test cases is crucial for the test quality. Thus, source test case generation strategy can make a big impact on the fault detection effectiveness of metamorphic testing. Most of the previous studies on metamorphic testing have used either random test data or existing test cases as source test cases. There has been limited research done on systematic source test case generation for metamorphic testing. This paper provides a comprehensive evaluation on the impact of source test case generation techniques on the fault finding effectiveness of metamorphic testing. We evaluated the effectiveness of line coverage, branch coverage, weak mutation and random test generation strategies for source test case generation. The experiments are conducted with 77 methods from 4 open source code repositories. Our results show that by systematically creating source test cases, we can significantly increase the fault finding effectiveness of metamorphic testing. Further, in this paper we introduce a simple metamorphic testing tool called "METtester" that we use to conduct metamorphic testing on these methods.},
  file        = {:http\://arxiv.org/pdf/1802.07361v1:PDF},
  keywords    = {cs.SE},
}

@Article{Zhu2019,
  author      = {Hong Zhu and Ian Bayley and Dongmei Liu and Xiaoyu Zheng},
  date        = {2019-12-20},
  title       = {Morphy: A Datamorphic Software Test Automation Tool},
  eprint      = {1912.09881},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {This paper presents an automated tool called Morphy for datamorphic testing. It classifies software test artefacts into test entities and test morphisms, which are mappings on testing entities. In addition to datamorphisms, metamorphisms and seed test case makers, Morphy also employs a set of other test morphisms including test case metrics and filters, test set metrics and filters, test result analysers and test executers to realise test automation. In particular, basic testing activities can be automated by invoking test morphisms. Test strategies can be realised as complex combinations of test morphisms. Test processes can be automated by recording, editing and playing test scripts that invoke test morphisms and strategies. Three types of test strategies have been implemented in Morphy: datamorphism combination strategies, cluster border exploration strategies and strategies for test set optimisation via genetic algorithms. This paper focuses on the datamorphism combination strategies by giving their definitions and implementation algorithms. The paper also illustrates their uses for testing both traditional software and AI applications with three case studies.},
  file        = {:http\://arxiv.org/pdf/1912.09881v1:PDF},
  keywords    = {cs.SE, cs.AI},
}

@Article{Sanchez2016,
  author      = {Jimi Sanchez},
  date        = {2016-06-01},
  title       = {A Review of Pair-wise Testing},
  eprint      = {1606.00288},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {In software testing, the large size of the input domain makes exhaustively testing the inputs a daunting and often impossible task. Pair-wise testing is a popular approach to combinatorial testing problems. This paper reviews Pair-wise testing and its history, strengths, weaknesses, and tools for generating test cases.},
  file        = {:http\://arxiv.org/pdf/1606.00288v1:PDF},
  keywords    = {cs.SE},
}

@Article{Wu2007,
  author       = {Cheng-Wen Wu},
  date         = {2007-10-25},
  journaltitle = {Dans Design, Automation and Test in Europe - DATE'05, Munich : Allemagne (2005)},
  title        = {SOC Testing Methodology and Practice},
  eprint       = {0710.4669},
  eprintclass  = {cs.AR},
  eprinttype   = {arXiv},
  abstract     = {On a commercial digital still camera (DSC) controller chip we practice a novel SOC test integration platform, solving real problems in test scheduling, test IO reduction, timing of functional test, scan IO sharing, embedded memory built-in self-test (BIST), etc. The chip has been fabricated and tested successfully by our approach. Test results justify that short test integration cost, short test time, and small area overhead can be achieved. To support SOC testing, a memory BIST compiler and an SOC testing integration system have been developed.},
  file         = {:http\://arxiv.org/pdf/0710.4669v1:PDF},
  keywords     = {cs.AR},
}

@Article{Kawai2020,
  author      = {Tetsuya Kawai and Masayuki Uchida},
  date        = {2020-10-26},
  title       = {Adaptive testing method for ergodic diffusion processes based on high frequency data},
  eprint      = {2010.13410},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {We consider parametric tests for multidimensional ergodic diffusions based on high frequency data. We propose two-step testing method for diffusion parameters and drift parameters. To construct test statistics of the tests, we utilize the adaptive estimator and provide three types of test statistics: likelihood ratio type test, Wald type test and Rao's score type test. It is proved that these test statistics converge in distribution to the chi-squared distribution under null hypothesis and have consistency of the tests against alternatives. Moreover, these test statistics converge in distribution to the non-central chi-squared distribution under local alternatives. We also give some simulation studies of the behavior of the three types of test statistics.},
  file        = {:http\://arxiv.org/pdf/2010.13410v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Aslan2002,
  author      = {B. Aslan and G. Zech},
  date        = {2002-07-31},
  title       = {Comparison of different goodness-of-fit tests},
  eprint      = {math/0207300},
  eprintclass = {math.PR},
  eprinttype  = {arXiv},
  abstract    = {Various distribution free goodness-of-fit test procedures have been extracted from literature. We present two new binning free tests, the univariate three-region-test and the multivariate energy test. The power of the selected tests with respect to different slowly varying distortions of experimental distributions are investigated. None of the tests is optimum for all distortions. The energy test has high power in many applications and is superior to the chi^2 test.},
  file        = {:http\://arxiv.org/pdf/math/0207300v1:PDF},
  keywords    = {math.PR},
}

@Article{Mrkvicka2015,
  author      = {TomÃ¡Å¡ MrkviÄ�ka and Mari MyllymÃ¤ki and Ute Hahn},
  date        = {2015-06-04},
  title       = {Multiple Monte Carlo Testing with Applications in Spatial Point Processes},
  eprint      = {1506.01646},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {The rank envelope test (Myllym\"aki et al., Global envelope tests for spatial processes, arXiv:1307.0239 [stat.ME]) is proposed as a solution to multiple testing problem for Monte Carlo tests. Three different situations are recognized: 1) a few univariate Monte Carlo tests, 2) a Monte Carlo test with a function as the test statistic, 3) several Monte Carlo tests with functions as test statistics. The rank test has correct (global) type I error in each case and it is accompanied with a $p$-value and with a graphical interpretation which shows which subtest or which distances of the used test function(s) lead to the rejection at the prescribed significance level of the test. Examples of null hypothesis from point process and random set statistics are used to demonstrate the strength of the rank envelope test. The examples include goodness-of-fit test with several test functions, goodness-of-fit test for one group of point patterns, comparison of several groups of point patterns, test of dependence of components in a multi-type point pattern, and test of Boolean assumption for random closed sets.},
  file        = {:http\://arxiv.org/pdf/1506.01646v1:PDF},
  keywords    = {stat.ME},
}

@Article{Morciniec2016,
  author      = {Tobias Morciniec and Andreas Podelski},
  date        = {2016-12-13},
  title       = {A Logical Approach to Generating Test Plans},
  eprint      = {1612.04351},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {During the execution of a test plan, a test manager may decide to drop a test case if its result can be inferred from already executed test cases. We show that it is possible to automatically generate a test plan to exploit the potential to justifiably drop a test case and thus reduce the number of test cases. Our approach uses Boolean formulas to model the mutual dependencies between test results. The algorithm to generate a test plan comes with the formal guarantee of optimality with regards to the inference of the result of a test case from already executed test cases.},
  file        = {:http\://arxiv.org/pdf/1612.04351v1:PDF},
  keywords    = {cs.SE},
}

@Article{Iwasaki2019,
  author      = {Atsushi Iwasaki},
  date        = {2019-08-20},
  title       = {Independent Randomness Tests based on the Orthogonalized Non-overlapping Template Matching Test},
  eprint      = {1908.07145},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {In general, randomness tests included in a test suite are not independent of each other. This renders it difficult to fix a rational criterion through the whole test suite with an explicit significance level. In this paper, we focus on the Non-overlapping Template Matching Test, which is a randomness test included in the NIST statistical test suite. The test uses a parameter called "template" and we can consider a test item for each template. We investigate dependency between two test items by deriving the joint probability density function of the two p-values and propose a transformation to make multi test items independent of each other.},
  file        = {:http\://arxiv.org/pdf/1908.07145v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Ceyhan2008,
  author      = {Elvan Ceyhan},
  date        = {2008-07-26},
  title       = {On the Use of Nearest Neighbor Contingency Tables for Testing Spatial Segregation},
  eprint      = {0807.4236},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {For two or more classes (or types) of points, nearest neighbor contingency tables (NNCTs) are constructed using nearest neighbor (NN) frequencies and are used in testing spatial segregation of the classes. Pielou's test of independence, Dixon's cell-specific, class-specific, and overall tests are the tests based on NNCTs (i.e., they are NNCT-tests). These tests are designed and intended for use under the null pattern of random labeling (RL) of completely mapped data. However, it has been shown that Pielou's test is not appropriate for testing segregation against the RL pattern while Dixon's tests are. In this article, we compare Pielou's and Dixon's NNCT-tests; introduce the one-sided versions of Pielou's test; extend the use of NNCT-tests for testing complete spatial randomness (CSR) of points from two or more classes (which is called \emph{CSR independence}, henceforth). We assess the finite sample performance of the tests by an extensive Monte Carlo simulation study and demonstrate that Dixon's tests are also appropriate for testing CSR independence; but Pielou's test and the corresponding one-sided versions are liberal for testing CSR independence or RL. Furthermore, we show that Pielou's tests are only appropriate when the NNCT is based on a random sample of (base, NN) pairs. We also prove the consistency of the tests under their appropriate null hypotheses. Moreover, we investigate the edge (or boundary) effects on the NNCT-tests and compare the buffer zone and toroidal edge correction methods for these tests. We illustrate the tests on a real life and an artificial data set.},
  file        = {:http\://arxiv.org/pdf/0807.4236v1:PDF},
  keywords    = {stat.ME, stat.AP},
}

@Article{Feng2018,
  author      = {Long Feng},
  date        = {2018-12-27},
  title       = {Power Comparison between High Dimensional t-Test, Sign, and Signed Rank Tests},
  eprint      = {1812.10625},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we propose a power comparison between high dimensional t-test, sign and signed rank test for the one sample mean test. We show that the high dimensional signed rank test is superior to a high dimensional t test, but inferior to a high dimensional sign test.},
  file        = {:http\://arxiv.org/pdf/1812.10625v1:PDF},
  keywords    = {stat.ME},
}

@Article{Steimle2019,
  author      = {Markus Steimle and Till Menzel and Markus Maurer},
  date        = {2019-05-22},
  title       = {A Method for Classifying Test Bench Configurations in a Scenario-Based Test Approach for Automated Vehicles},
  eprint      = {1905.09018},
  eprintclass = {eess.SP},
  eprinttype  = {arXiv},
  abstract    = {The introduction of automated vehicles demands a way to prove their safe operation. However, validating the safety of automated vehicles is still an unsolved problem. While the scenario-based test approach seems to provide a possible solution, it requires the execution of a high amount of test cases. Several test benches, from actual test vehicles to partly or fully simulated environments, are available, but choosing the optimal test bench, e.g. in terms of required execution time and costs, is a difficult task. Every test bench provides different elements, e.g. simulation models which can be used for test case execution. The composition of elements at a specific test bench is called test bench configuration. This test bench configuration determines the actual performance of a test bench and, therefore, whether the run of a particular test case provides valid test case results with respect to the intended purpose, e.g. a safety validation. For an effective and efficient test case execution, a method is required to assign test cases to the most appropriate test bench configuration. Therefore, it is indispensable to have a method to classify test bench configurations in a clear and reproducible manner. In this paper, we propose a method for classifying test benches and test bench configurations and illustrate the classification method with some examples. The classification method serves as a basis for a systematic assignment of test cases to test bench configurations which allows for an effective and efficient test case execution.},
  file        = {:http\://arxiv.org/pdf/1905.09018v1:PDF},
  keywords    = {eess.SP},
}

@Article{Kim2004,
  author      = {Song-Ju Kim and Ken Umeno and Akio Hasegawa},
  date        = {2004-01-27},
  title       = {Corrections of the NIST Statistical Test Suite for Randomness},
  eprint      = {nlin/0401040},
  eprintclass = {nlin.CD},
  eprinttype  = {arXiv},
  abstract    = {It is well known that the NIST statistical test suite was used for the evaluation of AES candidate algorithms. We have found that the test setting of Discrete Fourier Transform test and Lempel-Ziv test of this test suite are wrong. We give four corrections of mistakes in the test settings. This suggests that re-evaluation of the test results should be needed.},
  file        = {:http\://arxiv.org/pdf/nlin/0401040v1:PDF},
  keywords    = {nlin.CD},
}

@Article{Kattumannil2020,
  author      = {Sudheesh Kattumannil},
  date        = {2020-01-22},
  title       = {A new goodness of fit test for normal distribution based on Stein's characterization},
  eprint      = {2001.07932},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {We develop a new non-parametric test for testing normal distribution using Stein's characterization. We study asymptotic properties of the test statistic. We also develop jackknife empirical likelihood ratio test for testing normality. Using Monte Carlo simulation study, we evaluate the finite sample performance of the proposed JEL based test. Finally, we illustrate our test procedure using two real data.},
  file        = {:http\://arxiv.org/pdf/2001.07932v2:PDF},
  keywords    = {math.ST, stat.TH, 62G10, 62G20},
}

@Article{Enoiu2018,
  author      = {Eduard Enoiu and Mirgita Frasheri},
  date        = {2018-02-12},
  title       = {Test Agents: Adaptive, Autonomous and Intelligent Test Cases},
  eprint      = {1802.03921},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Growth of software size, lack of resources to perform regression testing, and failure to detect bugs faster have seen increased reliance on continuous integration and test automation. Even with greater hardware and software resources dedicated to test automation, software testing is faced with enormous challenges, resulting in increased dependence on complex mechanisms for automated test case selection and prioritization as part of a continuous integration framework. These mechanisms are currently using simple entities called test cases that are concretely realized as executable scripts. Our key idea is to provide test cases with more reasoning, adaptive behavior and learning capabilities by using the concepts of intelligent software agents. We refer to such test cases as test agents. The model that underlie a test agent is capable of flexible and autonomous actions in order to meet overall testing objectives. Our goal is to increase the decentralization of regression testing by letting test agents to know for themselves when they should be executing, how they should update their purpose, and when they should interact with each other. In this paper, we envision software test agents that display such adaptive autonomous behavior. Emerging developments and challenges regarding the use of test agents are explored-in particular, new research that seeks to use adaptive autonomous agents in software testing.},
  file        = {:http\://arxiv.org/pdf/1802.03921v1:PDF},
  keywords    = {cs.SE},
}

@Article{Betka2021,
  author      = {Maik Betka and Stefan Wagner},
  date        = {2021-03-15},
  title       = {Extreme mutation testing in practice: An industrial case study},
  eprint      = {2103.08480},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Mutation testing is used to evaluate the effectiveness of test suites. In recent years, a promising variation called extreme mutation testing emerged that is computationally less expensive. It identifies methods where their functionality can be entirely removed, and the test suite would not notice it, despite having coverage. These methods are called pseudo-tested. In this paper, we compare the execution and analysis times for traditional and extreme mutation testing and discuss what they mean in practice. We look at how extreme mutation testing impacts current software development practices and discuss open challenges that need to be addressed to foster industry adoption. For that, we conducted an industrial case study consisting of running traditional and extreme mutation testing in a large software project from the semiconductor industry that is covered by a test suite of more than 11,000 unit tests. In addition to that, we did a qualitative analysis of 25 pseudo-tested methods and interviewed two experienced developers to see how they write unit tests and gathered opinions on how useful the findings of extreme mutation testing are. Our results include execution times, scores, numbers of executed tests and mutators, reasons why methods are pseudo-tested, and an interview summary. We conclude that the shorter execution and analysis times are well noticeable in practice and show that extreme mutation testing supplements writing unit tests in conjunction with code coverage tools. We propose that pseudo-tested code should be highlighted in code coverage reports and that extreme mutation testing should be performed when writing unit tests rather than in a decoupled session. Future research should investigate how to perform extreme mutation testing while writing unit tests such that the results are available fast enough but still meaningful.},
  file        = {:http\://arxiv.org/pdf/2103.08480v1:PDF},
  keywords    = {cs.SE, D.2.5},
}

@Article{Haq2021,
  author      = {Fitash Ul Haq and Donghwan Shin and Shiva Nejati and Lionel Briand},
  date        = {2021-01-26},
  title       = {Can Offline Testing of Deep Neural Networks Replace Their Online Testing?},
  eprint      = {2101.11118},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {We distinguish two general modes of testing for Deep Neural Networks (DNNs): Offline testing where DNNs are tested as individual units based on test datasets obtained independently from the DNNs under test, and online testing where DNNs are embedded into a specific application environment and tested in a closed-loop mode in interaction with the application environment. Typically, DNNs are subjected to both types of testing during their development life cycle where offline testing is applied immediately after DNN training and online testing follows after offline testing and once a DNN is deployed within a specific application environment. In this paper, we study the relationship between offline and online testing. Our goal is to determine how offline testing and online testing differ or complement one another and if we can use offline testing results to run fewer tests during online testing to reduce the testing cost. Though these questions are generally relevant to all autonomous systems, we study them in the context of automated driving systems where, as study subjects, we use DNNs automating end-to-end controls of steering functions of self-driving vehicles. Our results show that offline testing is more optimistic than online testing as many safety violations identified by online testing could not be identified by offline testing, while large prediction errors generated by offline testing always led to severe safety violations detectable by online testing. Further, we cannot use offline testing results to run fewer tests during online testing in practice since we are not able to identify specific situations where offline testing could be as accurate as online testing in identifying safety requirement violations.},
  file        = {:http\://arxiv.org/pdf/2101.11118v1:PDF},
  keywords    = {cs.SE, cs.LG},
}

@Article{Yunus2007,
  author      = {Rossita M Yunus and Shahjahan Khan},
  date        = {2007-10-10},
  title       = {Increasing power of the test through pre-test - a robust method},
  eprint      = {0710.1919},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {This paper develops robust test procedures for testing the intercept of a simple regression model when it is \textit{apriori} suspected that the slope has a specified value. Defining unrestricted test (UT), restricted test (RT) and pre-test test (PTT) corresponding to the unrestricted (UE), restricted (RE), and preliminary test estimators (PTE) in the estimation case, the M-estimation methodology is used to formulate the M-tests and derive their asymptotic power functions. Analytical and graphical comparisons of the three tests are obtained by studying the power functions with respect to size and power of the tests. It is shown that PTT achieves a reasonable dominance over the others asymptotically.},
  file        = {:http\://arxiv.org/pdf/0710.1919v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Kada2008,
  author       = {Masaru Kada and Harumichi Nishimura and Tomoyuki Yamakami},
  date         = {2008-09-11},
  journaltitle = {J. Phys. A: Math. Theor. 41 (2008) 395309},
  title        = {The Efficiency of Quantum Identity Testing of Multiple States},
  doi          = {10.1088/1751-8113/41/39/395309},
  eprint       = {0809.2037},
  eprintclass  = {quant-ph},
  eprinttype   = {arXiv},
  abstract     = {We examine two quantum operations, the Permutation Test and the Circle Test, which test the identity of n quantum states. These operations naturally extend the well-studied Swap Test on two quantum states. We first show the optimality of the Permutation Test for any input size n as well as the optimality of the Circle Test for three input states. In particular, when n=3, we present a semi-classical protocol, incorporated with the Swap Test, which approximates the Circle Test efficiently. Furthermore, we show that, with help of classical preprocessing, a single use of the Circle Test can approximate the Permutation Test efficiently for an arbitrary input size n.},
  file         = {:http\://arxiv.org/pdf/0809.2037v1:PDF},
  keywords     = {quant-ph},
}

@Article{Hikima2021,
  author      = {Yasunari Hikima and Atsushi Iwasaki and Ken Umeno},
  date        = {2021-03-19},
  title       = {The reference distributions of Maurer's universal statistical test and its improved tests},
  eprint      = {2103.10660},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Maurer's universal statistical test can widely detect non-randomness of given sequences. Coron proposed an improved test, and further Yamamoto and Liu proposed a new test based on Coron's test. These tests use normal distributions as their reference distributions, but the soundness has not been theoretically discussed so far. Additionally, Yamamoto and Liu's test uses an experimental value as the variance of its reference distribution. In this paper, we theoretically derive the variance of the reference distribution of Yamamoto and Liu's test and prove that the true reference distribution of Coron's test converges to a normal distribution in some sense. We can apply the proof to the other tests with small changes.},
  file        = {:http\://arxiv.org/pdf/2103.10660v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Xu2019,
  author      = {Tianyin Xu and Owolabi Legunsen},
  date        = {2019-05-29},
  title       = {Configuration Testing: Testing Configuration Values as Code and with Code},
  eprint      = {1905.12195},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {This paper proposes configuration testing--evaluating configuration values (to be deployed) by exercising the code that uses the values and assessing the corresponding program behavior. We advocate that configuration values should be systematically tested like software code and that configuration testing should be a key reliability engineering practice for preventing misconfigurations from production deployment. The essential advantage of configuration testing is to put the configuration values (to be deployed) in the context of the target software program under test. In this way, the dynamic effects of configuration values and the impact of configuration changes can be observed during testing. Configuration testing overcomes the fundamental limitations of de facto approaches to combatting misconfigurations, namely configuration validation and software testing--the former is disconnected from code logic and semantics, while the latter can hardly cover all possible configuration values and their combinations. Our preliminary results show the effectiveness of configuration testing in capturing real-world misconfigurations. We present the principles of writing new configuration tests and the promises of retrofitting existing software tests to be configuration tests. We discuss new adequacy and quality metrics for configuration testing. We also explore regression testing techniques to enable incremental configuration testing during continuous integration and deployment in modern software systems.},
  file        = {:http\://arxiv.org/pdf/1905.12195v2:PDF},
  keywords    = {cs.SE},
}

@Article{Lu2019,
  author      = {Zeng-Hua Lu},
  date        = {2019-11-12},
  title       = {Extended MinP Tests of Multiple Hypotheses},
  eprint      = {1911.04696},
  eprintclass = {econ.EM},
  eprinttype  = {arXiv},
  abstract    = {Much empirical research in economics and finance involves simultaneously testing multiple hypotheses. This paper proposes extended MinP (EMinP) tests by expanding the minimand set of the MinP test statistic to include the $p$% -value of a global test such as a likelihood ratio test. We show that, compared with MinP tests, EMinP tests may considerably improve the global power in rejecting the intersection of all individual hypotheses. Compared with closed tests EMinP tests have the computational advantage by sharing the benefit of the stepdown procedure of MinP tests and can have a better global power over the tests used to construct closed tests. Furthermore, we argue that EMinP tests may be viewed as a tool to prevent data snooping when two competing tests that have distinct global powers are exploited. Finally, the proposed tests are applied to an empirical application on testing the effects of exercise.},
  file        = {:http\://arxiv.org/pdf/1911.04696v1:PDF},
  keywords    = {econ.EM},
}

@Article{Wildandyawan2020,
  author      = {Adrian Wildandyawan and Yasuharu Nishi},
  date        = {2020-02-12},
  title       = {Object-based Metamorphic Testing through Image Structuring},
  eprint      = {2002.07046},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {Testing software is often costly due to the need of mass-producing test cases and providing a test oracle for it. This is often referred to as the oracle problem. One method that has been proposed in order to alleviate the oracle problem is metamorphic testing. Metamorphic testing produces new test cases by altering an existing test case, and uses the metamorphic relation between the inputs and the outputs of the System Under Test (SUT) to predict the expected outputs of the produced test cases. Metamorphic testing has often been used for image processing software, where changes are applied to the image's attributes to create new test cases with annotations that are the same as the original image. We refer to this existing method as the image-based metamorphic testing. In this research, we propose an object-based metamorphic testing and a composite metamorphic testing which combines different metamorphic testing approaches to relatively increase test coverage.},
  file        = {:http\://arxiv.org/pdf/2002.07046v1:PDF},
  keywords    = {cs.LG, cs.SE},
}

@Article{Bures2018,
  author      = {Miroslav Bures and Bestoun S. Ahmed},
  date        = {2018-02-22},
  title       = {Employment of Multiple Algorithms for Optimal Path-based Test Selection Strategy},
  doi         = {10.1016/j.infsof.2019.06.006},
  eprint      = {1802.08005},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Executing various sequences of system functions in a system under test represents one of the primary techniques in software testing. The natural way to create effective, consistent and efficient test sequences is to model the system under test and employ an algorithm to generate the tests that satisfy a defined test coverage criterion. Several criteria of test set optimality can be defined. In addition, to optimize the test set from an economic viewpoint, the priorities of the various parts of the system model under test must be defined. Using this prioritization, the test cases exercise the high priority parts of the system under test more intensely than those with low priority. Evidence from the literature and our observations confirm that finding a universal algorithm that produces an optimal test set for all test coverage and test set optimality criteria is a challenging task. Moreover, for different individual problem instances, different algorithms provide optimal results. In this paper, we present a path-based strategy to perform optimal test selection. The strategy first employs a set of current algorithms to generate test sets; then, it assesses the optimality of each test set by the selected criteria, and finally, chooses the optimal test set. The experimental results confirm the validity and usefulness of this strategy. For individual instances of 50 system under test models, different algorithms provided optimal results; these results varied by the required test coverage level, the size of the priority parts of the model, and the selected test set optimality criteria.},
  file        = {:http\://arxiv.org/pdf/1802.08005v1:PDF},
  keywords    = {cs.SE},
}

@Article{Bures2019,
  author      = {Miroslav Bures and Bestoun S. Ahmed and Kamal Z. Zamli},
  date        = {2019-03-20},
  title       = {Prioritized Process Test: An Alternative to Current Process Testing Strategies},
  eprint      = {1903.08531},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Testing processes and workflows in information and Internet of Things systems is a major part of the typical software testing effort. Consistent and efficient path-based test cases are desired to support these tests. Because certain parts of software system workflows have a higher business priority than others, this fact has to be involved in the generation of test cases. In this paper, we propose a Prioritized Process Test (PPT), which is a model-based test case generation algorithm that represents an alternative to currently established algorithms that use directed graphs and test requirements to model the system under test. The PPT accepts a directed multigraph as a model to express priorities, and edge weights are used instead of test requirements. To determine the test-coverage level of test cases, a test-depth-level concept is used. We compared the presented PPT with five alternatives (i.e., the Process Cycle Test, a naive reduction of test set created by the Process Cycle Test, Brute Force algorithm, Set-covering Based Solution and Matching-based Prefix Graph Solution) for edge coverage and edge-pair coverage. To assess the optimality of the path-based test cases produced by these strategies, we used fourteen metrics based on the properties of these test cases and 59 models that were created for three real-world systems. For all edge coverage, the PPT produced more optimal test cases than the alternatives in terms of the majority of the metrics. For edge-pair coverage, the PPT strategy yielded similar results to those of the alternatives. Thus, the PPT strategy is an applicable alternative, as it reflects both the required test coverage level and the business priority in parallel.},
  file        = {:http\://arxiv.org/pdf/1903.08531v1:PDF},
  keywords    = {cs.SE},
}

@Article{Kayes2013,
  author      = {Imrul Kayes and Jacob Chakareski},
  date        = {2013-11-17},
  title       = {ComReg: A Complex Network Approach to Prioritize Test Cases for Regression Testing},
  eprint      = {1311.4176},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Regression testing is performed to provide confidence that changes in a part of software do not affect other parts of the software. An execution of all existing test cases is the best way to re-establish this confidence. However, regression testing is an expensive process---there might be insufficient resources (e.g., time, workforce) to allow for the re-execution of all test cases. Regression test prioritization techniques attempt to re-order a regression test suite based on some criteria so that highest priority test cases are executed earlier. In this study, we want to prioritize test cases for regression testing based on the dependency network of faults. In software testing, it is common that some faults are consequences of other faults (leading faults). Moreover, dependent faults can be removed if and only if the leading faults have been removed. Our goal is to prioritize test cases so that test cases that exposed leading faults (the most central faults in the fault dependency network) in the system testing phase, are executed first in regression testing. We present ComReg, a test case prioritization technique based on the dependency network of faults. We model a fault dependency network as a directed graph and identify leading faults to prioritize test cases for regression testing. We use a centrality aggregation technique which considers six network representative centrality metrics to identify leading faults in the fault dependency network. We also discuss the use of fault communities to select an arbitrary percentage of the test cases from a prioritized regression test suite. We conduct a case study that evaluates the effectiveness and applicability of the proposed method.},
  file        = {:http\://arxiv.org/pdf/1311.4176v3:PDF},
  keywords    = {cs.SE},
}

@Article{Cohen2009,
  author      = {Ernie Cohen},
  date        = {2009-10-06},
  title       = {Pessimistic Testing},
  eprint      = {0910.0996},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {We propose a new approach to testing conformance to a nondeterministic specification, in which testing proceeds only as long as increased test coverage is guaranteed.},
  file        = {:http\://arxiv.org/pdf/0910.0996v1:PDF},
  keywords    = {cs.SE, cs.GT, D.2.5},
}

@Article{Isabella2012,
  author       = {A. Isabella and Emi Retna},
  date         = {2012-02-21},
  journaltitle = {International Journal of Software Engineering & Applications (IJSEA), Vol.3, No.1, January 2012},
  title        = {Study Paper on Test Case generation for GUI Based Testing},
  eprint       = {1202.4527},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {With the advent of WWW and outburst in technology and software development, testing the software became a major concern. Due to the importance of the testing phase in a software development life cycle, testing has been divided into graphical user interface (GUI) based testing, logical testing, integration testing, etc.GUI Testing has become very important as it provides more sophisticated way to interact with the software. The complexity of testing GUI increased over time. The testing needs to be performed in a way that it provides effectiveness, efficiency, increased fault detection rate and good path coverage. To cover all use cases and to provide testing for all possible (success/failure) scenarios the length of the test sequence is considered important. Intent of this paper is to study some techniques used for test case generation and process for various GUI based software applications.},
  file         = {:http\://arxiv.org/pdf/1202.4527v1:PDF},
  keywords     = {cs.SE},
}

@Article{Bassil2012,
  author      = {Youssef Bassil},
  date        = {2012-03-24},
  title       = {Distributed, Cross-Platform, and Regression Testing Architecture for Service-Oriented Architecture},
  eprint      = {1203.5403},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {As per leading IT experts, today's large enterprises are going through business transformations. They are adopting service-based IT models such as SOA to develop their enterprise information systems and applications. In fact, SOA is an integration of loosely-coupled interoperable components, possibly built using heterogeneous software technologies and hardware platforms. As a result, traditional testing architectures are no more adequate for verifying and validating the quality of SOA systems and whether they are operating to specifications. This paper first discusses the various state-of-the-art methods for testing SOA applications, and then it proposes a novel automated, distributed, cross-platform, and regression testing architecture for SOA systems. The proposed testing architecture consists of several testing units which include test engine, test code generator, test case generator, test executer, and test monitor units. Experiments conducted showed that the proposed testing architecture managed to use parallel agents to test heterogeneous web services whose technologies were incompatible with the testing framework. As future work, testing non-functional aspects of SOA applications are to be investigated so as to allow the testing of such properties as performance, security, availability, and scalability.},
  file        = {:http\://arxiv.org/pdf/1203.5403v1:PDF},
  keywords    = {cs.SE},
}

@Article{Groce2017,
  author      = {Alex Groce and Josie Holmes},
  date        = {2017-11-05},
  title       = {Provenance and Pseudo-Provenance for Seeded Learning-Based Automated Test Generation},
  eprint      = {1711.01661},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {Many methods for automated software test generation, including some that explicitly use machine learning (and some that use ML more broadly conceived) derive new tests from existing tests (often referred to as seeds). Often, the seed tests from which new tests are derived are manually constructed, or at least simpler than the tests that are produced as the final outputs of such test generators. We propose annotation of generated tests with a provenance (trail) showing how individual generated tests of interest (especially failing tests) derive from seed tests, and how the population of generated tests relates to the original seed tests. In some cases, post-processing of generated tests can invalidate provenance information, in which case we also propose a method for attempting to construct "pseudo-provenance" describing how the tests could have been (partly) generated from seeds.},
  file        = {:http\://arxiv.org/pdf/1711.01661v2:PDF},
  keywords    = {stat.ML, cs.SE},
}

@Article{Erlenhov2020,
  author      = {Linda Erlenhov and Francisco Gomes de Oliveira Neto and Martin Chukaleski and Samer Daknache},
  date        = {2020-04-21},
  title       = {Challenges and guidelines on designing test cases for test bots},
  doi         = {10.1145/3387940.3391535},
  eprint      = {2004.10143},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Test bots are automated testing tools that autonomously and periodically run a set of test cases that check whether the system under test meets the requirements set forth by the customer. The automation decreases the amount of time a development team spends on testing. As development projects become larger, it is important to focus on improving the test bots by designing more effective test cases because otherwise time and usage costs can increase greatly and misleading conclusions from test results might be drawn, such as false positives in the test execution. However, literature currently lacks insights on how test case design affects the effectiveness of test bots. This paper uses a case study approach to investigate those effects by identifying challenges in designing tests for test bots. Our results include guidelines for test design schema for such bots that support practitioners in overcoming the challenges mentioned by participants during our study.},
  file        = {:http\://arxiv.org/pdf/2004.10143v1:PDF},
  keywords    = {cs.SE},
}

@Article{Cheng2021,
  author      = {Xiwei Cheng and Sidharth Jaggi and Qiaoqiao Zhou},
  date        = {2021-02-20},
  title       = {Generalized Non-adaptive Group Testing},
  eprint      = {2102.10256},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  abstract    = {In the problem of classical group testing one aims to identify a small subset (of expected size $d$) diseased individuals/defective items in a large population (of size $n$) based on a minimal number of suitably-designed group non-adaptive tests on subsets of items, where the test outcome is governed by an "OR" function, i.e., the test outcome is positive iff the given test contains at least one defective item. Motivated by physical considerations we consider a generalized scenario (that includes as special cases multiple other group-testing-like models in the literature) wherein the test outcome is governed by an arbitrary $\textit{monotone}$ (stochastic) test function $f(\cdot)$, with the test outcome being positive with probability $f(x)$, where $x$ is the number of defectives tested in that pool. For any monotone test function $f(\cdot)$ we present a non-adaptive generalized group-testing scheme that identifies all defective items with high probability. Our scheme requires at most ${\cal O}(d^2\log(n))$ tests for any monotone test function $f(\cdot)$, and at most ${\cal O}(d\log(n))$ in the physically relevant sub-class of $\textit{sensitive}$ test functions (and hence is information-theoretically order-optimal for this sub-class).},
  file        = {:http\://arxiv.org/pdf/2102.10256v1:PDF},
  keywords    = {cs.IT, math.IT},
}

@Article{Tsumura2009,
  author      = {Yu Tsumura},
  date        = {2009-12-29},
  title       = {Primality tests for 2^kn-1 using elliptic curves},
  eprint      = {0912.5279},
  eprintclass = {math.NT},
  eprinttype  = {arXiv},
  abstract    = {We propose some primality tests for 2^kn-1, where k, n in Z, k>= 2 and n odd. There are several tests depending on how big n is. These tests are proved using properties of elliptic curves. Essentially, the new primality tests are the elliptic curve version of the Lucas-Lehmer-Riesel primality test. Note:An anonymous referee suggested that Benedict H. Gross already proved the same result about a primality test for Mersenne primes using elliptic curve.},
  file        = {:http\://arxiv.org/pdf/0912.5279v1:PDF},
  keywords    = {math.NT, 11Y11; 11Y05},
}

@Article{Polivaev2012,
  author       = {Dimitry Polivaev},
  date         = {2012-02-28},
  journaltitle = {EPTCS 80, 2012, pp. 103-114},
  title        = {Rule-based Test Generation with Mind Maps},
  doi          = {10.4204/EPTCS.80.8},
  eprint       = {1202.6125},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {This paper introduces basic concepts of rule based test generation with mind maps, and reports experiences learned from industrial application of this technique in the domain of smart card testing by Giesecke & Devrient GmbH over the last years. It describes the formalization of test selection criteria used by our test generator, our test generation architecture and test generation framework.},
  file         = {:http\://arxiv.org/pdf/1202.6125v1:PDF},
  keywords     = {cs.SE},
}

@Article{Dorman2014,
  author      = {A. M. Dorman},
  date        = {2014-08-30},
  title       = {Computerized Multi Microphone Test System},
  eprint      = {1409.0117},
  eprintclass = {cs.SD},
  eprinttype  = {arXiv},
  abstract    = {An acoustic testing approach based on the concept of a microphone sensor surrounding the product under test is proposed. Microphone signals are processed simultaneously by a test system computer, according to the objective of the test. The spatial and frequency domain selectivity features of this method are examined. Sound-spatial visualization algorithm is observed. A test system design based on the concept of a microphone surrounding the tested product has the potential to improve distortion measurement accuracy.},
  file        = {:http\://arxiv.org/pdf/1409.0117v1:PDF},
  keywords    = {cs.SD},
}

@Article{Ryabko2006,
  author      = {Boris Ryabko and Jaakko Astola},
  date        = {2006-02-25},
  title       = {Universal Codes as a Basis for Time Series Testing},
  eprint      = {cs/0602084},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  abstract    = {We suggest a new approach to hypothesis testing for ergodic and stationary processes. In contrast to standard methods, the suggested approach gives a possibility to make tests, based on any lossless data compression method even if the distribution law of the codeword lengths is not known. We apply this approach to the following four problems: goodness-of-fit testing (or identity testing), testing for independence, testing of serial independence and homogeneity testing and suggest nonparametric statistical tests for these problems. It is important to note that practically used so-called archivers can be used for suggested testing.},
  file        = {:http\://arxiv.org/pdf/cs/0602084v1:PDF},
  keywords    = {cs.IT, math.IT},
}

@Article{McKague2016,
  author      = {Matthew McKague},
  date        = {2016-09-30},
  title       = {Self-testing in parallel with CHSH},
  eprint      = {1609.09584},
  eprintclass = {quant-ph},
  eprinttype  = {arXiv},
  abstract    = {Self-testing allows classical referees to verify the quantum behaviour of some untrusted devices. Recently we developed a framework for building large self-tests by repeating a smaller self-test many times in parallel. However, the framework did not apply to the CHSH test, which tests a maximally entangled pair of qubits. CHSH is the most well known and widely used test of this type. Here we extend the parallel self-testing framework to build parallel CHSH self-tests for any number of pairs of maximally entangled qubits. Our construction achieves an error bound which is polynomial in the number of tested qubit pairs.},
  file        = {:http\://arxiv.org/pdf/1609.09584v4:PDF},
  keywords    = {quant-ph},
}

@Article{Cuparic2018,
  author      = {Marija CupariÄ‡ and Bojana MiloÅ¡eviÄ‡ and Marko ObradoviÄ‡},
  date        = {2018-09-20},
  title       = {New $L^2$-type exponentiality tests},
  eprint      = {1809.07585},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {We introduce new consistent and scale-free goodness-of-fit tests for the exponential distribution based on Puri-Rubin characterization. For the construction of test statistics we employ weighted $L^2$ distance between $V$-empirical Laplace transforms of random variables that appear in the characterization. The resulting test statistics are degenerate V-statistics with estimated parameters. We compare our tests, in terms of the Bahadur efficiency, to the likelihood ratio test, as well as some recent characterization based goodness-of-fit tests for the exponential distribution. We also compare the powers of our tests to the powers of some recent and classical exponentiality tests. In both criteria, our tests are shown to be strong and outperform most of their competitors.},
  file        = {:http\://arxiv.org/pdf/1809.07585v1:PDF},
  keywords    = {stat.ME, 62G10, 62G20},
}

@Article{Charbachi2017,
  author      = {Peter Charbachi and Linus Eklund and Eduard Enoiu},
  date        = {2017-06-06},
  title       = {Can Pairwise Testing Perform Comparably to Manually Handcrafted Testing Carried Out by Industrial Engineers?},
  eprint      = {1706.01636},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Testing is an important activity in engineering of industrial software. For such software, testing is usually performed manually by handcrafting test suites based on specific design techniques and domain-specific experience. To support developers in testing, different approaches for producing good test suites have been proposed. In the last couple of years combinatorial testing has been explored with the goal of automatically combining the input values of the software based on a certain strategy. Pairwise testing is a combinatorial technique used to generate test suites by varying the values of each pair of input parameters to a system until all possible combinations of those parameters are created. There is some evidence suggesting that these kinds of techniques are efficient and relatively good at detecting software faults. Unfortunately, there is little experimental evidence on the comparison of these combinatorial testing techniques with, what is perceived as, rigorous manually handcrafted testing. In this study we compare pairwise test suites with test suites created manually by engineers for 45 industrial programs. The test suites were evaluated in terms of fault detection, code coverage and number of tests. The results of this study show that pairwise testing, while useful for achieving high code coverage and fault detection for the majority of the programs, is almost as effective in terms of fault detection as manual testing. The results also suggest that pairwise testing is just as good as manual testing at fault detection for 64% of the programs.},
  file        = {:http\://arxiv.org/pdf/1706.01636v1:PDF},
  keywords    = {cs.SE},
}

@Article{Kampmann2019,
  author      = {Alexander Kampmann and Andreas Zeller},
  date        = {2019-06-04},
  title       = {Bridging the Gap between Unit Test Generation and System Test Generation},
  eprint      = {1906.01463},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Common test generators fall into two categories. Generating test inputs at the unit level is fast, but can lead to false alarms when a function is called with inputs that would not occur in a system context. If a generated input at the system level causes a failure, this is a true alarm, as the input could also have come from the user or a third party; but system testing is much slower. In this paper, we introduce the concept of a test generation bridge, which joins the accuracy of system testing with the speed of unit testing. A Test Generation Bridge allows to combine an arbitrary system test generator with an arbitrary unit test generator. It does so by carving parameterized unit tests from system (test) executions. These unit tests run in a context recorded from the system test, but individual parameters are left free for the unit test generator to systematically explore. This allows symbolic test generators such as KLEE to operate on individual functions in the recorded system context. If the test generator detects a failure, we lift the failure-inducing parameter back to the system input; if the failure can be reproduced at the system level, it is reported as a true alarm. Our BASILISK prototype can extract and test units out of complex systems such as a Web/Python/SQLite/C stack; in its evaluation, it achieves a higher coverage than a state-of-the-art system test generator.},
  file        = {:http\://arxiv.org/pdf/1906.01463v1:PDF},
  keywords    = {cs.SE},
}

@Article{Junior2020,
  author      = {Nildo Silva Junior and Larissa Rocha and Luana Almeida Martins and Ivan Machado},
  date        = {2020-03-12},
  title       = {A survey on test practitioners' awareness of test smells},
  eprint      = {2003.05613},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Developing test code may be a time-consuming task that usually requires much effort and cost, especially when it is done manually. Besides, during this process, developers and testers are likely to adopt bad design choices, which may lead to the introduction of the so-called test smells in test code. Test smells are bad solutions to either implement or design test code. As the test code with test smells increases in size, these tests might become more complex, and as a consequence, much harder to understand and evolve correctly. Therefore, test smells may have a negative impact on the quality and maintenance of test code and may also harm the whole software testing activities. In this context, this study aims to understand whether test professionals non-intentionally insert test smells. We carried out an expert survey to analyze the usage frequency of a set of test smells. Sixty professionals from different companies participated in the survey. We selected 14 widely studied smells from the literature, which are also implemented in existing test smell detection tools. The yielded results indicate that experienced professionals introduce test smells during their daily programming tasks, even when they are using standardized practices from their companies, and not only for their personal assumptions. Another relevant evidence was that developers' professional experience can not be considered as a root-cause for the insertion of test smells in test code.},
  file        = {:http\://arxiv.org/pdf/2003.05613v1:PDF},
  keywords    = {cs.SE},
}

@Article{Tufano2020,
  author      = {Michele Tufano and Dawn Drain and Alexey Svyatkovskiy and Shao Kun Deng and Neel Sundaresan},
  date        = {2020-09-11},
  title       = {Unit Test Case Generation with Transformers},
  eprint      = {2009.05617},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Automated Unit Test Case generation has been the focus of extensive literature within the research community. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult to read or understand for developers. In this paper we propose AthenaTest, an approach that aims at generating unit test cases by learning from real-world, developer-written test cases. Our approach relies on a state-of-the-art sequence-to-sequence transformer model which is able to write useful test cases for a given method under test (i.e., focal method). We also introduce methods2test - the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 630k test cases mined from 70k open-source repositories hosted on GitHub. We use this dataset to train a transformer model to translate focal methods into the corresponding test cases. We evaluate the ability of our model in generating test cases using natural language processing as well as code-specific criteria. First, we assess the quality of the translation compared to the target test case, then we analyze properties of the test case such as syntactic correctness and number and variety of testing APIs (e.g., asserts). We execute the test cases, collect test coverage information, and compare them with test cases generated by EvoSuite and GPT-3. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated test cases.},
  file        = {:http\://arxiv.org/pdf/2009.05617v1:PDF},
  keywords    = {cs.SE, cs.CL, cs.LG},
}

@Article{Chen2020,
  author      = {Junjie Chen and Ming Yan and Zan Wang and Yuning Kang and Zhuo Wu},
  date        = {2020-10-10},
  title       = {Deep Neural Network Test Coverage: How Far Are We?},
  eprint      = {2010.04946},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {DNN testing is one of the most effective methods to guarantee the quality of DNN. In DNN testing, many test coverage metrics have been proposed to measure test effectiveness, including structural coverage and non-structural coverage (which are classified according to whether considering which structural elements are covered during testing). Those test coverage metrics are proposed based on the assumption: they are correlated with test effectiveness (i.e., the generation of adversarial test inputs or the error-revealing capability of test inputs in DNN testing studies). However, it is still unknown whether the assumption is tenable. In this work, we conducted the first extensive study to systematically validate the assumption by controlling for the size of test sets. In the study, we studied seven typical test coverage metrics based on 9 pairs of datasets and models with great diversity (including four pairs that have never been used to evaluate these test coverage metrics before). The results demonstrate that the assumption fails for structural coverage in general but holds for non-structural coverage on more than half of subjects, indicating that measuring the difference of DNN behaviors between test inputs and training data is more promising than measuring which structural elements are covered by test inputs for measuring test effectiveness. Even so, the current non-structural coverage metrics still can be improved from several aspects such as unfriendly parameters and unstable performance. That indicates that although a lot of test coverage metrics have been proposed before, there is still a lot of room for improvement of measuring test effectiveness in DNN testing, and our study has pointed out some promising directions.},
  file        = {:http\://arxiv.org/pdf/2010.04946v2:PDF},
  keywords    = {cs.SE},
}

@Article{Maragathavalli2011,
  author      = {P. Maragathavalli},
  date        = {2011-03-01},
  title       = {Search-based software test data generation using evolutionary computation},
  eprint      = {1103.0125},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Search-based Software Engineering has been utilized for a number of software engineering activities. One area where Search-Based Software Engineering has seen much application is test data generation. Evolutionary testing designates the use of metaheuristic search methods for test case generation. The search space is the input domain of the test object, with each individual or potential solution, being an encoded set of inputs to that test object. The fitness function is tailored to find test data for the type of test that is being undertaken. Evolutionary Testing (ET) uses optimizing search techniques such as evolutionary algorithms to generate test data. The effectiveness of GA-based testing system is compared with a Random testing system. For simple programs both testing systems work fine, but as the complexity of the program or the complexity of input domain grows, GA-based testing system significantly outperforms Random testing.},
  file        = {:http\://arxiv.org/pdf/1103.0125v1:PDF},
  keywords    = {cs.SE},
}

@Article{Panda2019,
  author      = {Sambit Panda and Cencheng Shen and Ronan Perry and Jelle Zorn and Antoine Lutz and Carey E. Priebe and Joshua T. Vogelstein},
  date        = {2019-10-20},
  title       = {Nonparametric MANOVA via Independence Testing},
  eprint      = {1910.08883},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  abstract    = {The $k$-sample testing problem tests whether or not $k$ groups of data points are sampled from the same distribution. Multivariate analysis of variance (MANOVA) is currently the gold standard for $k$-sample testing but makes strong, often inappropriate, parametric assumptions. Moreover, independence testing and $k$-sample testing are tightly related, and there are many nonparametric multivariate independence tests with strong theoretical and empirical properties, including distance correlation (Dcorr) and Hilbert-Schmidt-Independence-Criterion (Hsic). We prove that universally consistent independence tests achieve universally consistent $k$-sample testing and that $k$-sample statistics like Energy and Maximum Mean Discrepancy (MMD) are exactly equivalent to Dcorr. Empirically evaluating these tests for $k$-sample scenarios demonstrates that these nonparametric independence tests typically outperform MANOVA, even for Gaussian distributed settings. Finally, we extend these non-parametric $k$-sample testing procedures to perform multiway and multilevel tests. Thus, we illustrate the existence of many theoretically motivated and empirically performant $k$-sample tests. A Python package with all independence and k-sample tests called hyppo is available from https://hyppo.neurodata.io/.},
  file        = {:http\://arxiv.org/pdf/1910.08883v2:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Bertolotti2020,
  author      = {Paolo Bertolotti and Ali Jadbabaie},
  date        = {2020-12-04},
  title       = {Network Group Testing},
  eprint      = {2012.02847},
  eprintclass = {stat.AP},
  eprinttype  = {arXiv},
  abstract    = {We consider the problem of identifying infected individuals in a population of size $N$. Group testing provides an approach to test the entire population using significantly fewer than $N$ tests when infection prevalence is low. The original and most commonly utilized form of group testing, called Dorfman testing, treats each individual's infection probability as independent and homogenous. However, as communicable diseases spread from individual to individual through underlying social networks, an individual's network location affects their infection probability. In this work, we utilize network information to improve group testing. Specifically, we group individuals by community and demonstrate the performance gain over Dorfman testing. After introducing a network and epidemic model, we derive the number of tests used under network grouping. We prove the expected number of tests is upper bounded by Dorfman testing. In addition, we demonstrate network grouping successfully achieves the theoretical lower bound for two-stage testing procedures when networks have strong community structure. On the other hand, network grouping is equivalent to Dorfman testing when networks have no structure. We end by demonstrating network grouping outperforms Dorfman testing in the scenario of a university testing its population for COVID-19 cases.},
  file        = {:http\://arxiv.org/pdf/2012.02847v1:PDF},
  keywords    = {stat.AP},
}

@Article{Romano2021,
  author      = {Alan Romano and Zihe Song and Sampath Grandhi and Wei Yang and Weihang Wang},
  date        = {2021-03-03},
  title       = {An Empirical Analysis of UI-based Flaky Tests},
  eprint      = {2103.02669},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Flaky tests have gained attention from the research community in recent years and with good reason. These tests lead to wasted time and resources, and they reduce the reliability of the test suites and build systems they affect. However, most of the existing work on flaky tests focus exclusively on traditional unit tests. This work ignores UI tests that have larger input spaces and more diverse running conditions than traditional unit tests. In addition, UI tests tend to be more complex and resource-heavy, making them unsuited for detection techniques involving rerunning test suites multiple times. In this paper, we perform a study on flaky UI tests. We analyze 235 flaky UI test samples found in 62 projects from both web and Android environments. We identify the common underlying root causes of flakiness in the UI tests, the strategies used to manifest the flaky behavior, and the fixing strategies used to remedy flaky UI tests. The findings made in this work can provide a foundation for the development of detection and prevention techniques for flakiness arising in UI tests.},
  file        = {:http\://arxiv.org/pdf/2103.02669v1:PDF},
  keywords    = {cs.SE},
}

@Article{Peruma2021,
  author      = {Anthony Peruma and Christian D. Newman},
  date        = {2021-03-17},
  title       = {On the Distribution of "Simple Stupid Bugs" in Unit Test Files: An Exploratory Study},
  eprint      = {2103.09388},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {A key aspect of ensuring the quality of a software system is the practice of unit testing. Through unit tests, developers verify the correctness of production source code, thereby verifying the system's intended behavior under test. However, unit test code is subject to issues, ranging from bugs in the code to poor test case design (i.e., test smells). In this study, we compare and contrast the occurrences of a type of single-statement-bug-fix known as "simple stupid bugs" (SStuBs) in test and non-test (i.e., production) files in popular open-source Java Maven projects. Our results show that SStuBs occur more frequently in non-test files than in test files, with most fix-related code associated with assertion statements in test files. Further, most test files exhibiting SStuBs also exhibit test smells. We envision our findings enabling tool vendors to better support developers in improving the maintenance of test suites.},
  file        = {:http\://arxiv.org/pdf/2103.09388v1:PDF},
  keywords    = {cs.SE},
}

@Article{Swain2012,
  author      = {Ranjita Kumari Swain and Prafulla Kumar Behera and Durga Prasad Mohapatra},
  date        = {2012-06-02},
  title       = {Generation and Optimization of Test cases for Object-Oriented Software Using State Chart Diagram},
  eprint      = {1206.0373},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {The process of testing any software system is an enormous task which is time consuming and costly. The time and required effort to do sufficient testing grow, as the size and complexity of the software grows, which may cause overrun of the project budget, delay in the development of software system or some test cases may not be covered. During SDLC (software development life cycle), generally the software testing phase takes around 40-70% of the time and cost. State-based testing is frequently used in software testing. Test data generation is one of the key issues in software testing. A properly generated test suite may not only locate the errors in a software system, but also help in reducing the high cost associated with software testing. It is often desired that test data in the form of test sequences within a test suite can be automatically generated to achieve required test coverage. This paper proposes an optimization approach to test data generation for the state-based software testing. In this paper, first state transition graph is derived from state chart diagram. Then, all the required information are extracted from the state chart diagram. Then, test cases are generated. Lastly, a set of test cases are minimized by calculating the node coverage for each test case. It is also determined that which test cases are covered by other test cases. The advantage of our test generation technique is that it optimizes test coverage by minimizing time and cost. The proposed test data generation scheme generates test cases which satisfy transition path coverage criteria, path coverage criteria and action coverage criteria. A case study on Automatic Ticket Machine (ATM) has been presented to illustrate our approach.},
  file        = {:http\://arxiv.org/pdf/1206.0373v1:PDF},
  keywords    = {cs.SE},
}

@Article{Jeevarathinam2010,
  author       = {Mrs. R. Jeevarathinam and Dr. Antony Selvadoss Thanamani},
  date         = {2010-02-10},
  journaltitle = {International Journal of Computer Science and Information Security, IJCSIS, Vol. 7, No. 1, pp. 190-195, January 2010, USA},
  title        = {Test Case Generation using Mutation Operators and Fault Classification},
  eprint       = {1002.2197},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Software testing is the important phase of software development process. But, this phase can be easily missed by software developers because of their limited time to complete the project. Since, software developers finish their software nearer to the delivery time; they dont get enough time to test their program by creating effective test cases. . One of the major difficulties in software testing is the generation of test cases that satisfy the given adequacy criterion Moreover, creating manual test cases is a tedious work for software developers in the final rush hours. A new approach which generates test cases can help the software developers to create test cases from software specifications in early stage of software development (before coding) and as well as from program execution traces from after software development (after coding). Heuristic techniques can be applied for creating quality test cases. Mutation testing is a technique for testing software units that has great potential for improving the quality of testing, and to assure the high reliability of software. In this paper, a mutation testing based test cases generation technique has been proposed to generate test cases from program execution trace, so that the test cases can be generated after coding. The paper details about the mutation testing implementation to generate test cases. The proposed algorithm has been demonstrated for an example.},
  file         = {:http\://arxiv.org/pdf/1002.2197v1:PDF},
  keywords     = {cs.SE},
}

@Article{Haramoto2019,
  author      = {Hiroshi Haramoto},
  date        = {2019-12-23},
  title       = {Study on upper limit of sample sizes for a two-level test in NIST SP800-22},
  eprint      = {1912.10602},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {NIST SP800-22 is one of the most widely used statistical testing tools for pseudorandom number generators (PRNGs). This tool consists of 15 tests (one-level tests) and two additional tests (two-level tests). Each one-level test provides one or more $p$-values. The two-level tests measure the uniformity of the obtained $p$-values for a fixed one-level test. One of the two-level tests categorizes the $p$-values into ten intervals of equal length, and apply a chi-squared goodness-of-fit test. This two-level test is often more powerful than one-level tests, but sometimes it rejects even good PRNGs when the sample size at the second level is too large, since it detects approximation errors in the computation of $p$-values. In this paper, we propose a practical upper limit of the sample size in this two-level test, for each of six tests appeared in SP800-22. These upper limits are derived by the chi-squared discrepancy between the distribution of the approximated $p$-values and the uniform distribution $U(0, 1)$. We also computed a "risky" sample size at the second level for each one-level test. Our experiments show that the two-level test with the proposed upper limit gives appropriate results, while using the risky size often rejects even good PRNGs. We also propose another improvement: to use the exact probability for the ten categories in the computation of goodness-of-fit at the two-level test. This allows us to increase the sample size at the second level, and would make the test more sensitive than the NIST's recommending usage.},
  file        = {:http\://arxiv.org/pdf/1912.10602v3:PDF},
  keywords    = {stat.ME, cs.CR, 65C10, 65C60, 68N30},
}

@Article{Berrahou2012,
  author      = {Noureddine Berrahou and Lahcen Douge},
  date        = {2012-11-07},
  title       = {Bahadur efficiency of nonparametric test for independence based on $L_1$-error},
  eprint      = {1211.1725},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {We introduce new test statistic to test the independence of two multi-dimensional random variables. Based on the $L_1$-distance and the historgram density estimation method, the test is compared via Bahadur relative efficiency to several tests available in the literature. It arises that our test reaches better performances than a number of usual tests among whom we cite the Kolmogorov-Smirnov test. Beforehand, large deviation result is stated for the associated statistic. The local asymptotic optimality relative to the test is also studied.},
  file        = {:http\://arxiv.org/pdf/1211.1725v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Chakraborty2014,
  author      = {Anirvan Chakraborty and Probal Chaudhuri},
  date        = {2014-03-02},
  title       = {A Wilcoxon-Mann-Whitney type test for infinite dimensional data},
  eprint      = {1403.0201},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {The Wilcoxon-Mann-Whitney test is a robust competitor of the t-test in the univariate setting. For finite dimensional multivariate data, several extensions of the Wilcoxon-Mann-Whitney test have been shown to have better performance than Hotelling's $T^{2}$ test for many non-Gaussian distributions of the data. In this paper, we study a Wilcoxon-Mann-Whitney type test based on spatial ranks for data in infinite dimensional spaces. We demonstrate the performance of this test using some real and simulated datasets. We also investigate the asymptotic properties of the proposed test and compare the test with a wide range of competing tests.},
  file        = {:http\://arxiv.org/pdf/1403.0201v1:PDF},
  keywords    = {stat.ME},
}

@Article{Zhang2015,
  author      = {Xianyang Zhang},
  date        = {2015-09-28},
  title       = {Testing High Dimensional Mean Under Sparsity},
  eprint      = {1509.08444},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Motivated by the likelihood ratio test under the Gaussian assumption, we develop a maximum sum-of-squares test for conducting hypothesis testing on high dimensional mean vector. The proposed test which incorporates the dependence among the variables is designed to ease the computational burden and to maximize the asymptotic power in the likelihood ratio test. A simulation-based approach is developed to approximate the sampling distribution of the test statistic. The validity of the testing procedure is justified under both the null and alternative hypotheses. We further extend the main results to the two sample problem without the equal covariance assumption. Numerical results suggest that the proposed test can be more powerful than some existing alternatives.},
  file        = {:http\://arxiv.org/pdf/1509.08444v2:PDF},
  keywords    = {stat.ME},
}

@Article{Schepsmeier2013,
  author      = {Ulf Schepsmeier},
  date        = {2013-09-23},
  title       = {Efficient goodness-of-fit tests in multi-dimensional vine copula models},
  eprint      = {1309.5808},
  eprintclass = {stat.CO},
  eprinttype  = {arXiv},
  abstract    = {We introduce a new goodness-of-fit test for regular vine (R-vine) copula models, a flexible class of multivariate copulas based on a pair-copula construction (PCC). The test arises from the information matrix ratio. The corresponding test statistic is derived and its asymptotic normality is proven. The test's power is investigated and compared to 14 other goodness-of-fit tests, adapted from the bivariate copula case, in a high dimensional setting. The extensive simulation study shows the excellent performance with respect to size and power as well as the superiority of the information matrix ratio based test against most other goodness-of-fit tests. The best performing tests are applied to a portfolio of stock indices and their related volatility indices validating different R-vine specifications.},
  file        = {:http\://arxiv.org/pdf/1309.5808v1:PDF},
  keywords    = {stat.CO},
}

@Article{Petr2016,
  author      = {Koldanov Petr and Koldanov Alexander and Kalyagin Valeriy and Pardalos Panos},
  date        = {2016-10-02},
  title       = {Uniformly most powerful unbiased test for conditional independence in Gaussian graphical model},
  eprint      = {1610.00316},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Model selection for Gaussian concentration graph is based on multiple testing of pairwise conditional independence. In practical applications partial correlation tests are widely used. However it is not known whether partial correlation test is uniformly most powerful for pairwise conditional independence testing. This question is answered in the paper. Uniformly most powerful unbiased test of Neymann structure is obtained. It turns out, that this test can be reduced to usual partial correlation test. It implies that partial correlation test is uniformly most powerful unbiased one.},
  file        = {:http\://arxiv.org/pdf/1610.00316v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Iwasaki2017,
  author      = {Atsushi Iwasaki and Ken Umeno},
  date        = {2017-08-28},
  title       = {A new randomness test solving problems of Discrete Fourier Transform Test},
  eprint      = {1708.08218},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Discrete Fourier Transform Test (DFTT), which is a randomness test included in NIST SP800-22, has a problem. It is that theoretical reference distribution of the test statistic has not been derived. In this paper, we propose a new test using variance of power spectrum as the test statistic, whose reference distribution can be theoretically derived. The purpose of DFTT is to detect periodic features and that of the proposed test is the same. We make some experiments and show that the proposed test has stronger detection power than DFTT.},
  file        = {:http\://arxiv.org/pdf/1708.08218v1:PDF},
  keywords    = {stat.ME},
}

@Article{Poth2019,
  author       = {Alexander Poth and Quirin Beck and Andreas Riel},
  date         = {2019-06-07},
  journaltitle = {EuroAsiaSPI${}^2$ 2019, Sep 2019, Edinburgh, United Kingdom},
  title        = {Artificial Intelligence helps making Quality Assurance processes leaner},
  eprint       = {1906.02970},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still works fine in the new release. In many projects, a significant part of these regression tests are not automated and therefore executed manually. Manual tests are expensive and time intensive [1], which is why often only a relevant subset of all possible regression tests are executed in order to safe time and money. Depending on the software process, different approaches can be used to identify the right set of regression tests. The source code file level is a frequent entry point for this identification [2]. Advanced approaches combine different file level methods [3]. To handle black-box tests, methods like [4] or [5] can be used for test case prioritiza-tion. To decide which tests can be skipped, a relevance ranking of the tests in a regression test suite is needed. Based on the relevance a test is in or out of the regression test set for a specific release. This decision is a task of the test manager supported by experts. The task can be time-consuming in case of big (often a 4-to 5-digit number) regression test suites because the selection is specific to each release. Trends are going to continuous prioritization [6], which this work wants to support with the presented ML based approach for black box regression test case prioritization. Any regression test selection is made upon release specific changes. Changes can be new or deleted code based on refactoring or implementation of new features. But also changes on externals systems which are connected by interfaces have to be considered},
  file         = {:http\://arxiv.org/pdf/1906.02970v1:PDF},
  keywords     = {cs.SE},
}

@Article{Zhang2019,
  author      = {Jie M. Zhang and Mark Harman and Lei Ma and Yang Liu},
  date        = {2019-06-19},
  title       = {Machine Learning Testing: Survey, Landscapes and Horizons},
  eprint      = {1906.10742},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {This paper provides a comprehensive survey of Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.},
  file        = {:http\://arxiv.org/pdf/1906.10742v2:PDF},
  keywords    = {cs.LG, cs.AI, cs.SE, stat.ML},
}

@Article{Campbell2020,
  author      = {Harlan Campbell},
  date        = {2020-04-03},
  title       = {Equivalence testing for standardized effect sizes in linear regression},
  eprint      = {2004.01757},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we introduce equivalence testing procedures for standardized effect sizes in a linear regression. We show how to define valid hypotheses and calculate p-values for these tests. Such tests are necessary to confirm the lack of a meaningful association between an outcome and predictors. A simulation study is conducted to examine type I error rates and statistical power. We also compare using equivalence testing as part of a frequentist testing scheme with an alternative Bayesian testing approach. The results indicate that the proposed equivalence test is a potentially useful tool for "testing the null."},
  file        = {:http\://arxiv.org/pdf/2004.01757v4:PDF},
  keywords    = {stat.ME},
}

@Article{Saha2020,
  author      = {Prashanta Saha and Upulee Kanewala},
  date        = {2020-04-18},
  title       = {Improving The Effectiveness of Automatically Generated Test Suites Using Metamorphic Testing},
  doi         = {10.1145/3387940.3392253},
  eprint      = {2004.08518},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Automated test generation has helped to reduce the cost of software testing. However, developing effective test oracles for these automatically generated test inputs is a challenging task. Therefore, most automated test generation tools use trivial oracles that reduce the fault detection effectiveness of these automatically generated test cases. In this work, we provide results of an empirical study showing that utilizing metamorphic relations can increase the fault detection effectiveness of automatically generated test cases.},
  file        = {:http\://arxiv.org/pdf/2004.08518v1:PDF},
  keywords    = {cs.SE},
}

@Article{Aldridge2020,
  author      = {Matthew Aldridge},
  date        = {2020-05-06},
  title       = {Conservative two-stage group testing},
  eprint      = {2005.06617},
  eprintclass = {stat.AP},
  eprinttype  = {arXiv},
  abstract    = {Inspired by applications in testing for COVID-19, we consider a variant of two-stage group testing we call "conservative" two-stage testing, where every item declared to be defective must be definitively confirmed by being tested by itself in the second stage. We study this in the linear regime where the prevalence is fixed while the number of items is large. We study various nonadaptive test designs for the first stage, and derive a new lower bound for the total number of tests required. We find that a first-stage design with constant tests per item and constant items per test due to Broder and Kumar (arXiv:2004.01684) is extremely close to optimal. Simulations back up the theoretical results.},
  file        = {:http\://arxiv.org/pdf/2005.06617v1:PDF},
  keywords    = {stat.AP, cs.IT, math.IT},
}

@Article{Modonato2020,
  author      = {Matteo Modonato},
  date        = {2020-05-19},
  title       = {Combining Dynamic Symbolic Execution, Machine Learning and Search-Based Testing to Automatically Generate Test Cases for Classes},
  eprint      = {2005.09317},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {This article discusses a new technique to automatically generate test cases for object oriented programs. At the state of the art, the problem of generating adequate sets of complete test cases has not been satisfactorily solved yet. There are various techniques to automatically generate test cases (random testing, search-based testing, etc.) but each one has its own weaknesses. This article proposes an approach that distinctively combines dynamic symbolic execution, search-based testing and machine learning, to efficiently generate thorough class-level test suites. The preliminary data obtained carrying out some experiments confirm that we are going in the right direction.},
  file        = {:http\://arxiv.org/pdf/2005.09317v1:PDF},
  keywords    = {cs.SE},
}

@Article{Kodialam2020,
  author      = {Arjun Kodialam},
  date        = {2020-08-24},
  title       = {Efficient Detection Of Infected Individuals using Two Stage Testing},
  eprint      = {2008.10741},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Group testing is an efficient method for testing a large population to detect infected individuals. In this paper, we consider an efficient adaptive two stage group testing scheme. Using a straightforward analysis, we characterize the efficiency of several two stage group testing algorithms. We determine how to pick the parameters of the tests optimally for three schemes with different types of randomization, and show that the performance of two stage testing depends on the type of randomization employed. Seemingly similar randomization procedures lead to different expected number of tests to detect all infected individuals, we determine what kinds of randomization are necessary to achieve optimal performance. We further show that in the optimal setting, our testing scheme is robust to errors in the input parameters.},
  file        = {:http\://arxiv.org/pdf/2008.10741v1:PDF},
  keywords    = {stat.ME, cs.LG},
}

@Article{Pierson2020,
  author      = {Emma Pierson},
  date        = {2020-11-02},
  title       = {Assessing racial inequality in COVID-19 testing with Bayesian threshold tests},
  eprint      = {2011.01179},
  eprintclass = {stat.AP},
  eprinttype  = {arXiv},
  abstract    = {There are racial disparities in the COVID-19 test positivity rate, suggesting that minorities may be under-tested. Here, drawing on the literature on statistically assessing racial disparities in policing, we 1) illuminate a statistical flaw, known as infra-marginality, in using the positivity rate as a metric for assessing racial disparities in under-testing; 2) develop a new type of Bayesian threshold test to measure disparities in COVID-19 testing and 3) apply the test to measure racial disparities in testing thresholds in a real-world COVID-19 dataset.},
  file        = {:http\://arxiv.org/pdf/2011.01179v1:PDF},
  keywords    = {stat.AP},
}

@Article{Schieferdecker2012,
  author       = {Ina Schieferdecker and Juergen Grossmann and Martin Schneider},
  date         = {2012-02-28},
  journaltitle = {EPTCS 80, 2012, pp. 1-12},
  title        = {Model-Based Security Testing},
  doi          = {10.4204/EPTCS.80.1},
  eprint       = {1202.6118},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Security testing aims at validating software system requirements related to security properties like confidentiality, integrity, authentication, authorization, availability, and non-repudiation. Although security testing techniques are available for many years, there has been little approaches that allow for specification of test cases at a higher level of abstraction, for enabling guidance on test identification and specification as well as for automated test generation. Model-based security testing (MBST) is a relatively new field and especially dedicated to the systematic and efficient specification and documentation of security test objectives, security test cases and test suites, as well as to their automated or semi-automated generation. In particular, the combination of security modelling and test generation approaches is still a challenge in research and of high interest for industrial applications. MBST includes e.g. security functional testing, model-based fuzzing, risk- and threat-oriented testing, and the usage of security test patterns. This paper provides a survey on MBST techniques and the related models as well as samples of new methods and tools that are under development in the European ITEA2-project DIAMONDS.},
  file         = {:http\://arxiv.org/pdf/1202.6118v1:PDF},
  keywords     = {cs.SE},
}

@Article{Singh2012,
  author      = {Amandeep Singh and Balwinder Singh},
  date        = {2012-05-09},
  title       = {Microcontroller Based Testing of Digital IP-Core},
  doi         = {10.5121/vlsic.2012.3205},
  eprint      = {1205.1866},
  eprintclass = {cs.AR},
  eprinttype  = {arXiv},
  abstract    = {Testing core based System on Chip is a challenge for the test engineers. To test the complete SOC at one time with maximum fault coverage, test engineers prefer to test each IP-core separately. At speed testing using external testers is more expensive because of gigahertz processor. The purpose of this paper is to develop cost efficient and flexible test methodology for testing digital IP-cores . The prominent feature of the approach is to use microcontroller to test IP-core. The novel feature is that there is no need of test pattern generator and output response analyzer as microcontroller performs the function of both. This approach has various advantages such as at speed testing, low cost, less area overhead and greater flexibility since most of the testing process is based on software.},
  file        = {:http\://arxiv.org/pdf/1205.1866v1:PDF},
  keywords    = {cs.AR},
}

@Article{Ceyhan2012,
  author      = {Elvan Ceyhan},
  date        = {2012-06-08},
  title       = {New Cell-Specific and Overall Tests of Spatial Interaction Based on Nearest Neighbor Contingency Tables},
  eprint      = {1206.1850},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Spatial interaction patterns such as segregation and association can be tested using nearest neighbor contingency tables (NNCTs). We introduce new cell-specific (or pairwise) and overall segregation tests and determine their asymptotic distributions. In particular, we demonstrate that cell-specific tests enjoy asymptotic normality, while overall tests have chi-square distributions asymptotically. We also perform an extensive Monte Carlo simulation study to compare the finite sample performance of the tests in terms of empirical size and power. In addition to the cell-specific tests as post-hoc tests for overall tests, we discuss one-class-versus-rest type of NNCT-tests after an overall test yields significant interaction. We also introduce the concepts of total, strong, and partial segregation/association to label levels of these patterns. We compare these new tests with the existing NNCT-tests in literature with simulations as well and illustrate the NNCT-tests on an ecological data set.},
  file        = {:http\://arxiv.org/pdf/1206.1850v2:PDF},
  keywords    = {stat.ME, stat.AP, 62H11, 62M30, 62G10, 62P12},
}

@Article{Schrammel2013,
  author      = {Peter Schrammel and Tom Melham and Daniel Kroening},
  date        = {2013-06-14},
  title       = {Chaining Test Cases for Reactive System Testing (extended version)},
  eprint      = {1306.3882},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Testing of synchronous reactive systems is challenging because long input sequences are often needed to drive them into a state at which a desired feature can be tested. This is particularly problematic in on-target testing, where a system is tested in its real-life application environment and the time required for resetting is high. This paper presents an approach to discovering a test case chain---a single software execution that covers a group of test goals and minimises overall test execution time. Our technique targets the scenario in which test goals for the requirements are given as safety properties. We give conditions for the existence and minimality of a single test case chain and minimise the number of test chains if a single test chain is infeasible. We report experimental results with a prototype tool for C code generated from Simulink models and compare it to state-of-the-art test suite generators.},
  file        = {:http\://arxiv.org/pdf/1306.3882v2:PDF},
  keywords    = {cs.SE, cs.SY, D.2.4; D.2.5; F.3.1; F.2.2; C.3},
}

@Article{F|ilipov2015,
  author      = {Stefan M. F|ilipov and Ivan D. Gospodinov},
  date        = {2015-11-30},
  title       = {Meeting an absolute test information target with optimal number of test items via Grand Canonical Monte Carlo simulation},
  eprint      = {1511.09216},
  eprintclass = {math.NA},
  eprinttype  = {arXiv},
  abstract    = {This work studies IRT-based Automated Test Assembly (ATA) of multiple test forms (tests) that meet an absolute target information function, i.e. selecting from an item bank only the tests that have information functions that are at a small distance away from the target. The authors introduce the quantities multiplicity of tests and probability of selecting a test with particular number of items N and distance E from the target. A Grand Canonical Monte Carlo test-assembly algorithm is proposed that selects tests according to this probability. The algorithm allows N to vary during the simulation. This work demonstrates that the number of tests that meet the target depends strongly on N. The algorithm is capable of finding tests with small values of E and various values of N depending on the need of the test constructor. Most importantly, it can determine the optimal N for which a maximal number of tests with certain specified small E exists.},
  file        = {:http\://arxiv.org/pdf/1511.09216v1:PDF},
  keywords    = {math.NA},
}

@Article{Walkinshaw2016,
  author      = {Neil Walkinshaw and Gordon Fraser},
  date        = {2016-08-10},
  title       = {Uncertainty-Driven Black-Box Test Data Generation},
  eprint      = {1608.03181},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {We can never be certain that a software system is correct simply by testing it, but with every additional successful test we become less uncertain about its correctness. In absence of source code or elaborate specifications and models, tests are usually generated or chosen randomly. However, rather than randomly choosing tests, it would be preferable to choose those tests that decrease our uncertainty about correctness the most. In order to guide test generation, we apply what is referred to in Machine Learning as "Query Strategy Framework": We infer a behavioural model of the system under test and select those tests which the inferred model is "least certain" about. Running these tests on the system under test thus directly targets those parts about which tests so far have failed to inform the model. We provide an implementation that uses a genetic programming engine for model inference in order to enable an uncertainty sampling technique known as "query by committee", and evaluate it on eight subject systems from the Apache Commons Math framework and JodaTime. The results indicate that test generation using uncertainty sampling outperforms conventional and Adaptive Random Testing.},
  file        = {:http\://arxiv.org/pdf/1608.03181v1:PDF},
  keywords    = {cs.SE},
}

@Article{Niedermayr2016,
  author       = {Rainer Niedermayr and Elmar Juergens and Stefan Wagner},
  date         = {2016-11-22},
  journaltitle = {Proceedings of the International Workshop on Continuous Software Evolution and Delivery (CSED '16). ACM, 2016},
  title        = {Will My Tests Tell Me If I Break This Code?},
  doi          = {10.1145/2896941.2896944},
  eprint       = {1611.07163},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Automated tests play an important role in software evolution because they can rapidly detect faults introduced during changes. In practice, code-coverage metrics are often used as criteria to evaluate the effectiveness of test suites with focus on regression faults. However, code coverage only expresses which portion of a system has been executed by tests, but not how effective the tests actually are in detecting regression faults. Our goal was to evaluate the validity of code coverage as a measure for test effectiveness. To do so, we conducted an empirical study in which we applied an extreme mutation testing approach to analyze the tests of open-source projects written in Java. We assessed the ratio of pseudo-tested methods (those tested in a way such that faults would not be detected) to all covered methods and judged their impact on the software project. The results show that the ratio of pseudo-tested methods is acceptable for unit tests but not for system tests (that execute large portions of the whole system). Therefore, we conclude that the coverage metric is only a valid effectiveness indicator for unit tests.},
  file         = {:http\://arxiv.org/pdf/1611.07163v1:PDF},
  keywords     = {cs.SE},
}

@Article{Wang2014,
  author      = {Chao Wang and Qing Zhao and Chen-Nee Chuah},
  date        = {2014-07-08},
  title       = {Optimal Nested Test Plan for Combinatorial Quantitative Group Testing},
  eprint      = {1407.2283},
  eprintclass = {cs.IT},
  eprinttype  = {arXiv},
  abstract    = {We consider the quantitative group testing problem where the objective is to identify defective items in a given population based on results of tests performed on subsets of the population. Under the quantitative group testing model, the result of each test reveals the number of defective items in the tested group. The minimum number of tests achievable by nested test plans was established by Aigner and Schughart in 1985 within a minimax framework. The optimal nested test plan offering this performance, however, was not obtained. In this work, we establish the optimal nested test plan in closed form. This optimal nested test plan is also order optimal among all test plans as the population size approaches infinity. Using heavy-hitter detection as a case study, we show via simulation examples orders of magnitude improvement of the group testing approach over two prevailing sampling-based approaches in detection accuracy and counter consumption. Other applications include anomaly detection and wideband spectrum sensing in cognitive radio systems.},
  file        = {:http\://arxiv.org/pdf/1407.2283v5:PDF},
  keywords    = {cs.IT, math.IT, math.OC},
}

@Article{Guo2016,
  author      = {Jia Guo and Bu Zhou and Jin-Ting Zhang},
  date        = {2016-09-14},
  title       = {A Supremum-Norm Based Test for the Equality of Several Covariance Functions},
  eprint      = {1609.04232},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {In this paper, we propose a new test for the equality of several covariance functions for functional data. Its test statistic is taken as the supremum value of the sum of the squared differences between the estimated individual covariance functions and the pooled sample covariance function, hoping to obtain a more powerful test than some existing tests for the same testing problem. The asymptotic random expression of this test statistic under the null hypothesis is obtained. To approximate the null distribution of the proposed test statistic, we describe a parametric bootstrap method and a non-parametric bootstrap method. The asymptotic random expression of the proposed test is also studied under a local alternative and it is shown that the proposed test is root-$n$ consistent. Intensive simulation studies are conducted to demonstrate the finite sample performance of the proposed test and it turns out that the proposed test is indeed more powerful than some existing tests when functional data are highly correlated. The proposed test is illustrated with three real data examples.},
  file        = {:http\://arxiv.org/pdf/1609.04232v1:PDF},
  keywords    = {stat.ME, stat.AP, stat.CO},
}

@Article{Dwarakanath2018,
  author      = {Anurag Dwarakanath and Aruna Jankiti},
  date        = {2018-09-22},
  title       = {Minimum Number of Test Paths for Prime Path and other Structural Coverage Criteria},
  eprint      = {1809.08446},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {The software system under test can be modeled as a graph comprising of a set of vertices, (V) and a set of edges, (E). Test Cases are Test Paths over the graph meeting a particular test criterion. In this paper, we present a method to achieve the minimum number of Test Paths needed to cover different structural coverage criteria. Our method can accommodate Prime Path, Edge-Pair, Simple & Complete Round Trip, Edge and Node coverage criteria. Our method obtains the optimal solution by transforming the graph into a flow graph and solving the minimum flow problem. We present an algorithm for the minimum flow problem that matches the best known solution complexity of $O(|V| |E|)$. Our method is evaluated through two sets of tests. In the first, we test against graphs representing actual software. In the second test, we create random graphs of varying complexity. In each test we measure the number of Test Paths, the length of Test Paths, the lower bound on minimum number of Test Paths and the execution time.},
  file        = {:http\://arxiv.org/pdf/1809.08446v1:PDF},
  keywords    = {cs.SE},
}

@Article{Roach2018,
  author      = {Jeffrey Roach and William Valdar},
  date        = {2018-08-30},
  title       = {Permutation tests of non-exchangeable null models},
  eprint      = {1808.10483},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {Generalizations to the permutation test are introduced to allow for situations in which the null model is not exchangeable. It is shown that the generalized permutation tests are exact, and a partial converse: that any test function that is exact on all probability densities coincides with a generalized permutation test on a particular region, is established. A most powerful generalized permutation test is derived in closed form. Approximations to the most powerful generalized permutation test are proposed to reduce the computational burden required to compute the complete test. In particular, an explicit form for the approximate test is derived in terms of a multinomial Bernstein polynomial approximation, and its convergence to the most powerful generalized permutation test is demonstrated. In the case where the determination of p-values is of greater interest than testing of hypotheses, two approaches to estimation of significance are analyzed. Bounds on the deviation from significance of the exact most powerful test are given in terms of sample size. For both estimators, as sample size approaches infinity, the estimator converges to the significance of the most powerful generalized permutation test under mild conditions. Applications of generalized permutation testing to linear mixed models are provided.},
  file        = {:http\://arxiv.org/pdf/1808.10483v1:PDF},
  keywords    = {stat.ME},
}

@Article{Spieker2019,
  author       = {Helge Spieker and Arnaud Gotlieb},
  date         = {2019-10-01},
  journaltitle = {Journal of Systems and Software (JSS) Vol. 165 (2020) 110574},
  title        = {Adaptive Metamorphic Testing with Contextual Bandits},
  doi          = {10.1016/j.jss.2020.110574},
  eprint       = {1910.00262},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {Metamorphic Testing is a software testing paradigm which aims at using necessary properties of a system-under-test, called metamorphic relations, to either check its expected outputs, or to generate new test cases. Metamorphic Testing has been successful to test programs for which a full oracle is not available or to test programs for which there are uncertainties on expected outputs such as learning systems. In this article, we propose Adaptive Metamorphic Testing as a generalization of a simple yet powerful reinforcement learning technique, namely contextual bandits, to select one of the multiple metamorphic relations available for a program. By using contextual bandits, Adaptive Metamorphic Testing learns which metamorphic relations are likely to transform a source test case, such that it has higher chance to discover faults. We present experimental results over two major case studies in machine learning, namely image classification and object detection, and identify weaknesses and robustness boundaries. Adaptive Metamorphic Testing efficiently identifies weaknesses of the tested systems in context of the source test case.},
  file         = {:http\://arxiv.org/pdf/1910.00262v3:PDF},
  keywords     = {cs.SE},
}

@Article{Felderer2019,
  author      = {Michael Felderer and Ina Schieferdecker},
  date        = {2019-12-24},
  title       = {A taxonomy of risk-based testing},
  eprint      = {1912.11519},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Software testing has often to be done under severe pressure due to limited resources and a challenging time schedule facing the demand to assure the fulfillment of the software requirements. In addition, testing should unveil those software defects that harm the mission-critical functions of the software. Risk-based testing uses risk (re-)assessments to steer all phases of the test process in order to optimize testing efforts and limit risks of the software-based system. Due to its importance and high practical relevance several risk-based testing approaches were proposed in academia and industry. This paper presents a taxonomy of risk-based testing providing a framework to understand, categorize, assess, and compare risk-based testing approaches to support their selection and tailoring for specific purposes. The taxonomy is aligned with the consideration of risks in all phases of the test process and consists of the top-level classes risk drivers, risk assessment, and risk-based test process. The taxonomy of risk-based testing has been developed by analyzing the work presented in available publications on risk-based testing. Afterwards, it has been applied to the work on risk-based testing presented in this special section of the International Journal on Software Tools for Technology Transfer.},
  file        = {:http\://arxiv.org/pdf/1912.11519v1:PDF},
  keywords    = {cs.SE},
}

@Article{Ziegler2020,
  author      = {Gabriel Ziegler},
  date        = {2020-12-21},
  title       = {Binary Classification Tests, Imperfect Standards, and Ambiguous Information},
  eprint      = {2012.11215},
  eprintclass = {econ.EM},
  eprinttype  = {arXiv},
  abstract    = {New binary classification tests are often evaluated relative to a pre-established test. For example, rapid Antigen tests for the detection of SARS-CoV-2 are assessed relative to more established PCR tests. In this paper, I argue that the new test can be described as producing ambiguous information when the pre-established is imperfect. This allows for a phenomenon called dilation -- an extreme form of non-informativeness. As an example, I present hypothetical test data satisfying the WHO's minimum quality requirement for rapid Antigen tests which leads to dilation. The ambiguity in the information arises from a missing data problem due to imperfection of the established test: the joint distribution of true infection and test results is not observed. Using results from Copula theory, I construct the (usually non-singleton) set of all these possible joint distributions, which allows me to assess the new test's informativeness. This analysis leads to a simple sufficient condition to make sure that a new test is not a dilation. I illustrate my approach with applications to data from three COVID-19 related tests. Two rapid Antigen tests satisfy my sufficient condition easily and are therefore informative. However, less accurate procedures, like chest CT scans, may exhibit dilation.},
  file        = {:http\://arxiv.org/pdf/2012.11215v3:PDF},
  keywords    = {econ.EM},
}

@Article{Feldt2015,
  author      = {Robert Feldt and Simon Poulding and David Clark and Shin Yoo},
  date        = {2015-06-10},
  title       = {Test Set Diameter: Quantifying the Diversity of Sets of Test Cases},
  eprint      = {1506.03482},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {A common and natural intuition among software testers is that test cases need to differ if a software system is to be tested properly and its quality ensured. Consequently, much research has gone into formulating distance measures for how test cases, their inputs and/or their outputs differ. However, common to these proposals is that they are data type specific and/or calculate the diversity only between pairs of test inputs, traces or outputs. We propose a new metric to measure the diversity of sets of tests: the test set diameter (TSDm). It extends our earlier, pairwise test diversity metrics based on recent advances in information theory regarding the calculation of the normalized compression distance (NCD) for multisets. An advantage is that TSDm can be applied regardless of data type and on any test-related information, not only the test inputs. A downside is the increased computational time compared to competing approaches. Our experiments on four different systems show that the test set diameter can help select test sets with higher structural and fault coverage than random selection even when only applied to test inputs. This can enable early test design and selection, prior to even having a software system to test, and complement other types of test automation and analysis. We argue that this quantification of test set diversity creates a number of opportunities to better understand software quality and provides practical ways to increase it.},
  file        = {:http\://arxiv.org/pdf/1506.03482v1:PDF},
  keywords    = {cs.SE},
}

@Article{Gaboardi2016,
  author      = {Marco Gaboardi and Hyun woo Lim and Ryan Rogers and Salil Vadhan},
  date        = {2016-02-07},
  title       = {Differentially Private Chi-Squared Hypothesis Testing: Goodness of Fit and Independence Testing},
  eprint      = {1602.03090},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Hypothesis testing is a useful statistical tool in determining whether a given model should be rejected based on a sample from the population. Sample data may contain sensitive information about individuals, such as medical information. Thus it is important to design statistical tests that guarantee the privacy of subjects in the data. In this work, we study hypothesis testing subject to differential privacy, specifically chi-squared tests for goodness of fit for multinomial data and independence between two categorical variables. We propose new tests for goodness of fit and independence testing that like the classical versions can be used to determine whether a given model should be rejected or not, and that additionally can ensure differential privacy. We give both Monte Carlo based hypothesis tests as well as hypothesis tests that more closely follow the classical chi-squared goodness of fit test and the Pearson chi-squared test for independence. Crucially, our tests account for the distribution of the noise that is injected to ensure privacy in determining significance. We show that these tests can be used to achieve desired significance levels, in sharp contrast to direct applications of classical tests to differentially private contingency tables which can result in wildly varying significance levels. Moreover, we study the statistical power of these tests. We empirically show that to achieve the same level of power as the classical non-private tests our new tests need only a relatively modest increase in sample size.},
  file        = {:http\://arxiv.org/pdf/1602.03090v2:PDF},
  keywords    = {math.ST, cs.CR, stat.TH},
}

@Article{Gontscharuk2016,
  author       = {Veronika Gontscharuk and Sandra Landwehr and Helmut Finner},
  date         = {2016-03-17},
  journaltitle = {Bernoulli 2016, Vol. 22, No. 3, 1331-1363},
  title        = {Goodness of fit tests in terms of local levels with special emphasis on higher criticism tests},
  doi          = {10.3150/14-BEJ694},
  eprint       = {1603.05461},
  eprintclass  = {math.ST},
  eprinttype   = {arXiv},
  abstract     = {Instead of defining goodness of fit (GOF) tests in terms of their test statistics, we present an alternative method by introducing the concept of local levels, which indicate high or low local sensitivity of a test. Local levels can act as a starting point for the construction of new GOF tests. We study the behavior of local levels when applied to some well-known GOF tests such as Kolmogorov-Smirnov (KS) tests, higher criticism (HC) tests and tests based on phi-divergences. The main focus is on a rigorous characterization of the asymptotic behavior of local levels of the original HC tests which leads to several further asymptotic results for local levels of other GOF tests including GOF tests with equal local levels. While local levels of KS tests, which are related to the central range, are asymptotically strictly larger than zero, all local levels of HC tests converge to zero as the sample size increases. Consequently, there exists no asymptotic level $\alpha$ GOF test such that all local levels are asymptotically bounded away from zero. Finally, by means of numerical computations we compare classical KS and HC tests to a GOF test with equal local levels.},
  file         = {:http\://arxiv.org/pdf/1603.05461v1:PDF},
  keywords     = {math.ST, stat.TH},
}

@Article{Coppola2017,
  author      = {Riccardo Coppola and Maurizio Morisio and Marco Torchiano},
  date        = {2017-11-09},
  title       = {Scripted GUI Testing of Android Apps: A Study on Diffusion, Evolution and Fragility},
  eprint      = {1711.03565},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Background. Evidence suggests that mobile applications are not thoroughly tested as their desktop counterparts. In particular GUI testing is generally limited. Like web-based applications, mobile apps suffer from GUI test fragility, i.e. GUI test classes failing due to minor modifications in the GUI, without the application functionalities being altered. Aims. The objective of our study is to examine the diffusion of GUI testing on Android, and the amount of changes required to keep test classes up to date, and in particular the changes due to GUI test fragility. We define metrics to characterize the modifications and evolution of test classes and test methods, and proxies to estimate fragility-induced changes. Method. To perform our experiments, we selected six widely used open-source tools for scripted GUI testing of mobile applications previously described in the literature. We have mined the repositories on GitHub that used those tools, and computed our set of metrics. Results. We found that none of the considered GUI testing frameworks achieved a major diffusion among the open-source Android projects available on GitHub. For projects with GUI tests, we found that test suites have to be modified often, specifically 5\%-10\% of developers' modified LOCs belong to tests, and that a relevant portion (60\% on average) of such modifications are induced by fragility. Conclusions. Fragility of GUI test classes constitute a relevant concern, possibly being an obstacle for developers to adopt automated scripted GUI tests. This first evaluation and measure of fragility of Android scripted GUI testing can constitute a benchmark for developers, and the basis for the definition of a taxonomy of fragility causes, and actionable guidelines to mitigate the issue.},
  file        = {:http\://arxiv.org/pdf/1711.03565v1:PDF},
  keywords    = {cs.SE},
}

@Article{Chen2020a,
  author      = {T. Y. Chen and S. C. Cheung and S. M. Yiu},
  date        = {2020-02-28},
  title       = {Metamorphic Testing: A New Approach for Generating Next Test Cases},
  eprint      = {2002.12543},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {In software testing, a set of test cases is constructed according to some predefined selection criteria. The software is then examined against these test cases. Three interesting observations have been made on the current artifacts of software testing. Firstly, an error-revealing test case is considered useful while a successful test case which does not reveal software errors is usually not further investigated. Whether these successful test cases still contain useful information for revealing software errors has not been properly studied. Secondly, no matter how extensive the testing has been conducted in the development phase, errors may still exist in the software [5]. These errors, if left undetected, may eventually cause damage to the production system. The study of techniques for uncovering software errors in the production phase is seldom addressed in the literature. Thirdly, as indicated by Weyuker in [6], the availability of test oracles is pragmatically unattainable in most situations. However, the availability of test oracles is generally assumed in conventional software testing techniques. In this paper, we propose a novel test case selection technique that derives new test cases from the successful ones. The selection aims at revealing software errors that are possibly left undetected in successful test cases which may be generated using some existing strategies. As such, the proposed technique augments the effectiveness of existing test selection strategies. The technique also helps uncover software errors in the production phase and can be used in the absence of test oracles.},
  file        = {:http\://arxiv.org/pdf/2002.12543v1:PDF},
  keywords    = {cs.SE},
}

@Article{OliveiraNeto2020,
  author      = {Francisco Gomes de Oliveira Neto and Felix Dobslaw and Robert Feldt},
  date        = {2020-10-19},
  title       = {Using mutation testing to measure behavioural test diversity},
  doi         = {10.1109/ICSTW50294.2020.00051},
  eprint      = {2010.09144},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Diversity has been proposed as a key criterion to improve testing effectiveness and efficiency.It can be used to optimise large test repositories but also to visualise test maintenance issues and raise practitioners' awareness about waste in test artefacts and processes. Even though these diversity-based testing techniques aim to exercise diverse behavior in the system under test (SUT), the diversity has mainly been measured on and between artefacts (e.g., inputs, outputs or test scripts). Here, we introduce a family of measures to capture behavioural diversity (b-div) of test cases by comparing their executions and failure outcomes. Using failure information to capture the SUT behaviour has been shown to improve effectiveness of history-based test prioritisation approaches. However, history-based techniques require reliable test execution logs which are often not available or can be difficult to obtain due to flaky tests, scarcity of test executions, etc. To be generally applicable we instead propose to use mutation testing to measure behavioral diversity by running the set of test cases on various mutated versions of the SUT. Concretely, we propose two specific b-div measures (based on accuracy and Matthew's correlation coefficient, respectively) and compare them with artefact-based diversity (a-div) for prioritising the test suites of 6 different open-source projects. Our results show that our b-div measures outperform a-div and random selection in all of the studied projects. The improvement is substantial with an average increase in average percentage of faults detected (APFD) of between 19% to 31% depending on the size of the subset of prioritised tests.},
  file        = {:http\://arxiv.org/pdf/2010.09144v1:PDF},
  keywords    = {cs.SE},
}

@Article{Martin2020,
  author      = {Nirian MartÃ­n},
  date        = {2020-12-28},
  title       = {Rao's Score Tests on Correlation Matrices},
  eprint      = {2012.14238},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Even though the Rao's score tests are classical tests, such as the likelihood ratio tests, their application has been avoided until now in a multivariate framework, in particular high-dimensional setting. We consider they could play an important role for testing high-dimensional data, but currently the classical Rao's score tests for an arbitrary but fixed dimension remain being still not very well-known for tests on correlation matrices of multivariate normal distributions. In this paper, we illustrate how to create Rao's score tests, focussed on testing correlation matrices, showing their asymptotic distribution. Based on Basu et al. (2021), we do not only develop the classical Rao's score tests, but also their robust version, Rao's $\beta$-score tests. Despite of tedious calculations, their strength is the final simple expression, which is valid for any arbitrary but fixed dimension. In addition, we provide basic formulas for creating easily other tests, either for other variants of correlation tests or for location or variability parameters. We perform a simulation study with high-dimensional data and the results are compared to those of the likelihood ratio test with a variety of distributions, either pure and contaminated. The study shows that the classical Rao's score test for correlation matrices seems to work properly not only under multivariate normality but also under other multivariate distributions. Under perturbed distributions, the Rao's $\beta$-score tests outperform any classical test.},
  file        = {:http\://arxiv.org/pdf/2012.14238v2:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Madeja2021,
  author      = {Matej Madeja and Jaroslav PorubÃ¤n and Michaela BaÄ�Ã­kovÃ¡ and MatÃºÅ¡ SulÃ­r and JÃ¡n JuhÃ¡r and Sergej Chodarev and Filip GurbÃ¡Ä¾},
  date        = {2021-02-23},
  title       = {Automating Test Case Identification in Open Source Projects on GitHub},
  eprint      = {2102.11678},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Software testing is one of the very important Quality Assurance (QA) components. A lot of researchers deal with the testing process in terms of tester motivation and how tests should or should not be written. However, it is not known from the recommendations how the tests are actually written in real projects. In this paper the following was investigated: (i) the denotation of the test word in different natural languages; (ii) whether the test word correlates with the presence of test cases; and (iii) what testing frameworks are mostly used. The analysis was performed on 38 GitHub open source repositories thoroughly selected from the set of 4.3M GitHub projects. We analyzed 20,340 test cases in 803 classes manually and 170k classes using an automated approach. The results show that: (i) there exists weak correlation (r = 0.655) between the word test and test cases presence in a class; (ii) the proposed algorithm using static file analysis correctly detected 95\% of test cases; (iii) 15\% of the analyzed classes used main() function whose represent regular Java programs that test the production code without using any third-party framework. The identification of such tests is very low due to implementation diversity. The results may be leveraged to more quickly identify and locate test cases in a repository, to understand practices in customized testing solutions and to mine tests to improve program comprehension in the future.},
  file        = {:http\://arxiv.org/pdf/2102.11678v1:PDF},
  keywords    = {cs.SE, 68-04, D.2.5; D.2.3},
}

@Article{Langovoy2007,
  author       = {Mikhail Langovoy},
  date         = {2007-07-05},
  journaltitle = {Inverse Problems 24 (2008) 025028 17pp},
  title        = {Data-driven efficient score tests for deconvolution problems},
  doi          = {10.1088/0266-5611/24/2/025028},
  eprint       = {0707.0861},
  eprintclass  = {math.ST},
  eprinttype   = {arXiv},
  abstract     = {We consider testing statistical hypotheses about densities of signals in deconvolution models. A new approach to this problem is proposed. We constructed score tests for the deconvolution with the known noise density and efficient score tests for the case of unknown density. The tests are incorporated with model selection rules to choose reasonable model dimensions automatically by the data. Consistency of the tests is proved.},
  file         = {:http\://arxiv.org/pdf/0707.0861v1:PDF},
  keywords     = {math.ST, stat.AP, stat.TH},
}

@Article{Kadry2011,
  author       = {Seifedine Kadry},
  date         = {2011-11-23},
  journaltitle = {International Journal of Security and Its Applications Vol. 5 No. 3, July, 2011},
  title        = {A New Proposed Technique to Improve Software Regression Testing Cost},
  eprint       = {1111.5640},
  eprintclass  = {cs.SE},
  eprinttype   = {arXiv},
  abstract     = {In this article, we describe the regression test process to test and verify the changes made on software. A developed technique use the automation test based on decision tree and test selection process in order to reduce the testing cost is given. The developed technique is applied to a practical case and the result show its improvement.},
  file         = {:http\://arxiv.org/pdf/1111.5640v1:PDF},
  keywords     = {cs.SE},
}

@Article{Koldanov2016,
  author      = {Petr A. Koldanov and Alexander P. Koldanov and Panos Pardalos},
  date        = {2016-04-23},
  title       = {Multiple testing with optimal individual tests in Gaussian graphical model selection},
  eprint      = {1604.06874},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {Gaussian Graphical Model selection problem is considered. Concentration graph is identified by multiple decision procedure based on individual tests. Optimal unbiased individual tests are constructed. It is shown that optimal tests are equivalent to sample partial correlation tests. Associated multiple decision procedure is compared with standard procedure.},
  file        = {:http\://arxiv.org/pdf/1604.06874v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Neumann2019,
  author      = {AndrÃ© Neumann and Thorsten Dickhaus},
  date        = {2019-03-27},
  title       = {Non-parametric Archimedean generator estimation with implications for multiple testing},
  eprint      = {1903.11371},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {In multiple testing, the family-wise error rate can be bounded under some conditions by the copula of the test statistics. Assuming that this copula is Archimedean, we consider two non-parametric Archimedean generator estimators. More specifically, we use the non-parametric estimator from Genest et al. (2011) and a slight modification thereof. In simulations, we compare the resulting multiple tests with the Bonferroni test and the multiple test derived from the true generator as baselines.},
  file        = {:http\://arxiv.org/pdf/1903.11371v1:PDF},
  keywords    = {stat.ME, 62J15},
}

@Article{Pav2019,
  author      = {Steven Pav},
  date        = {2019-11-11},
  title       = {A post hoc test on the Sharpe ratio},
  eprint      = {1911.04090},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {We describe a post hoc test for the Sharpe ratio, analogous to Tukey's test for pairwise equality of means. The test can be applied after rejection of the hypothesis that all population Signal-Noise ratios are equal. The test is applicable under a simple correlation structure among asset returns. Simulations indicate the test maintains nominal type I rate under a wide range of conditions and is moderately powerful under reasonable alternatives.},
  file        = {:http\://arxiv.org/pdf/1911.04090v1:PDF},
  keywords    = {stat.ME, q-fin.PM, 91G70, G.3},
}

@Article{Haq2019,
  author      = {Fitash Ul Haq and Donghwan Shin and Shiva Nejati and Lionel Briand},
  date        = {2019-11-28},
  title       = {Comparing Offline and Online Testing of Deep Neural Networks: An Autonomous Car Case Study},
  eprint      = {1912.00805},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  abstract    = {There is a growing body of research on developing testing techniques for Deep Neural Networks (DNN). We distinguish two general modes of testing for DNNs: Offline testing where DNNs are tested as individual units based on test datasets obtained independently from the DNNs under test, and online testing where DNNs are embedded into a specific application and tested in a close-loop mode in interaction with the application environment. In addition, we identify two sources for generating test datasets for DNNs: Datasets obtained from real-life and datasets generated by simulators. While offline testing can be used with datasets obtained from either sources, online testing is largely confined to using simulators since online testing within real-life applications can be time-consuming, expensive and dangerous. In this paper, we study the following two important questions aiming to compare test datasets and testing modes for DNNs: First, can we use simulator-generated data as a reliable substitute to real-world data for the purpose of DNN testing? Second, how do online and offline testing results differ and complement each other? Though these questions are generally relevant to all autonomous systems, we study them in the context of automated driving systems where, as study subjects, we use DNNs automating end-to-end control of cars' steering actuators. Our results show that simulator-generated datasets are able to yield DNN prediction errors that are similar to those obtained by testing DNNs with real-life datasets. Further, offline testing is more optimistic than online testing as many safety violations identified by online testing could not be identified by offline testing, while large prediction errors generated by offline testing always led to severe safety violations detectable by online testing.},
  file        = {:http\://arxiv.org/pdf/1912.00805v1:PDF},
  keywords    = {cs.LG, cs.SE},
}

@Article{Strandberg2020,
  author      = {Per Erik Strandberg and Thomas J Ostrand and Elaine J Weyuker and Wasif Afzal and Daniel Sundmark},
  date        = {2020-05-14},
  title       = {Intermittently Failing Tests in the Embedded Systems Domain},
  eprint      = {2005.06826},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Software testing is sometimes plagued with intermittently failing tests and finding the root causes of such failing tests is often difficult. This problem has been widely studied at the unit testing level for open source software, but there has been far less investigation at the system test level, particularly the testing of industrial embedded systems. This paper describes our investigation of the root causes of intermittently failing tests in the embedded systems domain, with the goal of better understanding, explaining and categorizing the underlying faults. The subject of our investigation is a currently-running industrial embedded system, along with the system level testing that was performed. We devised and used a novel metric for classifying test cases as intermittent. From more than a half million test verdicts, we identified intermittently and consistently failing tests, and identified their root causes using multiple sources. We found that about 1-3% of all test cases were intermittently failing. From analysis of the case study results and related work, we identified nine factors associated with test case intermittence. We found that a fix for a consistently failing test typically removed a larger number of failures detected by other tests than a fix for an intermittent test. We also found that more effort was usually needed to identify fixes for intermittent tests than for consistent tests. An overlap between root causes leading to intermittent and consistent tests was identified. Many root causes of intermittence are the same in industrial embedded systems and open source software. However, when comparing unit testing to system level testing, especially for embedded systems, we observed that the test environment itself is often the cause of intermittence.},
  file        = {:http\://arxiv.org/pdf/2005.06826v1:PDF},
  keywords    = {cs.SE},
}

@Article{Qayed2020,
  author      = {Abdullah Qayed and Dong Han},
  date        = {2020-08-21},
  title       = {Homogeneity Test of Several High-Dimensional Covariance Matrices for Stationary Processes under Non-normality},
  eprint      = {2008.09259},
  eprintclass = {math.ST},
  eprinttype  = {arXiv},
  abstract    = {This article presents a homogeneity test for testing the equality of several high-dimensional covariance matrices for stationary processes with ignoring the assumption of normality. We give the asymptotic distribution of the proposed test. The simulation illustrates that the proposed test has perfect performance. Moreover, the power of the test can approach any high probability uniformly on a set of covariance matrices.},
  file        = {:http\://arxiv.org/pdf/2008.09259v1:PDF},
  keywords    = {math.ST, stat.TH},
}

@Article{Xuan2015,
  author      = {Jifeng Xuan and Benoit Cornu and Matias Martinez and Benoit Baudry and Lionel Seinturier and Martin Monperrus},
  date        = {2015-06-05},
  title       = {Dynamic Analysis can be Improved with Automatic Test Suite Refactoring},
  eprint      = {1506.01883},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Context: Developers design test suites to automatically verify that software meets its expected behaviors. Many dynamic analysis techniques are performed on the exploitation of execution traces from test cases. However, in practice, there is only one trace that results from the execution of one manually-written test case. Objective: In this paper, we propose a new technique of test suite refactoring, called B-Refactoring. The idea behind B-Refactoring is to split a test case into small test fragments, which cover a simpler part of the control flow to provide better support for dynamic analysis. Method: For a given dynamic analysis technique, our test suite refactoring approach monitors the execution of test cases and identifies small test cases without loss of the test ability. We apply B-Refactoring to assist two existing analysis tasks: automatic repair of if-statements bugs and automatic analysis of exception contracts. Results: Experimental results show that test suite refactoring can effectively simplify the execution traces of the test suite. Three real-world bugs that could previously not be fixed with the original test suite are fixed after applying B-Refactoring; meanwhile, exception contracts are better verified via applying B-Refactoring to original test suites. Conclusions: We conclude that applying B-Refactoring can effectively improve the purity of test cases. Existing dynamic analysis tasks can be enhanced by test suite refactoring.},
  file        = {:http\://arxiv.org/pdf/1506.01883v1:PDF},
  keywords    = {cs.SE},
}

@Article{Srinivasan2018,
  author      = {Madhusudan Srinivasan and Morteza Pourreza Shahri and Upulee Kanewala and Indika Kahanda},
  date        = {2018-02-20},
  title       = {Quality Assurance of Bioinformatics Software: A Case Study of Testing a Biomedical Text Processing Tool Using Metamorphic Testing},
  eprint      = {1802.07354},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Bioinformatics software plays a very important role in making critical decisions within many areas including medicine and health care. However, most of the research is directed towards developing tools, and little time and effort is spent on testing the software to assure its quality. In testing, a test oracle is used to determine whether a test is passed or failed during testing, and unfortunately, for much of bioinformatics software, the exact expected outcomes are not well defined. Thus, the main challenge associated with conducting systematic testing on bioinformatics software is the oracle problem. Metamorphic testing (MT) is a technique used to test programs that face the oracle problem. MT uses metamorphic relations (MRs) to determine whether a test has passed or failed and specifies how the output should change according to a specific change made to the input. In this work, we use MT to test LingPipe, a tool for processing text using computational linguistics, often used in bioinformatics for bio-entity recognition from biomedical literature. First, we identify a set of MRs for testing any bio-entity recognition program. Then we develop a set of test cases that can be used to test LingPipe's bio-entity recognition functionality using these MRs. To evaluate the effectiveness of this testing process, we automatically generate a set of faulty versions of LingPipe. According to our analysis of the experimental results, we observe that our MRs can detect the majority of these faulty versions, which shows the utility of this testing technique for quality assurance of bioinformatics software.},
  file        = {:http\://arxiv.org/pdf/1802.07354v1:PDF},
  keywords    = {cs.SE},
}

@Article{Ahmad2019,
  author      = {Azeem Ahmad and Ola Leifler and Kristian Sandahl},
  date        = {2019-06-03},
  title       = {Empirical Analysis of Factors and their Effect on Test Flakiness - Practitioners' Perceptions},
  eprint      = {1906.00673},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Developers always wish to ensure that their latest changes to the code base do not break existing functionality. If test cases fail, they expect these failures to be connected to the submitted changes. Unfortunately, a flaky test can be the reason for a test failure. Developers spend time to relate possible test failures to the submitted changes only to find out that the cause for these failures is test flakiness. The dilemma of an identification of the real failures or flaky test failures affects developers' perceptions about what is test flakiness. Prior research on test flakiness has been limited to test smells and tools to detect test flakiness. In this paper, we have conducted a multiple case study with four different industries in Scandinavia to understand practitioners' perceptions about test flakiness and how this varies between industries. We observed that there are little differences in how the practitioners perceive test flakiness. We identified 23 factors that are perceived to affect test flakiness. These perceived factors are categorized as 1) Software test quality, 2) Software Quality, 3) Actual Flaky test and 4) Company-specific factors. We have studied the nature of effects such as whether factors increase, decrease or affect the ability to detect test flakiness. We validated our findings with different participants of the 4 companies to avoid biases. The average agreement rate of the identified factors and their effects are 86% and 86% respectively, among participants.},
  file        = {:http\://arxiv.org/pdf/1906.00673v1:PDF},
  keywords    = {cs.SE},
}

@Article{Gonzalez2020,
  author      = {Danielle Gonzalez and Michael Rath and Mehdi Mirakhorli},
  date        = {2020-06-25},
  title       = {Did You Remember to Test Your Tokens?},
  doi         = {10.1145/3379597.3387471},
  eprint      = {2006.14553},
  eprintclass = {cs.SE},
  eprinttype  = {arXiv},
  abstract    = {Authentication is a critical security feature for confirming the identity of a system's users, typically implemented with help from frameworks like Spring Security. It is a complex feature which should be robustly tested at all stages of development. Unit testing is an effective technique for fine-grained verification of feature behaviors that is not widely-used to test authentication. Part of the problem is that resources to help developers unit test security features are limited. Most security testing guides recommend test cases in a "black box" or penetration testing perspective. These resources are not easily applicable to developers writing new unit tests, or who want a security-focused perspective on coverage. In this paper, we address these issues by applying a grounded theory-based approach to identify common (unit) test cases for token authentication through analysis of 481 JUnit tests exercising Spring Security-based authentication implementations from 53 open source Java projects. The outcome of this study is a developer-friendly unit testing guide organized as a catalog of 53 test cases for token authentication, representing unique combinations of 17 scenarios, 40 conditions, and 30 expected outcomes learned from the data set in our analysis. We supplement the test guide with common test smells to avoid. To verify the accuracy and usefulness of our testing guide, we sought feedback from selected developers, some of whom authored unit tests in our dataset.},
  file        = {:http\://arxiv.org/pdf/2006.14553v1:PDF},
  keywords    = {cs.SE},
}

@Article{Jaszkiewicz2020,
  author      = {Andrzej Jaszkiewicz},
  date        = {2020-11-30},
  title       = {Modified Dorfman procedure for pool tests with dilution -- COVID-19 case study},
  eprint      = {2012.00673},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  abstract    = {The outbreak of the global COVID-19 pandemic results in unprecedented demand for fast and efficient testing of large numbers of patients for the presence of SARS-CoV-2 coronavirus. Beside technical improvements of the cost and speed of individual tests, pool testing may be used to improve efficiency and throughput of a population test. Dorfman pool testing procedure is one of the best known and studied methods of this kind. This procedure is, however, based on unrealistic assumptions that the pool test has perfect sensitivity and the only objective is to minimize the number of tests, and is not well adapted to the case of imperfect pool tests. We propose and analyze a simple modification of this procedure in which test of a pool with negative result is independently repeated up to several times. The proposed procedure is evaluated in a computational study using recent data about dilution effect for SARS-CoV-2 PCR tests, showing that the proposed approach significantly reduces the number of false negatives with a relatively small increase of the number of tests, especially for small prevalence rates. For example, for prevalence rate 0.001 the number of tests could be reduced to 22.1% of individual tests, increasing the expected number of false negatives by no more than 1%, and to 16.8% of individual tests increasing the expected number of false negatives by no more than 10%. At the same time, a similar reduction of the expected number of tests in the standard Dorfman procedure would yield 675% and 821% increase of the expected number of false negatives, respectively. This makes the proposed procedure an interesting choice for screening tests in the case of diseases like COVID-19.},
  file        = {:http\://arxiv.org/pdf/2012.00673v2:PDF},
  keywords    = {stat.ME, stat.AP},
}